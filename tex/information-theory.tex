%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Information Theory}\label{part:information_theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

1923 - Szilard - \emph{On The Manifestation of Thermodynamic Fluctuations}

\cite{shannon48}

(Video) 2018 - Witten - \emph{Introduction to Information Theory} -
\url{https://video.ias.edu/PiTP/2018/0716-EdwardWitten}

Classical Information Theory (Shannon Theory) -- Ideal Gas Model

Quantum Information Theory -- doesn't have a good analog to defining a
Conditional Probability Distribution (\S\ref{sec:conditional_probability}), but
does have an analog of \emph{Strong Subadditivity of Entropy}

2014 -
\url{https://ldtopology.wordpress.com/2014/05/04/low-dimensional-topology-of-information/} -
\emph{Low dimensional topology of information}

2016 - Abramsky - \emph{Information, Processes and Games}



% ==============================================================================
\section{Information}\label{sec:information}
% ==============================================================================

\fist cf. Fisher Information (Statistics \S\ref{sec:fisher_metric})

1965 - Kolmogorov -
\emph{Three Approaches to the Quantitative Definition of Information}:

Combinatorial, Probabilistic, Algorithmic definitions

\emph{Combinatorial}

a Variable $x$ capable of taking Values from a Finite Set $X$ containing $N$
Elements has Entropy (\S\ref{sec:entropy}) $H(x) = \log_2 N$

giving $x$ a definite Value $x = a$ ``removes'' the Entropy and ``Communicates''
(cf. Communication \S\ref{sec:communication}) Information $I = \log_2 N$

\fist cf. \emph{Data} (\S\ref{sec:data}), \emph{(Statistical) Samples}
(\S\ref{sec:sample}) -- conveys Information when viewed ``in context''



% ------------------------------------------------------------------------------
\subsection{Information Content}\label{sec:information_content}
% ------------------------------------------------------------------------------

\emph{Information Content}, \emph{Self-information}, or \emph{Surprisal} of a
Random Variable (\S\ref{sec:random_variable}) or Signal (\S\ref{sec:signal}) is
the amount of Information ``\emph{gained}'' when Sampled (\S\ref{sec:sample});
the Information Content is a Random Variable defined for any Event
(\S\ref{sec:probability_event}) as the Negative Log-probability
(\S\ref{sec:log_probability}) of the Event, whether or not a Random Variable is
being measured. (wiki)

Entropy (\S\ref{sec:entropy}) is the Expected Value
(\S\ref{sec:expected_value}) of the Information Content



% ------------------------------------------------------------------------------
\subsection{Entropy}\label{sec:entropy}
% ------------------------------------------------------------------------------

or \emph{Shannon Entropy}

the Entropy of a Random Variable (\S\ref{sec:random_variable}) or Signal
(\S\ref{sec:signal}) is the Expected Value (\S\ref{sec:expected_value}) of its
Self-information (\S\ref{sec:information_content})

\emph{Maximum Entropy Principle}: the \emph{Maximum Entropy Distribution}
(\S\ref{sec:maximum_entropy}) for a certain Class of Probability Distributions
minimizes the amount of ``Prior Information'' (\S\ref{sec:prior_distribution})
``built in'' to the Distribution
(FIXME: clarify)

the Entropy of a Distribution is the Mean number of Bits-per-symbols in an
Optimal Encoding (\S\ref{sec:encoding}) --
\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iii_expla.html}

\url{https://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html}
-- ``the \emph{Exponential} of Entropy is like Cardinality''

2011 - Baez, Fritz, Leinster - \emph{A Characterization of Entropy in Terms of
  Information Loss}

\asterism

(Witten18):

Subadditivity (\S\ref{sec:subadditive_function}) of Entropy

\emph{Strong Subadditivity of Entropy} (Positivity of Mutual Information) --
a special case of Monotonicity of Relative Entropy

cf. Quantum Mechanics, Quantum Information Theory
-- doesn't have a good analog to defining a Conditional Probability Distribution
(\S\ref{sec:conditional_probability}), but does have an analog of Strong
Subadditivity of Entropy

\fist von Neumann Entropy (\S\ref{sec:vonneumann_entropy})

\asterism

\emph{Firewalls, AdS/CFT, and the Complexity of States and Unitaries: A Computer
  Science Perspective}
(video lectures)
-
\url{https://video.ias.edu/PiTP/2018/0719-ScottAaronson}

Circuit Complexity (\S\ref{sec:circuit_complexity}) can play a similar role to
Entropy in ``breaking Symmetries''

\asterism

\url{https://golem.ph.utexas.edu/category/2019/02/sporadic_sics_and_exceptional.html}:

Quantum Information Theory (\S\ref{sec:quantum_information_theory}): in a
two-level Quantum System (Qubit), the Antipodal SICs minimize Shannon Entropy



\subsubsection{Binary Entropy}\label{sec:binary_entropy}

Entropy of a Bernoulli Process (\S\ref{sec:bernoulli_process}) with Probability
$p$

the Logit Function (\S\ref{sec:logit}) is the Negative of the Derivative of the
Binary Entropy Function



\subsubsection{Relative Entropy}\label{sec:relative_entropy}

or \emph{Kullback-Liebler Divergence}

cf. \emph{Information Radius} (Jensen-Shannon Divergence
\S\ref{sec:information_radius})

\fist Baye's Theorem (\S\ref{sec:bayes_theorem})

Monotonicity of Relative Entropy -- (Positivity of Mutual Information) Strong
Subadditivity of Entropy is a special case

\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iv_a_simp.html}



\paragraph{Mutual Information}\label{sec:mutual_information}\hfill

Taleb2020 \S 6.4 \emph{Fat Tails and Mutual Information} -- Metrics linked to
Entropy such as Mutual Information are more ``potent'' than Correlation; Mutual
Information can detect Non-linearities



\subsubsection{Conditional Entropy}\label{sec:conditional_entropy}

\fist Quantum Conditional Entropy (\S\ref{sec:conditional_quantum_entropy}) --
may be Negative; note there is not a ``classical'' sense of Conditioning in
Quantum Information Theory (FIXME: clarify)

\asterism

\emph{Firewalls, AdS/CFT, and the Complexity of States and Unitaries: A Computer
  Science Perspective}
(video lectures)
-
\url{https://video.ias.edu/PiTP/2018/0719-ScottAaronson}

Symmetry of Conditional Entropy (\S\ref{sec:conditional_entropy})



\subsubsection{Cross Entropy}\label{sec:cross_entropy}

measures the average number of Bits needed to identify an Event drawn from an
underlying Set of Events under two Probability Distributions
(\S\ref{sec:probability_distribution}) $p$ and $q$, if the ``coding scheme'' is
optimized for an ``unnatural'' Distribution $q$ rather than a ``true''
Distribution $p$
%FIXME: clarify

Cross-entropy Loss (\S\ref{sec:objective_function})

\emph{Categorical Cross-entropy} -- Loss Function for Statistical Classification
(\S\ref{sec:classification}); cf. Squared Error Loss for Regression Analysis

\fist Cross-entropy Error Function -- minimizing \emph{Negative Log-likelihood}
(\S\ref{sec:log_likelihood}) is the same as minimizing Cross Entropy (cf.
Logistic Regression \S\ref{sec:logistic_regression})



% ------------------------------------------------------------------------------
\subsection{Negentropy}\label{sec:negentropy}
% ------------------------------------------------------------------------------



% ==============================================================================
\section{Channel}\label{sec:channel}
% ==============================================================================

% ------------------------------------------------------------------------------
\subsection{Channel Capacity}\label{sec:channel_capacity}
% ------------------------------------------------------------------------------

\fist Cf. \emph{Controllability} (Control Theory
\S\ref{sec:control_theory})



% ------------------------------------------------------------------------------
\subsection{Channel State Information (CSI)}\label{sec:csi}
% ------------------------------------------------------------------------------

known Channel Properties describing how a Signal propagates and represents the
combined effect of Scattering, Fading, and Power Decay

\emph{Channel Estimation}



% ==============================================================================
\section{Coding Theory}\label{sec:coding_theory}
% ==============================================================================

% ------------------------------------------------------------------------------
\subsection{Hamming Space}\label{sec:hamming_space}
% ------------------------------------------------------------------------------

Set of all $2^N$ Binary Strings of Length $N$

%FIXME:

\url{http://bit-player.org/2018/the-mind-wanders}:

Sparse Distributed Memory (SDM)

(Kanerva88 - \emph{Sparse Distributed Memory})

Content-addressable Memory (or Associative Memory)

a Set of Binary Vectors is closed under \emph{Binding} and \emph{Bundling},
except that the result extracted from a Bundled Vector has to be ``cleaned up''
by Recursive Recall



\subsubsection{Hamming Distance}\label{sec:hamming_distance}

Distance Metric (\S\ref{sec:metric}) for Hamming Space



% ------------------------------------------------------------------------------
\subsection{Encoding}\label{sec:encoding}
% ------------------------------------------------------------------------------

the Entropy (\S\ref{sec:entropy}) of a Distribution
(\S\ref{sec:probability_distribution}) is the Mean number of
Bits-per-symbols in an Optimal Encoding --
\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iii_expla.html}

\fist Autoencoders (\S\ref{sec:autoencoder})



% ==============================================================================
\section{Algorithmic Information Theory}\label{sec:algorithmic_information}
% ==============================================================================

1965 - Komogorov - \emph{Three Approaches to the Quantitative Definition of
  Information}

\emph{Combinatorial approach} -- a Variable $x$ capable of taking Values in a
Finite Set $X$ containing $N$ Elements; Entropy $H(x) = \log_2 N$; Conditional
Entropy $H(y/a) = \log_2 N(Y_a)$; Information conveyed by $x$ with respect to
$y$:
\[
  I(x:y) = H(y) - H(y/x)
\]
(where $H(y/x)$ and $I(x : y)$ are Functions of $x$ and $y$ is a Bound Variable)

Combinatorial approach allows for a ``Universal Coding''
(\S\ref{sec:coding_theory}) method permitting the transmission of any
sufficiently long messages, and it is not necessary to know the Frequencies of
each Symbol beforehand

\emph{Probabilistic approach} -- take $x$ and $y$ as Random Vaiables with given
Joint Probability Distributions (\S\ref{sec:joint_probability}); Entropy
$H_W(x) = -\sum_x p(x) \log_2 p(x)$, Conditional Entropy
$H_W(y/x) = -\sum_y p(y/x) \log_2 p(y/x)$

Inequalities:
\begin{align*}
  H_W(x) \leq H(x) \\
  H_W(y/x) \leq H(y/x)
\end{align*}
hold when the corresponding Distributions on both $X$ and $Y_X$ are Uniform

Information conveyed by $x$ with respect to $y$: $I_W(x:y) = H_W(y) - H_W(y/x)$

Information Content: $I_w(x,y) = MI_W(x : y) = MI_W(y : x)$ where $M$ is the
mathematical \emph{Expectation} (\S\ref{sec:expected_value})

in the Combinatorial approach, $I(x:y)$ is always Non-negative, but in the
Probabilistic approach, $I_W(x:y)$ may be Negative-- only the \emph{averaged}
quantity $I_W(x,y)$ is a measure of the Information Content

Probabilistic approach allows for transmission over Channels carrying ``bulk''
Information consisting of a large number of unrelated (or weakly related)
messages obeying definite Probabilistic laws; empirically Frequencies and
Probabilities are often interchangeable over sufficiently long time sequences

\emph{Algorithmic approach} -- quantitative Information conveyed by an
\emph{individual} object $x$ about an \emph{individual} object $y$ is only
meaningful when the quantity of Information is sufficiently large

Relative Complexity (\S\ref{sec:algorithmic_complexity}) $K_A(y/x)$ and the
quantity of Information are Equivalent in that the difference between them is
bounded

quantity of Information conveyed by $x$ about $y$:
\[
  I_A(x:y) = K_A(y) - K_A(y/x)
\]
is no less than some Negative Constant $C$

(wiki):

(Solomonoff60, Kolmogorov65) \textbf{Thm.}
\emph{Among Algorithms that Decode Strings from their descriptions (Codes),
  there exists an Asymptotically Optimal one.}

statement in (Martin-L\"of66):
\emph{
  There exists an algorithm $A$ such that for any Algorithm $B$:
  \[
    K_A(x) \leq K_B(x) + c
  \]
  where $c$ is a Constant dependent on $A$ and $B$ but not $x$.
}

-- can be used to define several Functions of Strings including Complexity,
Randomness, and Information



% ------------------------------------------------------------------------------
\subsection{Algorithmic Complexity}\label{sec:algorithmic_complexity}
% ------------------------------------------------------------------------------

\emph{Algorithmic Complexity}, \emph{Kolmogorov-Chaitin Complexity}, or
\emph{Descriptive Complexity}

cf. Computational Complexity (\S\ref{sec:complexity_theory})

(Kolmogorov65):

the \emph{Relative Complexity} of an object $y$ with a given $x$
is the minimal length $l(p)$ of the Program $p$ for obtaining $y$ from $x$

a ``Programming Method'' is a Partial Recursive Function $\varphi(p, x) = y$
that associates an object $y$ with a Program $p$ and an object $x$; for any such
Programming Method:
\[
  K_\varphi(y/x) =
  \begin{cases}
    \nexists p : \varphi(p,x) = y & \infty \\
    \text{otherwise}              & min_{\varphi(p, x)=y} l(p)
  \end{cases}
\]

\emph{Asymptotically Optimal} (Komogorov) or \emph{Universal} (Solomonoff)
Algorithm $A$

\textbf{Fundamental Theorem} \emph{
  There exists a Partial Recursive Function $A(p, x)$ such that for any other
  Partial Recursive Function $\varphi(p, x)$:
  \[
    K_A(y/x) \leq K_\varphi(y/x) + C_\varphi
  \]
  where Constant $C_\varphi$ does not depend on $x$ or $y$.
}

-- can be used to define several Functions of Strings including Complexity,
Randomness, and Information

statement in (Martin-L\"of66):
\emph{
  There exists an algorithm $A$ such that for any Algorithm $B$:
  \[
    K_A(x) \leq K_B(x) + c
  \]
  where $c$ is a Constant dependent on $A$ and $B$ but not $x$.
}



% ------------------------------------------------------------------------------
\subsection{Algorithmic Randomness}\label{sec:algorithmic_randomness}
% ------------------------------------------------------------------------------

cf. \emph{Statistical Randomness} (\S\ref{sec:statistical_randomness})

Random Sequence (\S\ref{sec:random_sequence})

1966 - Martin-L\"of - \emph{The Definition of Random Sequences}

Elements of a given large Finite population are \emph{Random} whose Complexity
(\S\ref{sec:algorithmic_complexity}) is \emph{maximal}, and almost all Elements
have a Complexity close to the maximal value

Collectives (Von Mises57); cf. Frequency Interpretation of Probability Theory
(Part \ref{part:probability_theory})

Universal Test (\S\ref{sec:hypothesis_testing})

Critical Level

Infinite Binary Squences -- Universal Sequential Test

Critical Region (\S\ref{sec:critical_region})

Non-random Infinite Binary Random Sequences form a maximal Constructive Null Set
(\S\ref{sec:null_set}), i.e. a Constructive Null Set $\mathcal{A}$ such that any
Constructive Null Set $\mathcal{B}$ is Contained in it:
$\mathcal{B} \subseteq \mathcal{A}$

Random Sequences (\S\ref{sec:random_sequence}) with respect to an arbitrary
Computable Probability Distribution (\S\ref{sec:probability_distribution})

\emph{Subsequence Selection Criterion} --
\emph{Mises-Church Randomness}: any Recursive Function which having read the
first $N$ elements of the Sequence decides if it wants to select element $N+1$

Bernoulli Sequences (\S\ref{sec:bernoulli_sequence}); Computable ``Success
Probability''

``Irregularity Condition''



% ------------------------------------------------------------------------------
\subsection{Algorithmic Probability}\label{sec:algorithmic_probability}
% ------------------------------------------------------------------------------

or \emph{Solomonoff Probability}

\fist Probability (\S\ref{sec:probability})

Universal Prior

Occam's Razor, Epicurus' Principle



% ------------------------------------------------------------------------------
\subsection{Kolmogorov Structure Function}\label{sec:kolmogorov_structure}
% ------------------------------------------------------------------------------

Non-probabilistic approach to Statistics and Model Selection
(\S\ref{sec:statistical_model})



% ==============================================================================
\section{Signal Processing}\label{sec:signal_processing}
% ==============================================================================

``Signals and Systems'' -- electrical engineering; Input Signals and Output
Signals and ``mathematical representations'' (Systems) between them

\fist cf. Dynamical Systems (\S\ref{sec:dynamical_system}), Open Systems
(\S\ref{sec:open_system})

%FIXME: move the following to dynamical systems ???

Discrete-time Systems -- Excitation (Input), Response (Output)
(\S\ref{sec:impulse_response})

Relaxed System -- Initial Condition $f(t_0-1) = 0$

Static Discrete-time System -- Response depends at most on the current (not
future or past) Input

Dynamic Discrete-time System -- current Response can depend on past or future
Input

Continuous-time Systems



% ------------------------------------------------------------------------------
\subsection{Signal}\label{sec:signal}
% ------------------------------------------------------------------------------

\fist Fourier Transform (\S\ref{sec:fourier_transform})

cf. \emph{Wave} (\S\ref{sec:wave})

(wiki):

a \emph{Periodic Signal} has a \emph{Discrete Frequency Spectrum} -- a Periodic
Signal can be analyzed using a \emph{Discrete Frequency Domain}, i.e. the
Fourier Transform of a Periodic Signal only has energy at a \emph{Base
  Frequency} and its \emph{Harmonics}

a \emph{Discrete-time Signal} has a \emph{Periodic Frequency Spectrum}

a \emph{Discrete, Periodic Signal} has a \emph{Periodic and Discrete Frequency
  Spectrum}

The \emph{Information Content} (\emph{Self-information} or \emph{Surprisal}
\S\ref{sec:information_content}) of a Sampled (\S\ref{sec:sample}) Random
Variable or Signal is the amount of Information (\S\ref{sec:information})
``gained'' by the Sample; this Information Content is a Random Variable defined
for any Event regardless of whether a Random Variable is being measured or not.
(wiki)

Independent \& Identically Distributed (IID \S\ref{sec:iid}) Signals --
\emph{Identically Distributed} implies that the ``Signal Level must be
Balanced'' (FIXME: xref/clarify) and \emph{Independent} implies a White Noise
Spectrum (flat)

FIXME: is it true that ``independence implies a white (flat) spectrum'',
but ``white'' spectrum does *not* imply independence ?

(Mandelbrot97E, Ch.19\S 1) if $Z(t)$ is a Martingale, its Derivative is
Spectrally ``white'' in the sense that the Covariance $C(\tau)$ between $Z'(t)$
and $Z'(t + \tau)$ vanishes if $\tau \neq 0$; it follows that the Expected Value
of the Sample Spectral Density of $Z'(t)$ will be a Constant, Independent of
Frequency; ``Spectral Methods'' are concerned with measuring Correlation rather
than Statistical Dependence; Spectral ``whiteness'' expresses lack of
Correlation, but it is not \emph{synonymous} with Independence, except in the
atypical case when the Marginal Distribution and the Joint distributions at
different times are Gaussian

(Mandelbrot97E, Ch.20) when $\expect(P(t+s) - P(t))^2$ is Finite for all $t$ and
$s$ and $P(t)$ is a Martingale (\S\ref{sec:martingale_process}), increments are
Uncorrelated (``Orthogonal'' or ``Spectrally White'')

(Mandelbrot97E, Ch.20) a Process is called ``White'' if its Spectral Density
(\S\ref{sec:spectrum_analysis}) is Independent of the Frequency



\subsubsection{Transient}\label{sec:transient}

\subsubsection{Continuous Signal}\label{sec:continuous_signal}

\subsubsection{Discrete Signal}\label{sec:discrete_signal}

\fist Time Series (\S\ref{sec:time_series})



\subsubsection{Sampling Frequency}\label{sec:sampling_frequency}

reduction of a Continuous-time Signal to a Discrete-time Signal

\emph{Nyquist Frequency} -- half of the Sampling Rate



\subsubsection{Quasiperiodic Signal}\label{sec:quasiperiodic_signal}

cf. Almost Periodic Function (\S\ref{sec:almost_periodic})



\subsubsection{Energy}\label{sec:energy}

\url{https://www.gaussianwaves.com/2013/12/power-and-energy-of-a-signal/}:

the \emph{Energy} of a Continuous-time Complex Signal $x(t)$ is defined as:
\[
  E_x = \int_{-\infty}^\infty |x(t)|^2 dt
\]

Discrete-time:
\[
  E_x = \sum_{n \in \ints} |x(n)|^2
\]



\subsubsection{Power}\label{sec:signal_power}

Signal $x(t)$

\emph{Instantaneous Power} $p(t) = |s(t)|^2$

\url{https://www.gaussianwaves.com/2013/12/power-and-energy-of-a-signal/}:

Discrete-time:
\[
  P_x = \lim_{N\to\infty} \frac{1}{2N+1} \sum_{n \in [-N,N]}|x(n)|^2
\]

Total Power (FIXME)

Peak Power (FIXME)

a Finite-energy Signal has Zero Total Power

a Signal with Finite, Non-zero Total Power has Infinite Energy



\subsubsection{Resampling}\label{sec:resampling}

\paragraph{Downsampling}\label{sec:downsampling}\hfill

or \emph{Sub-sampling}

Compression, Decimation

``$p$-norm Sub-sampling'' -- Pooling Layers in Convolutional Neural Networks
(\S\ref{sec:cnn})



\paragraph{Upsampling}\label{sec:upsampling}\hfill

Expansion, Interpolation



% ------------------------------------------------------------------------------
\subsection{Time Domain}\label{sec:time_domain}
% ------------------------------------------------------------------------------

Waveforms (\S\ref{sec:waveform}) -- Time-domain Graph of a Variable; the form or
``shape'' of $F$ is D'Alembert's Formula (Wave Equations
\S\ref{sec:wave_equation})

Laplace Transform (\S\ref{sec:laplace_transform}) from Time Domain to Frequency
Domain transforms Differential Equations (\S\ref{sec:differential_equation})
into Algebraic (Polynomial) Equations (\S\ref{sec:polynomial_equation}), and
Convolution (\S\ref{sec:convolution}) into Multiplication

\fist Time-series Analysis (\S\ref{sec:time_series_analysis})



\subsubsection{Waveform}\label{sec:waveform}

Time-domain Graph of a Variable

the form or \emph{shape} of $F$ is D'Alembert's Formula

\begin{itemize}
  \item Sinusoid (\S\ref{sec:sinusoid}) -- Waveform of a Sine Function
    (\S\ref{sec:trigonometric_function})
  \item ...
\end{itemize}



\subsubsection{Jitter}\label{sec:jitter}

Phase Noise (\S\ref{sec:phase_noise})



% ------------------------------------------------------------------------------
\subsection{Frequency Domain}\label{sec:frequency_domain}
% ------------------------------------------------------------------------------

Frequency (\S\ref{sec:frequency}), Wavenumber (\S\ref{sec:wavenumber})

\begin{itemize}
  \item Fourier Series (\S\ref{sec:fourier_series}) -- Periodic Signals,
    Oscillating Systems
  \item Fourier Transform (\S\ref{sec:fourier_transform}) -- Aperiodic Signals,
    Transients (\S\ref{sec:transient})
  \item Laplace Transform (\S\ref{sec:laplace_transform}) -- Control Systems
    (\S\ref{sec:control_system})
  \item Z-Transform (\S\ref{sec:z_transform}) -- Discrete-time Laplace
    Transform, DSP (\S\ref{sec:dsp}
  \item Wavelet Transform (\S\ref{sec:wavelet_transform}) -- Image Processing,
    Data Compression
\end{itemize}

Laplace Transform (\S\ref{sec:laplace_transform}) from Time Domain to Frequency
Domain transforms Differential Equations (\S\ref{sec:differential_equation})
into Algebraic (Polynomial) Equations (\S\ref{sec:polynomial_equation}), and
Convolution (\S\ref{sec:convolution}) into Multiplication

\fist Frequency Domain Analysis (Statistical Signal Processing
\S\ref{sec:spectrum_analysis})

\fist Frequency Response (\S\ref{sec:frequency_response})

``Forecasting in Frequency Space diverges from expected payoff''
(Taleb 2020 \S 3.3)



\subsubsection{Frequency Spectrum}\label{sec:frequency_spectrum}

\fist Frequency Estimation (Spectrum Analysis \S\ref{sec:frequency_estimation})

Mandelbrot97E

note that a Signal may have a ``White'' Frequency Spectrum but still be Serially
Dependent

FIXME: is it true that ``independence implies a white (flat) spectrum'',
but ``white'' spectrum does *not* imply independence ?

(Ch.15) ``Spectral White-ness'' -- absence of \emph{Correlation}
(\S\ref{sec:correlation})

Spectra of Compound Processes (\S\ref{sec:compound_process}) are only sensitive
to the Spectrum of the Directed (Compounding) Function (and completely blind to
properties of the Directing Function); e.g. the Directing Function (Increments)
may be strongly Dependent, but \emph{Uncorrelated} (i.e. Spectrally White)

(Mandelbrot97E, Ch.19\S 1) if $Z(t)$ is a Martingale, its Derivative is
Spectrally ``white'' in the sense that the Covariance $C(\tau)$ between $Z'(t)$
and $Z'(t + \tau)$ vanishes if $\tau \neq 0$; it follows that the Expected Value
of the Sample Spectral Density of $Z'(t)$ will be a Constant, Independent of
Frequency; ``Spectral Methods'' are concerned with measuring Correlation rather
than Statistical Dependence; Spectral ``whiteness'' expresses lack of
Correlation, but it is not \emph{synonymous} with Independence, except in the
atypical case when the Marginal Distribution and the Joint distributions at
different times are Gaussian

(Mandelbrot97E, Ch.20) when $\expect(P(t+s) - P(t))^2$ is Finite for all $t$ and
$s$ and $P(t)$ is a Martingale (\S\ref{sec:martingale_process}), increments are
Uncorrelated (``Orthogonal'' or ``Spectrally White'')

(Mandelbrot97E, Ch.20) a Process is called ``White'' if its Spectral Density
(\S\ref{sec:spectrum_analysis}) is Independent of the Frequency



\subsubsection{Phase Noise}\label{sec:phase_noise}

Frequency-domain representation of random fluctuations in Phase, corresponding
to Time-domain Jitter (\S\ref{sec:jitter})



% ------------------------------------------------------------------------------
\subsection{Noise}\label{sec:noise}
% ------------------------------------------------------------------------------

a \emph{Noise Signal} is a Signal produced by a Stochastic Process
(\S\ref{sec:stochastic_process})

cf. ``Statistical Noise'': Fraction of Variance Unexplained (Regression Analysis
\S\ref{sec:fvu})

cf. Phase Noise (\S\ref{sec:phase_noise})



\subsubsection{Gaussian Noise}\label{sec:gaussian_noise}

\subsubsection{White Noise}\label{sec:white_noise}

Statistical Independence (\S\ref{sec:independence})

(wiki):

White Noise Vectors -- the Covariance Matrix $R$ of the Components of a White
Noise Vector $\{ w_t \}$ of length $n$ is an $n \times n$ Diagonal Matrix where
each Diagonal Element $R_{ii}$ is the Variance of the Component $w_i$, and the
Correlation Matrix must be the $n \times n $ Identity Matrix

the Power Spectrum (\S\ref{sec:psd}) of a White Noise Vector is the Expected
Value of the Squared Modulus (Absolute Value) of each Coefficients of the
Fourier Transform $W$:
\[
  \expect(|W_i|^2)
\]
for Non-Gaussian White Noise Vectors, the Fourier Coefficients $W_i$ will not be
completely Independent of each other

Brownian Motion (Wiener Process \S\ref{sec:wiener_process}) can be viewed as
White Noise Filtered by $s^{-1}$

Fractional Brownian Motion (\S\ref{sec:fractional_brownian_motion}) can be
viewed as White Noise Filtered by $s^{-H-\frac{1}{2}}$;
Weyl Integral (\S\ref{sec:weyl_integral}) with respect to White Noise Measure
(TODO: xref Bochner-Minlos Theorem for Nuclear Spaces)

Mandelbrot97E:

note that a Signal may have a ``White'' Frequency Spectrum but still be Serially
Dependent (\S\ref{sec:dependence})

Spectra of Compound Processes (\S\ref{sec:compound_process}) are only sensitive
to the Spectrum of the Directed (Compounding) Function (and completely blind to
properties of the Directing Function); e.g. the Directing Function (Increments)
may be strongly Dependent, but \emph{Uncorrelated} (i.e. Spectrally White)

(Mandelbrot97E, Ch.19\S 1) if $Z(t)$ is a Martingale, its Derivative is
Spectrally ``white'' in the sense that the Covariance $C(\tau)$ between $Z'(t)$
and $Z'(t + \tau)$ vanishes if $\tau \neq 0$; it follows that the Expected Value
of the Sample Spectral Density of $Z'(t)$ will be a Constant, Independent of
Frequency; ``Spectral Methods'' are concerned with measuring Correlation rather
than Statistical Dependence; Spectral ``whiteness'' expresses lack of
Correlation, but it is not \emph{synonymous} with Independence, except in the
atypical case when the Marginal Distribution and the Joint distributions at
different times are Gaussian

(Mandelbrot97E, Ch.20) a Process is called ``White'' if its Spectral Density
(\S\ref{sec:spectrum_analysis}) is Independent of the Frequency

% FIXME: how is the above consistent with white noise being defined by
% "statistical independence" ???



\paragraph{Gaussian White Noise}\label{sec:gaussian_white_noise}\hfill

any Orthogonal Transformation of a Gaussian White Noise Vector results in a
Gaussian White Noise Vector

under most types of Discrete Fourier Transform (FFT, Hartley), the Transform of
$\{ w_t \}$ will be a Gaussian White Noise Vector, i.e. the $n$ Fourier
Coefficients will be Independent Gaussian Variables with zero Mean and the same
Variance $\sigma^2$

the Power Spectrum of a Gaussian White Noise Vector is constant $P_i = \sigma^2$
for all $i$ where $i$ are the $n$ Fourier Coefficients



\paragraph{Additive White Gaussian Noise (AWGN)}\label{sec:awgn}\hfill



\subsubsection{Pink Noise}\label{sec:pink_noise}

cf. Self-affine Function (\S\ref{sec:self_affinity})

Mandelbrot97E:

``flicker noise''; cf. concentration of discontinuities in price series: ARCH
Models (\S\ref{sec:arch}), Multifractal Time (\S\ref{sec:multifractal_time})

Ch.6

Fractional Gaussian Noise (\S\ref{sec:fractional_gaussian}) $B'_H(t)$ as a
``continuing'' or ``humming'' form of $\frac{1}{f}$ Noise with Spectral Density
(\S\ref{sec:spectrum_analysis}) Proportional to $f^{-2H+1}$



\subsubsection{Brownian Noise}\label{sec:brownian_noise}

\emph{Brown Noise} or \emph{Red Noise}



% ------------------------------------------------------------------------------
\subsection{Impulse Response}\label{sec:impulse_response}
% ------------------------------------------------------------------------------

%FIXME: move to dynamical systems, filters ???

\emph{Impulse Response Function (IRF)} $h(t)$ -- cf. \emph{Transfer Function}
$H(s)$

Green's Function (\S\ref{sec:greens_function})

for a Linear Time-Invariant (LTI) System (\S\ref{sec:lti_system}), the IRF and
the Transfer Function both fully define the Input/Output characteristics of the
LTI System

the Convolution (\S\ref{sec:convolution}) of an Input Signal with an Impulse
Response gives the Output of an LTI System

the Impulse Response is equivalent to the Inverse Laplace Transform
(\S\ref{sec:laplace_transform}) of a Dynamical System's Transfer Function
(Evolution Function \S\ref{sec:evolution_function})

Acoustics -- Convolution Reverb; Impulse Response can be recorded from the
Reverberation caused by an ideal Impulse (\S\ref{sec:impulse}) is played in an
acoustic space or hardware reverberation unit



\subsubsection{Impulse}\label{sec:impulse}

Unit Impulse (Dirac Delta \S\ref{sec:dirac_delta})



\paragraph{Impulse Train}\label{sec:impulse_train}\hfill

Sampling Function (Dirac Comb \S\ref{sec:dirac_comb})

Boltzmann Machines (\S\ref{sec:boltzmann_machine}) use a Boltzmann Distribution
in their Sampling Function



\subsubsection{Linear Response Function}\label{sec:linear_response}

\subsubsection{Finite Impulse Response}\label{sec:fir}

\subsubsection{Infinite Impulse Response}\label{sec:iir}



% ------------------------------------------------------------------------------
\subsection{Phase Response}\label{sec:phase_response}
% ------------------------------------------------------------------------------

Linear Phase Filter (\S\ref{sec:linear_phase}) -- Phase Response is a Linear
Function of Frequency-- all Frequency Components of the Input Signal are shifted
in Time by the same constant amount (that is the Derivative of a Linear
Function), viz. the Group Delay (\S\ref{sec:group_delay})



\subsubsection{Group Delay}\label{sec:group_delay}

measure of Time Distortion computed by Differentiating the Phase Response with
respect to Frequency

Linear Phase Filter (\S\ref{sec:linear_phase}) -- Phase Response is a Linear
Function of Frequency-- all Frequency Components of the Input Signal are shifted
in Time by the same constant amount (that is the Derivative of a Linear
Function), viz. the Group Delay



\subsubsection{Phase Delay}\label{sec:phase_delay}



% ------------------------------------------------------------------------------
\subsection{Frequency Response}\label{sec:frequency_response}
% ------------------------------------------------------------------------------

\fist Frequency Domain Analysis (\S\ref{sec:spectrum_analysis})



% ------------------------------------------------------------------------------
\subsection{Filter}\label{sec:signal_filter}
% ------------------------------------------------------------------------------

Time-invariant Systems (\S\ref{sec:tiv_system})

Time-variant Systems (\S\ref{sec:time_variant_system})

Discrete-time (Sampled):
\begin{itemize}
  \item Finite Impulse Response (FIR \S\ref{sec:fir})
  \item Infinite Impulse Response (IIR \S\ref{sec:iir})
\end{itemize}

Continuous-time:
\begin{itemize}
  \item Passive Filter (\S\ref{sec:passive_filter})
  \item Active Filter (\S\ref{sec:active_filter})
\end{itemize}



\subsubsection{Passive Filter}\label{sec:passive_filter}

\subsubsection{Active Filter}\label{sec:active_filter}

\subsubsection{Linear Filter}\label{sec:linear_filter}

Linear Continuous-time Filter -- generally designed to \emph{remove} certain
Frequencies

Network Synthesis Filters:
\begin{itemize}
  \item Chebyshev Filter
  \item Elliptic Filter
  \item Butterworth Filter
  \item Bessel Filter
\end{itemize}

Image Parameter Filters (``Wave Filters''):
\begin{itemize}
  \item Constant $k$ Filter
  \item $m$-derived Filter
\end{itemize}



\subsubsection{Non-linear Filter}\label{sec:nonlinear_filter}

\subsubsection{Causal Filter}\label{sec:causal_filter}

Output does not depend on Future Input



\subsubsection{Linear Phase Filter}\label{sec:linear_phase}

Phase Response (\S\ref{sec:phase_response}) is a Linear Function of Frequency



\paragraph{Sinc Filter}\label{sec:sinc_filter}\hfill

\emph{Ideal Low-pass Filter}

Frequency Response is a Rectangular Function and is a ``Brick-wall Filter''

Time-domain Impulse Response is the Sinc Function (\S\ref{sec:sinc_function})



\subsubsection{Adaptive Filter}\label{sec:adaptive_filter}

Optimization



\paragraph{Least Mean Squares (LMS)}\label{sec:lms}\hfill

Stochastic Gradient Descent (SGD \S\ref{sec:sgd}) method

cf. Backpropagation (\S\ref{sec:backpropagation})



% ------------------------------------------------------------------------------
\subsection{Moving Average}\label{sec:moving_average}
% ------------------------------------------------------------------------------

Moving-average Model (\S\ref{sec:moving_average_model})

a type of Finite Impulse Response (\S\ref{sec:fir}) Filter (\S\ref{sec:filter})

(wiki): compared to the Autoregressive Model (\S\ref{sec:autoregressive_model}),
the Moving-average Model is always Stationary and may not contain a Unit Root
(TODO: xrefs)

Mandelbrot97E, Ch.20:
every ``purely Non-deterministic'' Stationary Random Process has the form of a
Moving Averge Process; (Fama 1965 \S 12.4)



\subsubsection{Simple Moving Average (SMA)}\label{sec:sma}

\subsubsection{Cumulative Moving Average (CMA)}\label{sec:cma}

\subsubsection{Weighted Moving Average (WMA)}\label{sec:wma}

Convolution (\S\ref{sec:convolution})



\paragraph{Exponentially Weighted Moving Average (EWMA)}\label{sec:ewma}\hfill



\subsubsection{Vector Moving Average (VMA)}\label{sec:vma}

Vector AutoRegression (VAR \S\ref{sec:var})



% ------------------------------------------------------------------------------
\subsection{Wavelet}\label{sec:wavelet}
% ------------------------------------------------------------------------------

\subsubsection{Mother Wavelet}\label{sec:mother_transform}

\subsubsection{Scaling Function}\label{sec:scaling_function}

or \emph{Father Wavelet}



\subsubsection{Wavelet Transform}\label{sec:wavelet_transform}

Frequency Domain (\S\ref{sec:frequency_domain})

cf. Fourier Transform (\S\ref{sec:fourier_transform})

Multiresolution Analysis (\S\ref{sec:multiresolution_analysis})

Orthonormal Wavelet Basis; Haar (\S\ref{sec:haar_wavelet}) Basis

Model-free Time Series Analysis (\S\ref{sec:model_free_analysis}) -- Locally
Stationary Wavelets, Wavelet Decomposed Neural Networks



\paragraph{Discrete Wavelet Transform}
\label{sec:discrete_wavelet_transform}\hfill

\paragraph{Fast Wavelet Transform (FWT)}\label{sec:fwt}\hfill



\subsubsection{Haar Wavelet}\label{sec:haar_wavelet}

(Wasserman04, \S21.4) Haar Wavelet Regression, Haar Wavelet Density Estimation

Haar Basis



% ------------------------------------------------------------------------------
\subsection{Coherence}\label{sec:signal_coherence}
% ------------------------------------------------------------------------------

(wiki):

Statistic (\S\ref{sec:statistic}) used to relate two Signals (or Datasets)

used to Estimate power transfer between input and output of a Linear System; if
the Signals are Ergodic and the System (Transfer) Function is Linear, then
Coherence can be used to Estimate the \emph{Causality} between input and output



% ------------------------------------------------------------------------------
\subsection{Compressed Sensing}\label{sec:compressed_sensing}
% ------------------------------------------------------------------------------

or \emph{Sparse Sampling}



% ------------------------------------------------------------------------------
\subsection{MultiResolution Analysis (MRA)}\label{sec:multiresolution_analysis}
% ------------------------------------------------------------------------------

\emph{MultiScale Approximation} (MSA)

Wavelet Transforms (\S\ref{sec:wavelet_transform})

Orthonormal Wavelet Basis



% ------------------------------------------------------------------------------
\subsection{Digital Signal Processing (DSP)}\label{sec:dsp}
% ------------------------------------------------------------------------------

\fist Z-transform (Discrete-time Laplace Transform \S\ref{sec:z_transform})



% ------------------------------------------------------------------------------
\subsection{Image Processing}\label{sec:image_processing}
% ------------------------------------------------------------------------------

Convolution (\S\ref{sec:convolution})

Point Spread Function -- Convolution by a Gaussian ``spreading'' function



% ------------------------------------------------------------------------------
\subsection{Time-Frequency Analysis}\label{sec:time_frequency_analysis}
% ------------------------------------------------------------------------------

techniques for characterizing and manipulting Signals with Statistics (cf.
Entropy \S\ref{sec:entropy}) that vary in Time, e.g. \emph{Transient Signals}

Non-stationary Signals

\fist Quasiprobability Distributions
(\S\ref{sec:quasiprobability_distribution})



\subsection{Time-Frequency Representation}
\label{sec:time_frequency_representation}



% ------------------------------------------------------------------------------
\subsection{Statistical Signal Processing}\label{sec:statistical_signal}
% ------------------------------------------------------------------------------

treats Signals as \emph{Stochastic Processes} (\S\ref{sec:stochastic_process})

\fist cf. Time-series Analysis (\S\ref{sec:time_series_analysis}) of Stationary
and Ergodic Processes



\subsubsection{Window Function}\label{sec:window_function}

or \emph{Apodization Function} or \emph{Tapering Function}

\fist Kernel (Statistics \S\ref{sec:distribution_kernel})



\paragraph{Hamming Window}\label{sec:hamming_window}\hfill



\subsubsection{Spectrum Analysis}\label{sec:spectrum_analysis}

or \emph{Spectral Density Estimation (SDE)} or \emph{Frequency Domain Analysis}

Harmonic Analysis (\S\ref{sec:harmonic_analysis}): Fourier Analysis
(\S\ref{sec:fourier_analysis})

Periodic Functions (\S\ref{sec:periodic_function})

Frequency Spectrum (\S\ref{sec:frequency_spectrum})

\fist Frequency Response (\S\ref{sec:frequency_response})

\fist Fractal Geometry (Part \ref{part:fractal_geometry}): ``Geometric face'' of
Harmonic Analysis (Mandelbrot82)

1982 - Thomson - \emph{Spectrum Estimation and Harmonic Analysis}

Mandelbrot97E:

note that a Signal may have a ``White'' (\S\ref{sec:white_noise}) Frequency
Spectrum but still be Serially Dependent (\S\ref{sec:dependence})

Spectra of Compound Processes (\S\ref{sec:compound_process}) are only sensitive
to the Spectrum of the Directed (Compounding) Function (and completely blind to
properties of the Directing Function); e.g. the Directing Function (Increments)
may be strongly Dependent, but \emph{Uncorrelated} (i.e. Spectrally White)

(Mandelbrot97E, Ch.19\S 1) if $Z(t)$ is a Martingale, its Derivative is
Spectrally ``white'' in the sense that the Covariance $C(\tau)$ between $Z'(t)$
and $Z'(t + \tau)$ vanishes if $\tau \neq 0$; it follows that the Expected Value
of the Sample Spectral Density of $Z'(t)$ will be a Constant, Independent of
Frequency; ``Spectral Methods'' are concerned with measuring Correlation rather
than Statistical Dependence; Spectral ``whiteness'' expresses lack of
Correlation, but it is not \emph{synonymous} with Independence, except in the
atypical case when the Marginal Distribution and the Joint distributions at
different times are Gaussian

(Mandelbrot97E, Ch.20) when $\expect(P(t+s) - P(t))^2$ is Finite for all $t$ and
$s$ and $P(t)$ is a Martingale (\S\ref{sec:martingale_process}), increments are
Uncorrelated (``Orthogonal'' or ``Spectrally White'')

(Mandelbrot97E, Ch.20) a Process is called ``White'' if its Spectral Density is
Independent of the Frequency



\paragraph{Periodogram}\label{sec:periodogram}\hfill

Spectral Density Estimator

cf. Histogram (\S\ref{sec:histogram})



\paragraph{Spectrogram}\label{sec:spectrogram}\hfill

a Sequence of Periodograms over Time



\paragraph{Frequency Estimation}\label{sec:frequency_estimation}\hfill

Estimation of Complex Frequency Components of a Signal in the presence of Noise
given \emph{assumptions} about the number fo the Components



\subparagraph{Sinusoidal Model}\label{sec:sinusoidal_model}\hfill



\paragraph{Energy Spectral Density (ESD)}\label{sec:esd}\hfill

\paragraph{Power Spectral Density (PSD)}\label{sec:psd}\hfill

or \emph{Power Spectral Density (PSD)}

the PSD of Fractional Brownian Motion (\S\ref{sec:fbm}) with Index of Dependence
$H$:
\begin{itemize}
  \item $\frac{1}{f^{2H+1}}$ -- $H < 0.5$; ``Sub-diffusive Processes''
  \item $\frac{1}{f^2H}$ -- $0.5 < H < 1$; ``Super-diffusive Processes''
\end{itemize}

$F(f)$ -- Power Spectral Distribution Function (\emph{Integrated Spectrum})

\emph{Wiener-Khinchin Theorem} -- the Autocorrelation Function
(\S\ref{sec:autocorrelation_function}) of a Covariance-stationary Process
(\S\ref{sec:covariance_stationary}) has a Spectral Decomposition
(\S\ref{sec:spectral_decomposition}) given by the Power Spectrum of the Process



% ==============================================================================
\section{Time Series Analysis}\label{sec:time_series_analysis}
% ==============================================================================

%FIXME: should this section be re-categorized ???

Ergodic Processes (\S\ref{sec:ergodic_process})

\fist cf. Statistical Signal Processing (\S\ref{sec:statistical_signal})

Forecasting (\S\ref{sec:forecasting})

Predictive Analytics (\S\ref{sec:predictive_analytics})

\fist Markov Models (\S\ref{sec:markov_model})

Stationary Processes (\S\ref{sec:stationary_process}) -- Stationarity assumption
underlies many procedures in Time-series Analysis

if a Stochastic Process has a Unit Root (\S\ref{sec:unit_root}), i.e. if $1$ is
a Root of the Process's Characteristic Equation (FIXME: characteristic function
???), then the Process is Non-stationary, but does not necessarily have a
``trend''

Hurst Exponent (Index of Long-range Dependence in Time Series
\S\ref{sec:hurst_exponent})



% -----------------------------------------------------------------------------
\subsection{Time Series}\label{sec:time_series}
% -----------------------------------------------------------------------------

Discrete-time Signal (\S\ref{sec:discrete_signal})

Mandelbrot97:

%FIXME: move these notes ???

Model for ``Tail-driven'' variability

Time Series $Z(t)$

$L(t, T) = \ln Z(t+T) - \ln Z(t)$ -- assumed to follow an \emph{L-stable
  Distribution}

when successive $L(t, T)$ are Independent, $\ln Z(t)$ is said to follow an
\emph{L-Stable Motion (LSM)} --FIXME: does ``successive'' mean for fixed $T$ and
successive $t$ ???

Exponent $\alpha \in [0, 2]$ -- in the case of ``Price Series'', usually
$\alpha \in [1, 2]$, but not for certain prices or certain forms of income

Wiener (Brownian) Motion (\S\ref{sec:wiener_process}) is L-Stable Motion with
$\alpha = 2$

generalized for $\alpha > 2$

Model for ``Dependence-driven'' variability



\subsubsection{Order of Integration}\label{sec:order_of_integration}

Summary Statistic reporting the minimum number of ``differences'' required to
obtain a Covariance-stationary Series (\S\ref{sec:covariance_stationary})



% -----------------------------------------------------------------------------
\subsection{Lag Operator}\label{sec:lag_operator}
% -----------------------------------------------------------------------------

or \emph{Backshift Operator}



% -----------------------------------------------------------------------------
\subsection{Lag Polynomial}\label{sec:lag_polynomial}
% -----------------------------------------------------------------------------

a Polynomial of Lag Operators



% -----------------------------------------------------------------------------
\subsection{Correlogram}\label{sec:correlogram}
% -----------------------------------------------------------------------------

Cross-correlogram (\S\ref{sec:cross_correlation})

Autocorrelogram (\S\ref{sec:autocorrelation})



% -----------------------------------------------------------------------------
\subsection{Integrated Model}\label{sec:integrated_model}
% -----------------------------------------------------------------------------

cf. ARIMA



% -----------------------------------------------------------------------------
\subsection{Moving-average Model}\label{sec:moving_average_model}
% -----------------------------------------------------------------------------

or \emph{Moving-average Process}

Moving Average (\S\ref{sec:moving_average})

cf. ARMA, ARIMA

the \emph{Wold Decomposition Theorem} (\S\ref{sec:wold_decomposition}) for
Isometric Linear Operators on a given Hilbert Space implies that any Stationary
Discrete-time Stochastic Process can be \emph{Decomposed} into a pair of
Uncorrelated Processes where one is \emph{Deterministic} and the other is a
\emph{Moving Average Process}



% -----------------------------------------------------------------------------
\subsection{Autoregressive Model}\label{sec:autoregressive_model}
% -----------------------------------------------------------------------------

cf. Regression Models (\S\ref{sec:regression_model})

(wiki): compared to the Autoregressive Model, the Moving-average Model
(\S\ref{sec:moving_average}) is always Stationary and may not contain a Unit
Root (TODO: xrefs)

cf. ARMA, ARIMA

(Mandelbrot63) Invariance under Linear Aggregation
(S\ref{sec:observational_transformation}); cf. Stable Distributions
(\S\ref{sec:stable_distribution}), Central Limit Theorem
(\S\ref{sec:central_limit})

$AR(p)$ of Order $p$

$X_t = c + \sum_{i=1}^p \varphi_i X_{t-i} + \varepsilon_t$

where $\varphi_1, \ldots, \varphi_p$ are \emph{Parameters} of the Model, $c$ is
Constant, and $\varepsilon_t$ is White Noise

can be viewed as the output of an All-pole Infinite Impulse Response Filter with
White Noise input

the Autocorrelation Function (\S\ref{sec:autocorrelation_function}) of an
$AR(p)$ Process is a Sum of decaying Exponentials:
\[
  \rho(\tau) = \sum_{k=1}^p a_k y_k^{-|\tau|}
\]
where $y_k$ are the roots of the Polynomial:
\[
  \phi(B) = 1 - \sum_{k=1}^p \varphi_k B^k
\]
where $B$ is the \emph{Backshift (Lag) Operator} (\S\ref{sec:lag_operator}),
$\phi$ is the Function defining the Autoregression, and $\varphi_k$ are the
Coefficients in the Autoregression;
each Real Root contributes a component to the Autocorrelation Function that
decays Exponentially, and each pair of Complex Conjugate Roots contributes an
Exponentially Damped Oscillation

\fist cf. Recurrent Neural Networks (RNNs \S\ref{sec:rnn})

2018 - Le -
\emph{A Purely Functional Typed Approach to Trainable Models (Part 2)} -
\url{https://blog.jle.im/entry/purely-functional-typed-models-2.html}
-- $AR(2)$ as a stateful model

2018 - Le -
\emph{A Purely Functional Typed Approach to Trainable Models (Part 3)} -
\url{https://blog.jle.im/entry/purely-functional-typed-models-3.html}
-- a general Autoregressive Model $AR(p)$ of any Order $p$ can be defined as a
\emph{Lagged} Fully Connected Layer




\subsubsection{Vector AutoRegression (VAR)}\label{sec:var}

used to capture Linear Interdependencies among multiple Time Series

Vector Moving Average (VMA \S\ref{sec:vma})

cf. Reservoir Computing (\S\ref{sec:reservoir_computing})

2020 - Bollt - \emph{On Explaining the Surprising Success of Reservoir
  Computing: Forecaster of Chaos?}

the ``Universal Machine Learning Dynamical System'' with contrasts to VAR and
Dynamic Mode Decomposition (DMD \S\ref{sec:dmd})



\subsubsection{Non-linear Autoregressive Exogenous Model (NARX)}\label{sec:narx}

\subsubsection{AutoRegressive Conditional Heteroskedasticity (ARCH)}
\label{sec:arch}

(Mandelbrot97E)

``flicker noise'' (1/f, Pink Noise)

concentration of discontinuities in prices series; cf. Multifractal Time
(\S\ref{sec:multifractal_time})

Short-term Dependence



\subsubsection{Generalized AutoRegressive Conditional Heteroskedasticity (GARCH)}
\label{sec:garch}



% -----------------------------------------------------------------------------
\subsection{AutoRegressive Moving Average (ARMA)}\label{sec:arma}
% -----------------------------------------------------------------------------

Weakly Stationary (\S\ref{sec:stationary_process}) Stochastic Process described
by two Polynomials: one for Autoregression and one for Moving Average

Short-term Dependence



% -----------------------------------------------------------------------------
\subsection{AutoRegressive Integrated Moving Average (ARIMA)}\label{sec:arima}
% -----------------------------------------------------------------------------

\subsubsection{AutoRegressive Fractionally Integrated Moving Average (ARFIMA)}
\label{sec:arfima}

Long-range Dependence (\S\ref{sec:long_range_dependence})



% -----------------------------------------------------------------------------
\subsection{Doubly Stochastic Model}\label{sec:doubly_stochastic}
% -----------------------------------------------------------------------------

cf. Compound Probability Distribution (\S\ref{sec:compound_probability})



% -----------------------------------------------------------------------------
\subsection{Model-free Analysis}\label{sec:model_free_analysis}
% -----------------------------------------------------------------------------

Wavelet Transform (\S\ref{sec:wavelet_transform}) methods: Locally Stationary
Wavelets, Wavelet Decomposed Neural Networks



% -----------------------------------------------------------------------------
\subsection{Multiresolution Model}\label{sec:multiresolution_model}
% -----------------------------------------------------------------------------

\subsubsection{Markov Switching MultiFractal (MSMF)}\label{sec:msmf}

%FIXME: move this section ???

a Stochastic Volatility Model

\fist Multifractals (\S\ref{sec:multifractal_system})

\fist Markov Models (\S\ref{sec:markov_model})



% ==============================================================================
\section{Queueing Theory}\label{sec:queueing_theory}
% ==============================================================================

\emph{Little's Law}

Poisson Processes (\S\ref{sec:poisson_process})

1993 - \emph{The Distributional Little's Law and its Applications} - Bertsimas,
Nakazato



% ==============================================================================
\section{Quantum Information Theory}\label{sec:quantum_information_theory}
% ==============================================================================

Quantum Probability Theory (\S\ref{sec:quantum_probability})

Quantum Systems (\S\ref{sec:quantum_system})

Quantum Logic (\S\ref{sec:quantum_logic})

Quantum Channels, Kraus Operators (TODO)

\url{https://golem.ph.utexas.edu/category/2019/02/sporadic_sics_and_exceptional.html}:

\emph{Symmetic Informationally Complete (SIC) Measurements} -- the maximum
number of Equi-angular Lines in a Space of Dimension $d$ (the \emph{Gerzon
  Bound}) is $d^2$ for Complex Vector Spaces and a Set of $d^2$ Equi-angular
Lines in $\comps^d$ specifies a \emph{Measurement} (SIC) that can be performed
on a Quantum-mechanical System;
it is an open problem whether SICs exist for $d$; in a two-level Quantum System
(Qubit), the Antipodal SIC minimize Shannon Entropy (\S\ref{sec:entropy})

\url{https://golem.ph.utexas.edu/category/2019/02/sporadic_sics_and_exceptional_1.html}:

\emph{Mutually Unbiased Bases (MUBs)}



% ------------------------------------------------------------------------------
\subsection{von Neumann Entropy}\label{sec:vonneumann_entropy}
% ------------------------------------------------------------------------------

Density Matrix (\S\ref{sec:density_matrix})

cf. (Shannon) Entropy (\S\ref{sec:entropy})

a Quantum System $A$ and a Purifying System $B$ always have the same Entropy

\emph{Concavity}

Concave Function (\S\ref{sec:concave_function})

Quantum Channels, Kraus Operators (TODO)

Relative Entropy can only go down under a Quantum Channel

Free Energy can only go down under a Quantum Channel that preserves Thermal
Equilibrium (Second Law of Thermodynamics)

Entanglement Entropy -- von Neumann Entropies of a Subsystem
(\url{https://www.youtube.com/watch?v=MN9Li4vE6Qo})



\subsubsection{Conditional Quantum Entropy}
\label{sec:conditional_quantum_entropy}

(Witten18):

note that there is not a true Quantum notion of ``Conditional''

Positivity of Mutual Information (FIXME)

Conditional Entropy (\S\ref{sec:conditional_entropy}) doesn't need to be
Positive as in the classical case (FIXME: clarify)

Araki-Lieb Inequality

\emph{Strong Subadditivity of Entropy} (Positivity of Mutual Information) --
a special case of Monotonicity of Relative Entropy

Positivity of Relative Entropy

Monotonicity of Relative Entropy -- Strong Subadditivity as corollary
