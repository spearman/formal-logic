%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Information Theory}\label{part:information_theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cite{shannon48}

(Video) 2018 - Witten - \emph{Introduction to Information Theory} -
\url{https://video.ias.edu/PiTP/2018/0716-EdwardWitten}

Classical Information Theory (Shannon Theory) -- Ideal Gas Model

Quantum Information Theory -- doesn't have a good analog to defining a
Conditional Probability Distribution (\S\ref{sec:conditional_probability}), but
does have an analog of \emph{Strong Subadditivity of Entropy}

2014 -
\url{https://ldtopology.wordpress.com/2014/05/04/low-dimensional-topology-of-information/} -
\emph{Low dimensional topology of information}



% ==============================================================================
\section{Information}\label{sec:information}
% ==============================================================================

\fist cf. Fisher Information (Statistics \S\ref{sec:fisher_metric})

1965 - Kolmogorov -
\emph{Three Approaches to the Quantitative Definition of Information}:

Combinatorial, Probabilistic, Algorithmic definitions

\emph{Combinatorial}

a Variable $x$ capable of taking Values from a Finite Set $X$ containing $N$
Elements has Entropy (\S\ref{sec:entropy}) $H(x) = \log_2 N$

giving $x$ a definite Value $x = a$ ``removes'' the Entropy and ``Communicates''
(cf. Communication \S\ref{sec:communication}) Information $I = \log_2 N$

\fist cf. \emph{Data} (\S\ref{sec:data}), \emph{(Statistical) Samples}
(\S\ref{sec:sample}) -- conveys Information when viewed ``in context''



% ------------------------------------------------------------------------------
\subsection{Information Content}\label{sec:information_content}
% ------------------------------------------------------------------------------

\emph{Information Content}, \emph{Self-information}, or \emph{Surprisal} of a
Random Variable (\S\ref{sec:random_variable}) or Signal (\S\ref{sec:signal}) is
the amount of Information ``\emph{gained}'' when Sampled (\S\ref{sec:sample});
the Information Content is a Random Variable defined for any Event
(\S\ref{sec:probability_event}) as the Negative Log-probability
(\S\ref{sec:log_probability}) of the Event, whether or not a Random Variable is
being measured. (wiki)

Entropy (\S\ref{sec:entropy}) is the Expected Value
(\S\ref{sec:expected_value}) of the Information Content



% ------------------------------------------------------------------------------
\subsection{Entropy}\label{sec:entropy}
% ------------------------------------------------------------------------------

or \emph{Shannon Entropy}

the Entropy of a Random Variable (\S\ref{sec:random_variable}) or Signal
(\S\ref{sec:signal}) is the Expected Value (\S\ref{sec:expected_value}) of its
Self-information (\S\ref{sec:information_content})

the Entropy of a Distribution (\S\ref{sec:probability_distribution})
is the Mean number of Bits-per-symbols in an Optimal Encoding
(\S\ref{sec:encoding}) --
\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iii_expla.html}

\url{https://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html}
-- ``the \emph{Exponential} of Entropy is like Cardinality''

2011 - Baez, Fritz, Leinster - \emph{A Characterization of Entropy in Terms of
  Information Loss}

\asterism

(Witten18):

Subadditivity (\S\ref{sec:subadditive_function}) of Entropy

\emph{Strong Subadditivity of Entropy} (Positivity of Mutual Information) --
a special case of Monotonicity of Relative Entropy

cf. Quantum Mechanics, Quantum Information Theory
-- doesn't have a good analog to defining a Conditional Probability Distribution
(\S\ref{sec:conditional_probability}), but does have an analog of Strong
Subadditivity of Entropy

\fist von Neumann Entropy (\S\ref{sec:vonneumann_entropy})

\asterism

\emph{Firewalls, AdS/CFT, and the Complexity of States and Unitaries: A Computer
  Science Perspective}
(video lectures)
-
\url{https://video.ias.edu/PiTP/2018/0719-ScottAaronson}

Circuit Complexity (\S\ref{sec:circuit_complexity}) can play a similar role to
Entropy in ``breaking Symmetries''

\asterism

\url{https://golem.ph.utexas.edu/category/2019/02/sporadic_sics_and_exceptional.html}:

Quantum Information Theory (\S\ref{sec:quantum_information}): in a two-level
Quantum System (Qubit), the Antipodal SICs minimize Shannon Entropy



\subsubsection{Binary Entropy}\label{sec:binary_entropy}

Entropy of a Bernoulli Process (\S\ref{sec:bernoulli_process}) with Probability
$p$

the Logit Function (\S\ref{sec:logit}) is the Negative of the Derivative of the
Binary Entropy Function



\subsubsection{Relative Entropy}\label{sec:relative_entropy}

or \emph{Kullback-Liebler Divergence}

\fist Baye's Theorem (\S\ref{sec:bayes_theorem})

Monotonicity of Relative Entropy -- (Positivity of Mutual Information) Strong
Subadditivity of Entropy is a special case

\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iv_a_simp.html}



\paragraph{Mutual Information}\label{sec:mutual_information}\hfill



\subsubsection{Conditional Entropy}\label{sec:conditional_entropy}

\fist Quantum Conditional Entropy (\S\ref{sec:conditional_quantum_entropy}) --
may be Negative; note there is not a ``classical'' sense of Conditioning in
Quantum Information Theory (FIXME: clarify)

\asterism

\emph{Firewalls, AdS/CFT, and the Complexity of States and Unitaries: A Computer
  Science Perspective}
(video lectures)
-
\url{https://video.ias.edu/PiTP/2018/0719-ScottAaronson}

Symmetry of Conditional Entropy (\S\ref{sec:conditional_entropy})



\subsubsection{Cross Entropy}\label{sec:cross_entropy}

measures the average number of Bits needed to identify an Event drawn from an
underlying Set of Events under two Probability Distributions
(\S\ref{sec:probability_distribution}) $p$ and $q$, if the ``coding scheme'' is
optimized for an ``unnatural'' Distribution $q$ rather than a ``true''
Distribution $p$
%FIXME: clarify

Cross-entropy Loss (\S\ref{sec:objective_function})

\emph{Categorical Cross-entropy} -- Loss Function for Statistical Classification
(\S\ref{sec:classification}); cf. Squared Error Loss for Regression Analysis

\fist Cross-entropy Error Function -- minimizing \emph{Negative Log-likelihood}
(\S\ref{sec:log_likelihood}) is the same as minimizing Cross Entropy (cf.
Logistic Regression \S\ref{sec:logistic_regression})



% ------------------------------------------------------------------------------
\subsection{Negentropy}\label{sec:negentropy}
% ------------------------------------------------------------------------------



% ==============================================================================
\section{Channel}\label{sec:channel}
% ==============================================================================

% ------------------------------------------------------------------------------
\subsection{Channel Capacity}\label{sec:channel_capacity}
% ------------------------------------------------------------------------------

\fist Cf. \emph{Controllability} (Control Theory
\S\ref{sec:control_theory})



% ------------------------------------------------------------------------------
\subsection{Channel State Information (CSI)}\label{sec:csi}
% ------------------------------------------------------------------------------

known Channel Properties describing how a Signal propagates and represents the
combined effect of Scattering, Fading, and Power Decay

\emph{Channel Estimation}



% ==============================================================================
\section{Coding Theory}\label{sec:coding_theory}
% ==============================================================================

% ------------------------------------------------------------------------------
\subsection{Hamming Space}\label{sec:hamming_space}
% ------------------------------------------------------------------------------

Set of all $2^N$ Binary Strings of Length $N$

%FIXME:

\url{http://bit-player.org/2018/the-mind-wanders}:

Sparse Distributed Memory (SDM)

(Kanerva88 - \emph{Sparse Distributed Memory})

Content-addressable Memory (or Associative Memory)

a Set of Binary Vectors is closed under \emph{Binding} and \emph{Bundling},
except that the result extracted from a Bundled Vector has to be ``cleaned up''
by Recursive Recall



\subsubsection{Hamming Distance}\label{sec:hamming_distance}

Distance Metric (\S\ref{sec:metric}) for Hamming Space



% ------------------------------------------------------------------------------
\subsection{Encoding}\label{sec:encoding}
% ------------------------------------------------------------------------------

the Entropy (\S\ref{sec:entropy}) of a Distribution
(\S\ref{sec:probability_distribution}) is the Mean number of
Bits-per-symbols in an Optimal Encoding --
\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iii_expla.html}

\fist Autoencoders (\S\ref{sec:autoencoder})



% ==============================================================================
\section{Algorithmic Information Theory}\label{sec:algorithmic_information}
% ==============================================================================

1965 - Komogorov - \emph{Three Approaches to the Quantitative Definition of
  Information}

\emph{Combinatorial approach} -- a Variable $x$ capable of taking Values in a
Finite Set $X$ containing $N$ Elements; Entropy $H(x) = \log_2 N$; Conditional
Entropy $H(y/a) = \log_2 N(Y_a)$; Information conveyed by $x$ with respect to
$y$:
\[
  I(x:y) = H(y) - H(y/x)
\]
(where $H(y/x)$ and $I(x : y)$ are Functions of $x$ and $y$ is a Bound Variable)

Combinatorial approach allows for a ``Universal Coding''
(\S\ref{sec:coding_theory}) method permitting the transmission of any
sufficiently long messages, and it is not necessary to know the Frequencies of
each Symbol beforehand

\emph{Probabilistic approach} -- take $x$ and $y$ as Random Vaiables with given
Joint Probability Distributions (\S\ref{sec:joint_probability}); Entropy
$H_W(x) = -\sum_x p(x) \log_2 p(x)$, Conditional Entropy
$H_W(y/x) = -\sum_y p(y/x) \log_2 p(y/x)$

Inequalities:
\begin{align*}
  H_W(x) \leq H(x) \\
  H_W(y/x) \leq H(y/x)
\end{align*}
hold when the corresponding Distributions on both $X$ and $Y_X$ are Uniform

Information conveyed by $x$ with respect to $y$: $I_W(x:y) = H_W(y) - H_W(y/x)$

Information Content: $I_w(x,y) = MI_W(x : y) = MI_W(y : x)$ where $M$ is the
mathematical \emph{Expectation} (\S\ref{sec:expected_value})

in the Combinatorial approach, $I(x:y)$ is always Non-negative, but in the
Probabilistic approach, $I_W(x:y)$ may be Negative-- only the \emph{averaged}
quantity $I_W(x,y)$ is a measure of the Information Content

Probabilistic approach allows for transmission over Channels carrying ``bulk''
Information consisting of a large number of unrelated (or weakly related)
messages obeying definite Probabilistic laws; empirically Frequencies and
Probabilities are often interchangeable over sufficiently long time sequences

\emph{Algorithmic approach} -- quantitative Information conveyed by an
\emph{individual} object $x$ about an \emph{individual} object $y$ is only
meaningful when the quantity of Information is sufficiently large

Relative Complexity (\S\ref{sec:algorithmic_complexity}) $K_A(y/x)$ and the
quantity of Information are Equivalent in that the difference between them is
bounded

quantity of Information conveyed by $x$ about $y$:
\[
  I_A(x:y) = K_A(y) - K_A(y/x)
\]
is no less than some Negative Constant $C$

(wiki):

(Solomonoff60, Kolmogorov65) \textbf{Thm.}
\emph{Among Algorithms that Decode Strings from their descriptions (Codes),
  there exists an Asymptotically Optimal one.}

statement in (Martin-L\"of66):
\emph{
  There exists an algorithm $A$ such that for any Algorithm $B$:
  \[
    K_A(x) \leq K_B(x) + c
  \]
  where $c$ is a Constant dependent on $A$ and $B$ but not $x$.
}

-- can be used to define several Functions of Strings including Complexity,
Randomness, and Information



% ------------------------------------------------------------------------------
\subsection{Algorithmic Complexity}\label{sec:algorithmic_complexity}
% ------------------------------------------------------------------------------

\emph{Algorithmic Complexity}, \emph{Kolmogorov-Chaitin Complexity}, or
\emph{Descriptive Complexity}

cf. Computational Complexity (\S\ref{sec:complexity_theory})

(Kolmogorov65):

the \emph{Relative Complexity} of an object $y$ with a given $x$
is the minimal length $l(p)$ of the Program $p$ for obtaining $y$ from $x$

a ``Programming Method'' is a Partial Recursive Function $\varphi(p, x) = y$
that associates an object $y$ with a Program $p$ and an object $x$; for any such
Programming Method:
\[
  K_\varphi(y/x) =
  \begin{cases}
    \nexists p : \varphi(p,x) = y & \infty \\
    \text{otherwise}              & min_{\varphi(p, x)=y} l(p)
  \end{cases}
\]

\emph{Asymptotically Optimal} (Komogorov) or \emph{Universal} (Solomonoff)
Algorithm $A$

\textbf{Fundamental Theorem} \emph{
  There exists a Partial Recursive Function $A(p, x)$ such that for any other
  Partial Recursive Function $\varphi(p, x)$:
  \[
    K_A(y/x) \leq K_\varphi(y/x) + C_\varphi
  \]
  where Constant $C_\varphi$ does not depend on $x$ or $y$.
}

-- can be used to define several Functions of Strings including Complexity,
Randomness, and Information

statement in (Martin-L\"of66):
\emph{
  There exists an algorithm $A$ such that for any Algorithm $B$:
  \[
    K_A(x) \leq K_B(x) + c
  \]
  where $c$ is a Constant dependent on $A$ and $B$ but not $x$.
}



% ------------------------------------------------------------------------------
\subsection{Algorithmic Randomness}\label{sec:algorithmic_randomness}
% ------------------------------------------------------------------------------

cf. \emph{Statistical Randomness} (\S\ref{sec:statistical_randomness})

Random Sequence (\S\ref{sec:random_sequence})

1966 - Martin-L\"of - \emph{The Definition of Random Sequences}

Elements of a given large Finite population are \emph{Random} whose Complexity
(\S\ref{sec:algorithmic_complexity}) is \emph{maximal}, and almost all Elements
have a Complexity close to the maximal value

Collectives (Von Mises57); cf. Frequency Interpretation of Probability Theory
(Part \ref{part:probability_theory})

Universal Test (\S\ref{sec:hypothesis_testing})

Critical Level

Infinite Binary Squences -- Universal Sequential Test

Critical Region (\S\ref{sec:critical_region})

Non-random Infinite Binary Random Sequences form a maximal Constructive Null Set
(\S\ref{sec:null_set}), i.e. a Constructive Null Set $\mathcal{A}$ such that any
Constructive Null Set $\mathcal{B}$ is Contained in it:
$\mathcal{B} \subseteq \mathcal{A}$

Random Sequences (\S\ref{sec:random_sequence}) with respect to an arbitrary
Computable Probability Distribution (\S\ref{sec:probability_distribution})

\emph{Subsequence Selection Criterion} --
\emph{Mises-Church Randomness}: any Recursive Function which having read the
first $N$ elements of the Sequence decides if it wants to select element $N+1$

Bernoulli Sequences (\S\ref{sec:bernoulli_sequence}); Computable ``Success
Probability''

``Irregularity Condition''



% ------------------------------------------------------------------------------
\subsection{Algorithmic Probability}\label{sec:algorithmic_probability}
% ------------------------------------------------------------------------------

or \emph{Solomonoff Probability}

\fist Probability (\S\ref{sec:probability})

Universal Prior

Occam's Razor, Epicurus' Principle



% ==============================================================================
\section{Signal Processing}\label{sec:signal_processing}
% ==============================================================================

% ------------------------------------------------------------------------------
\subsection{Signal}\label{sec:signal}
% ------------------------------------------------------------------------------

\fist Fourier Transform (\S\ref{sec:fourier_transform})

cf. \emph{Wave} (\S\ref{sec:wave})

(wiki):

a \emph{Periodic Signal} has a \emph{Discrete Frequency Spectrum} -- a Periodic
Signal can be analyzed using a \emph{Discrete Frequency Domain}, i.e. the
Fourier Transform of a Periodic Signal only has energy at a \emph{Base
  Frequency} and its \emph{Harmonics}

a \emph{Discrete-time Signal} has a \emph{Periodic Frequency Spectrum}

a \emph{Discrete, Periodic Signal} has a \emph{Periodic and Discrete Frequency
  Spectrum}

The \emph{Information Content} (\emph{Self-information} or \emph{Surprisal}
\S\ref{sec:information_content}) of a Sampled (\S\ref{sec:sample}) Random
Variable or Signal is the amount of Information (\S\ref{sec:information})
``gained'' by the Sample; this Information Content is a Random Variable defined
for any Event regardless of whether a Random Variable is being measured or not.
(wiki)



\subsubsection{Transient}\label{sec:transient}

\subsubsection{Continuous Signal}\label{sec:continuous_signal}

\subsubsection{Discrete Signal}\label{sec:discrete_signal}

\subsubsection{Sampling Frequency}\label{sec:sampling_frequency}

reduction of a Continuous-time Signal to a Discrete-time Signal

\emph{Nyquist Frequency} -- half of the Sampling Rate



\subsubsection{Quasiperiodic Signal}\label{sec:quasiperiodic_signal}

cf. Almost Periodic Function (\S\ref{sec:almost_periodic})



\subsubsection{Energy}\label{sec:energy}

\url{https://www.gaussianwaves.com/2013/12/power-and-energy-of-a-signal/}:

the \emph{Energy} of a Continuous-time Complex Signal $x(t)$ is defined as:
\[
  E_x = \int_{-\infty}^\infty |x(t)|^2 dt
\]

Discrete-time:
\[
  E_x = \sum_{n \in \ints} |x(n)|^2
\]



\subsubsection{Power}\label{sec:power}

Signal $x(t)$

\emph{Instantaneous Power} $p(t) = |s(t)|^2$

\url{https://www.gaussianwaves.com/2013/12/power-and-energy-of-a-signal/}:

Discrete-time:
\[
  P_x = \lim_{N\to\infty} \frac{1}{2N+1} \sum_{n \in [-N,N]}|x(n)|^2
\]

Total Power (FIXME)

Peak Power (FIXME)

a Finite-energy Signal has Zero Total Power

a Signal with Finite, Non-zero Total Power has Infinite Energy



% ------------------------------------------------------------------------------
\subsection{Time Domain}\label{sec:time_domain}
% ------------------------------------------------------------------------------

Waveforms (\S\ref{sec:waveform}) -- Time-domain Graph of a Variable; the form or
``shape'' of $F$ is D'Alembert's Formula (Wave Equations
\S\ref{sec:wave_equation})

Laplace Transform (\S\ref{sec:laplace_transform}) from Time Domain to Frequency
Domain transforms Differential Equations (\S\ref{sec:differential_equation})
into Algebraic (Polynomial) Equations (\S\ref{sec:polynomial_equation}), and
Convolution (\S\ref{sec:convolution}) into Multiplication

\fist Time-series Analysis (\S\ref{sec:time_series_analysis})



\subsubsection{Waveform}\label{sec:waveform}

Time-domain Graph of a Variable

the form or \emph{shape} of $F$ is D'Alembert's Formula

\begin{itemize}
  \item Sinusoid (\S\ref{sec:sinusoid}) -- Waveform of a Sine Function
    (\S\ref{sec:trigonometric_function})
  \item ...
\end{itemize}



\subsubsection{Jitter}\label{sec:jitter}

Phase Noise (\S\ref{sec:phase_noise})



% ------------------------------------------------------------------------------
\subsection{Frequency Domain}\label{sec:frequency_domain}
% ------------------------------------------------------------------------------

Frequency (\S\ref{sec:frequency}), Wavenumber (\S\ref{sec:wavenumber})

\begin{itemize}
  \item Fourier Series (\S\ref{sec:fourier_series}) -- Periodic Signals,
    Oscillating Systems
  \item Fourier Transform (\S\ref{sec:fourier_transform}) -- Aperiodic Signals,
    Transients (\S\ref{sec:transient})
  \item Laplace Transform (\S\ref{sec:laplace_transform}) -- Control Systems
    (\S\ref{sec:control_system})
  \item Z-Transform (\S\ref{sec:z_transform}) -- Discrete-time Laplace
    Transform, DSP (\S\ref{sec:dsp}
  \item Wavelet Transform (\S\ref{sec:wavelet_transform}) -- Image Processing,
    Data Compression
\end{itemize}

Laplace Transform (\S\ref{sec:laplace_transform}) from Time Domain to Frequency
Domain transforms Differential Equations (\S\ref{sec:differential_equation})
into Algebraic (Polynomial) Equations (\S\ref{sec:polynomial_equation}), and
Convolution (\S\ref{sec:convolution}) into Multiplication

\fist Frequency Domain Analysis (Statistical Signal Processing
\S\ref{sec:spectrum_analysis})



\subsubsection{Frequency Spectrum}\label{sec:frequency_spectrum}

\fist Frequency Estimation (Spectrum Analysis \S\ref{sec:frequency_estimation})

note that a Signal may have a ``White'' Frequency Spectrum but still be Serially
Dependent (Mandelbrot97)



\subsubsection{Phase Noise}\label{sec:phase_noise}

Frequency-domain representation of random fluctuations in Phase, corresponding
to Time-domain Jitter (\S\ref{sec:jitter})



% ------------------------------------------------------------------------------
\subsection{Noise}\label{sec:noise}
% ------------------------------------------------------------------------------

a \emph{Noise Signal} is a Signal produced by a Stochastic Process
(\S\ref{sec:stochastic_process})

cf. ``Statistical Noise'': Fraction of Variance Unexplained (Regression Analysis
\S\ref{sec:fvu})

cf. Phase Noise (\S\ref{sec:phase_noise})



\subsubsection{Gaussian Noise}\label{sec:gaussian_noise}

\subsubsection{White Noise}\label{sec:white_noise}

\paragraph{Additive White Gaussian Noise (AWGN)}\label{sec:awgn}\hfill



\subsubsection{Pink Noise}\label{sec:white_noise}

\subsubsection{Brownian Noise}\label{sec:brownian_noise}

\emph{Brown Noise} or \emph{Red Noise}



% ------------------------------------------------------------------------------
\subsection{Filter}\label{sec:signal_filter}
% ------------------------------------------------------------------------------

\subsubsection{Adaptive Filter}\label{sec:adaptive_filter}

Optimization



\paragraph{Least Mean Squares (LMS)}\label{sec:lms}\hfill

Stochastic Gradient Descent (SGD \S\ref{sec:sgd}) method

cf. Backpropagation (\S\ref{sec:backpropagation})



% ------------------------------------------------------------------------------
\subsection{Wavelet}\label{sec:wavelet}
% ------------------------------------------------------------------------------

\subsubsection{Mother Wavelet}\label{sec:mother_transform}

\subsubsection{Scaling Function}\label{sec:scaling_function}

or \emph{Father Wavelet}



\subsubsection{Wavelet Transform}\label{sec:wavelet_transform}

Frequency Domain (\S\ref{sec:frequency_domain})

cf. Fourier Transform (\S\ref{sec:fourier_transform})

Multiresolution Analysis (\S\ref{sec:multiresolution_analysis})

Orthonormal Wavelet Basis; Haar (\S\ref{sec:haar_wavelet}) Basis

Model-free Time Series Analysis (\S\ref{sec:model_free_analysis}) -- Locally
Stationary Wavelets, Wavelet Decomposed Neural Networks



\paragraph{Discrete Wavelet Transform}
\label{sec:discrete_wavelet_transform}\hfill

\paragraph{Fast Wavelet Transform (FWT)}\label{sec:fwt}\hfill



\subsubsection{Haar Wavelet}\label{sec:haar_wavelet}

(Wasserman04, \S21.4) Haar Wavelet Regression, Haar Wavelet Density Estimation

Haar Basis



% ------------------------------------------------------------------------------
\subsection{Coherence}\label{sec:signal_coherence}
% ------------------------------------------------------------------------------

(wiki):

Statistic (\S\ref{sec:statistic}) used to relate two Signals (or Datasets)

used to Estimate power transfer between input and output of a Linear System; if
the Signals are Ergodic and the System (Transfer) Function is Linear, then
Coherence can be used to Estimate the \emph{Causality} between input and output



% ------------------------------------------------------------------------------
\subsection{Compressed Sensing}\label{sec:compressed_sensing}
% ------------------------------------------------------------------------------

or \emph{Sparse Sampling}



% ------------------------------------------------------------------------------
\subsection{MultiResolution Analysis (MRA)}\label{sec:multiresolution_analysis}
% ------------------------------------------------------------------------------

\emph{MultiScale Approximation} (MSA)

Wavelet Transforms (\S\ref{sec:wavelet_transform})

Orthonormal Wavelet Basis



% ------------------------------------------------------------------------------
\subsection{Digital Signal Processing (DSP)}\label{sec:dsp}
% ------------------------------------------------------------------------------

\fist Z-transform (Discrete-time Laplace Transform \S\ref{sec:z_transform})



% ------------------------------------------------------------------------------
\subsection{Image Processing}\label{sec:image_processing}
% ------------------------------------------------------------------------------

Convolution (\S\ref{sec:convolution})

Point Spread Function -- Convolution by a Gaussian ``spreading'' function



% ------------------------------------------------------------------------------
\subsection{Time-Frequency Analysis}\label{sec:time_frequency_analysis}
% ------------------------------------------------------------------------------

techniques for characterizing and manipulting Signals with Statistics (cf.
Entropy \S\ref{sec:entropy}) that vary in Time, e.g. \emph{Transient Signals}

\fist Quasiprobability Distributions
(\S\ref{sec:quasiprobability_distribution})



\subsection{Time-Frequency Representation}
\label{sec:time_frequency_representation}



% ------------------------------------------------------------------------------
\subsection{Impulse Response}\label{sec:impulse_response}
% ------------------------------------------------------------------------------

Green's Function (\S\ref{sec:greens_function})

Linear Time-Invariant (LTI) System (\S\ref{sec:lti_system})

the Impulse Response is equivalent to the Inverse Laplace Transform
(\S\ref{sec:laplace_transform}) of a Dynamical System's Transfer Function
(Evolution Function \S\ref{sec:evolution_function})



\subsubsection{Linear Response Function}\label{sec:linear_response}



% ------------------------------------------------------------------------------
\subsection{Statistical Signal Processing}\label{sec:statistical_signal}
% ------------------------------------------------------------------------------

treats Signals as \emph{Stochastic Processes} (\S\ref{sec:stochastic_process})

\fist cf. Time-series Analysis (\S\ref{sec:time_series_analysis}) of Stationary
and Ergodic Processes



\subsubsection{Window Function}\label{sec:window_function}

or \emph{Apodization Function} or \emph{Tapering Function}

\fist Kernel (Statistics \S\ref{sec:distribution_kernel})



\paragraph{Hamming Window}\label{sec:hamming_window}\hfill



\subsubsection{Spectrum Analysis}\label{sec:spectrum_analysis}

or \emph{Spectral Density Estimation (SDE)} or \emph{Frequency Domain Analysis}

Periodic Functions (\S\ref{sec:periodic_function})

Fourier Analysis (\S\ref{sec:fourier_analysis})

note that a Signal may have a ``White'' Frequency Spectrum but still be Serially
Dependent (Mandelbrot97)



\paragraph{Periodogram}\label{sec:periodogram}\hfill

Spectral Density Estimator

cf. Histogram (\S\ref{sec:histogram})



\paragraph{Spectrogram}\label{sec:spectrogram}\hfill

a Sequence of Periodograms over Time



\paragraph{Frequency Estimation}\label{sec:frequency_estimation}\hfill

Estimation of Complex Frequency Components of a Signal in the presence of Noise
given \emph{assumptions} about the number fo the Components



\subparagraph{Sinusoidal Model}\label{sec:sinusoidal_model}\hfill



\paragraph{Energy Spectral Density (ESD)}\label{sec:esd}\hfill

\paragraph{Power Spectral Density (PSD)}\label{sec:psd}\hfill

or \emph{Power Spectral Density (PSD)}



% ==============================================================================
\section{Queueing Theory}\label{sec:queueing_theory}
% ==============================================================================

\emph{Little's Law}

Poisson Processes (\S\ref{sec:poisson_process})

1993 - \emph{The Distributional Little's Law and its Applications} - Bertsimas,
Nakazato



% ==============================================================================
\section{Quantum Information Theory}\label{sec:quantum_information_theory}
% ==============================================================================

Quantum Probability Theory (\S\ref{sec:quantum_probability})

Quantum Systems (\S\ref{sec:quantum_system})

Quantum Logic (\S\ref{sec:quantum_logic})

Quantum Channels, Kraus Operators (TODO)

\url{https://golem.ph.utexas.edu/category/2019/02/sporadic_sics_and_exceptional.html}:

\emph{Symmetic Informationally Complete (SIC) Measurements} -- the maximum
number of Equi-angular Lines in a Space of Dimension $d$ (the \emph{Gerzon
  Bound}) is $d^2$ for Complex Vector Spaces and a Set of $d^2$ Equi-angular
Lines in $\comps^d$ specifies a \emph{Measurement} (SIC) that can be performed
on a Quantum-mechanical System;
it is an open problem whether SICs exist for $d$; in a two-level Quantum System
(Qubit), the Antipodal SIC minimize Shannon Entropy (\S\ref{sec:entropy})

\url{https://golem.ph.utexas.edu/category/2019/02/sporadic_sics_and_exceptional_1.html}:

\emph{Mutually Unbiased Bases (MUBs)}



% ------------------------------------------------------------------------------
\subsection{von Neumann Entropy}\label{sec:vonneumann_entropy}
% ------------------------------------------------------------------------------

Density Matrix (\S\ref{sec:density_matrix})

cf. (Shannon) Entropy (\S\ref{sec:entropy})

a Quantum System $A$ and a Purifying System $B$ always have the same Entropy

\emph{Concavity}

Concave Function (\S\ref{sec:concave_function})

Quantum Channels, Kraus Operators (TODO)

Relative Entropy can only go down under a Quantum Channel

Free Energy can only go down under a Quantum Channel that preserves Thermal
Equilibrium (Second Law of Thermodynamics)

Entanglement Entropy -- von Neumann Entropies of a Subsystem
(\url{https://www.youtube.com/watch?v=MN9Li4vE6Qo})



\subsubsection{Conditional Quantum Entropy}
\label{sec:conditional_quantum_entropy}

(Witten18):

note that there is not a true Quantum notion of ``Conditional''

Positivity of Mutual Information (FIXME)

Conditional Entropy (\S\ref{sec:conditional_entropy}) doesn't need to be
Positive as in the classical case (FIXME: clarify)

Araki-Lieb Inequality

\emph{Strong Subadditivity of Entropy} (Positivity of Mutual Information) --
a special case of Monotonicity of Relative Entropy

Positivity of Relative Entropy

Monotonicity of Relative Entropy -- Strong Subadditivity as corollary
