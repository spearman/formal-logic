%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Information Theory}\label{part:information_theory} \cite{shannon48}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

(Video) 2018 - Witten - \emph{Introduction to Information Theory} -
\url{https://video.ias.edu/PiTP/2018/0716-EdwardWitten}

Classical Information Theory (Shannon Theory) -- Ideal Gas Model

Quantum Information Theory -- doesn't have a good analog to defining a
Conditional Probability Distribution (\S\ref{sec:conditional_probability}), but
does have an analog of \emph{Strong Subadditivity of Entropy}



% ====================================================================
\section{Channel}\label{sec:channel}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Channel Capacity}\label{sec:channel_capacity}
% --------------------------------------------------------------------

\fist Cf. \emph{Controllability} (Control Theory
\S\ref{sec:control_theory})



% ====================================================================
\section{Coding Theory}\label{sec:coding_theory}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Hamming Space}\label{sec:hamming_space}
% --------------------------------------------------------------------

Set of all $2^N$ Binary Strings of Length $N$

%FIXME:

\url{http://bit-player.org/2018/the-mind-wanders}:

Sparse Distributed Memory (SDM)

(Kanerva88 - \emph{Sparse Distributed Memory})

Content-addressable Memory (or Associative Memory)

a Set of Binary Vectors is closed under \emph{Binding} and \emph{Bundling},
except that the result extracted from a Bundled Vector has to be ``cleaned up''
by Recursive Recall



\subsubsection{Hamming Distance}\label{sec:hamming_distance}

Distance Metric (\S\ref{sec:metric}) for Hamming Space



% --------------------------------------------------------------------
\subsection{Encoding}\label{sec:encoding}
% --------------------------------------------------------------------

the Entropy (\S\ref{sec:entropy}) of a Distribution
(\S\ref{sec:probability_distribution}) is the Mean number of
Bits-per-symbols in an Optimal Encoding --
\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iii_expla.html}



% ====================================================================
\section{Entropy}\label{sec:entropy}
% ====================================================================

or \emph{Shannon Entropy}

the Entropy of a Distribution (\S\ref{sec:probability_distribution})
is the Mean number of Bits-per-symbols in an Optimal Encoding
(\S\ref{sec:encoding}) --
\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iii_expla.html}

\url{https://golem.ph.utexas.edu/category/2008/10/entropy_diversity_and_cardinal.html}
-- ``the \emph{Exponential} of Entropy is like Cardinality''

2011 - Baez, Fritz, Leinster - \emph{A Characterization of Entropy in Terms of
  Information Loss}

\asterism

(Witten18):

Subadditivity (\S\ref{sec:subadditive_function}) of Entropy

\emph{Strong Subadditivity of Entropy} (Positivity of Mutual Information) --
a special case of Monotonicity of Relative Entropy

cf. Quantum Mechanics, Quantum Information Theory
-- doesn't have a good analog to defining a Conditional Probability Distribution
(\S\ref{sec:conditional_probability}), but does have an analog of Strong
Subadditivity of Entropy

\fist von Neumann Entropy (\S\ref{sec:vonneumann_entropy})

\asterism

\emph{Firewalls, AdS/CFT, and the Complexity of States and Unitaries: A Computer
  Science Perspective}
(video lectures)
-
\url{https://video.ias.edu/PiTP/2018/0719-ScottAaronson}

Circuit Complexity (\S\ref{sec:circuit_complexity}) can play a similar role to
Entropy in ``breaking Symmetries''



% --------------------------------------------------------------------
\subsection{Relative Entropy}\label{sec:relative_entropy}
% --------------------------------------------------------------------

or \emph{Kullback-Liebler Divergence}

Monotonicity of Relative Entropy -- (Positivity of Mutual Information) Strong
Subadditivity of Entropy is a special case

\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iv_a_simp.html}



\subsubsection{Mutual Information}\label{sec:mutual_information}



% --------------------------------------------------------------------
\subsection{Conditional Entropy}\label{sec:conditional_entropy}
% --------------------------------------------------------------------

\fist Quantum Conditional Entropy (\S\ref{sec:conditional_quantum_entropy}) --
may be Negative; note there is not a ``classical'' sense of Conditioning in
Quantum Information Theory (FIXME: clarify)

\asterism

\emph{Firewalls, AdS/CFT, and the Complexity of States and Unitaries: A Computer
  Science Perspective}
(video lectures)
-
\url{https://video.ias.edu/PiTP/2018/0719-ScottAaronson}

Symmetry of Conditional Entropy (\S\ref{sec:conditional_entropy})



% --------------------------------------------------------------------
\subsection{Cross Entropy}\label{sec:cross_entropy}
% --------------------------------------------------------------------

measures the average number of Bits needed to identify an Event drawn from an
underlying Set of Events under two Probability Distributions
(\S\ref{sec:probability_distribution}) $p$ and $q$, if the ``coding scheme'' is
optimized for an ``unnatural'' Distribution $q$ rather than a ``true''
Distribution $p$
%FIXME: clarify



% ====================================================================
\section{Signal Processing}\label{sec:signal_processing}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Signal}\label{sec:signal}
% --------------------------------------------------------------------

\fist Fourier Transform (\S\ref{sec:fourier_transform})

(wiki):

a \emph{Periodic Signal} has a \emph{Discrete Frequency Spectrum} -- a Periodic
Signal can be analyzed using a \emph{Discrete Frequency Domain}, i.e. the
Fourier Transform of a Periodic Signal only has energy at a \emph{Base
  Frequency} and its \emph{Harmonics}

a \emph{Discrete-time Signal} has a \emph{Periodic Frequency Spectrum}

a \emph{Discrete, Periodic Signal} has a \emph{Periodic and Discrete Frequency
  Spectrum}



\subsubsection{Transient}\label{sec:transient}



% --------------------------------------------------------------------
\subsection{Time Domain}\label{sec:time_domain}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Frequency Domain}\label{sec:frequency_domain}
% --------------------------------------------------------------------

\begin{itemize}
  \item Fourier Series (\S\ref{sec:fourier_series}) -- Periodic Signals,
    Oscillating Systems
  \item Fourier Transform (\S\ref{sec:fourier_transform}) -- Aperiodic Signals,
    Transients (\S\ref{sec:transient})
  \item Laplace Transform (\S\ref{sec:laplace_transform}) -- Control Systems
    (\S\ref{sec:control_system})
  \item Z-Transform (\S\ref{sec:z_transform}) -- Discrete-time Laplace
    Transform, DSP (\S\ref{sec:dsp}
  \item Wavelet Transform (\S\ref{sec:wavelet_transform}) -- Image Processing,
    Data Compression
\end{itemize}



% --------------------------------------------------------------------
\subsection{Digital Signal Processing (DSP)}\label{sec:dsp}
% --------------------------------------------------------------------

\fist Z-transform (Discrete-time Laplace Transform \S\ref{sec:z_transform})



% --------------------------------------------------------------------
\subsection{Image Processing}\label{sec:image_processing}
% --------------------------------------------------------------------

\subsubsection{Wavelet Transform}\label{sec:wavelet_transform}

Frequency Domain (\S\ref{sec:frequency_domain})



% --------------------------------------------------------------------
\subsection{Time-Frequency Analysis}\label{sec:time_frequency_analysis}
% --------------------------------------------------------------------

techniques for characterizing and manipulting Signals with Statistics (cf.
Entropy \S\ref{sec:entropy}) that vary in Time, e.g. \emph{Transient Signals}

\fist Quasiprobability Distributions
(\S\ref{sec:quasiprobability_distribution})



\subsection{Time-Frequency Representation}
\label{sec:time_frequency_representation}



% ====================================================================
\section{Quantum Information Theory}\label{sec:quantum_information_theory}
% ====================================================================

Quantum Systems (\S\ref{sec:quantum_system})

Quantum Channels, Kraus Operators (TODO)



% --------------------------------------------------------------------
\subsection{von Neumann Entropy}\label{sec:vonneumann_entropy}
% --------------------------------------------------------------------

Density Matrix (\S\ref{sec:density_matrix})

cf. (Shannon) Entropy (\S\ref{sec:entropy})

a Quantum System $A$ and a Purifying System $B$ always have the same Entropy

\emph{Concavity}

Concave Function (\S\ref{sec:concave_function})

Quantum Channels, Kraus Operators (TODO)

Relative Entropy can only go down under a Quantum Channel

Free Energy can only go down under a Quantum Channel that preserves Thermal
Equilibrium (Second Law of Thermodynamics)

Entanglement Entropy -- von Neumann Entropies of a Subsystem
(\url{https://www.youtube.com/watch?v=MN9Li4vE6Qo})



\subsubsection{Conditional Quantum Entropy}
\label{sec:conditional_quantum_entropy}

(Witten18):

note that there is not a true Quantum notion of ``Conditional''

Positivity of Mutual Information (FIXME)

Conditional Entropy (\S\ref{sec:conditional_entropy}) doesn't need to be
Positive as in the classical case (FIXME: clarify)

Araki-Lieb Inequality

\emph{Strong Subadditivity of Entropy} (Positivity of Mutual Information) --
a special case of Monotonicity of Relative Entropy

Positivity of Relative Entropy

Monotonicity of Relative Entropy -- Strong Subadditivity as corollary
