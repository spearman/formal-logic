%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Formal Language Theory}\label{sec:formal_language_theory} \cite{hammel03}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Formal Language Theory} is the study of \emph{Formal Languages}
(\S\ref{sec:formal_language}) and their finite representations,
\emph{Automata} (\S\ref{sec:automata_theory}). These topics are
closely related to questions of \emph{Decidability}
(\S\ref{sec:computable_function}), an important concept in
\emph{Recursion Theory} (Part \ref{sec:recursion_theory}).



% ====================================================================
\section{Formal Language}\label{sec:formal_language}
% ====================================================================

A \emph{Formal Language}, $L$, is a possibly infinite Subset of an
infinite \emph{Vocabulary}, $\Sigma^*$, that is the Set of all
possible finite string \emph{Expressions} (or \emph{Words}) over a
possibly infinite \emph{Alphabet} of \emph{Symbols}, $\Sigma$. This
Set $\Sigma^*$ is the \emph{Kleene star} or \emph{Free Monoid}
(\S\ref{sec:monoid}) of $\Sigma$; the smallest Superset of $\Sigma$
that is closed under string concatenation.

The entire content of a Language is uniquely determined by the Set of
all \emph{Terminal Expressions} that are \emph{Generated} by the
\emph{Production} or \emph{Rewrite Rules} of a \emph{Generative
  Grammer} (\S\ref{sec:generative_grammar}) or the Set of
Expressions \emph{Recognized} by a \emph{Determinative Grammar}
(\S\ref{sec:determinative_grammar}). This possibly infinite Set of
Terminal Expressions will be a Subset of $\Sigma^*$.

For two Languages $L1$ and $L2$ over a common Alphabet $\Sigma$:
\begin{itemize}
    \item $L1 \cup L2$ is a Language (Set Union)
    \item $L1 \cap L2$ is a Language (Set Intersection)
    \item $L1 - L2$ is a Language (Set Difference)
    \item $(L1 - L2 \cup L2 - L1)$ is a Language (Symmetric Difference)
    \item $L1 \times L2$ is a Language (Cartesian Product)
\end{itemize}
If $L2 \subset L1$ then $L2$ is a \emph{Sublanguage} of $L1$.

For a given Language $L \subseteq \Sigma^*$ there exists a
\emph{Complement Language} $L^C = \Sigma^* - L$.



% ====================================================================
\section{Formal Grammar}\label{sec:formal_grammar}
% ====================================================================

The term \emph{Syntax} may be used as a synonym for \emph{Grammar}:
that is a Set of Rules through which the strings of a Language can be
either \emph{Generated} (\S\ref{sec:generative_grammar}) or
\emph{Recognized} (\S\ref{sec:determinative_grammar}).

Syntax is the aspect of Formal Languages that refers only to the
literal strings of Symbols of a Language with no regard to the meaning
or \emph{Interpretation} (\S\ref{sec:interpretation}); only the
condition that they can be identified and differentiated from
one-another is required. The process of \emph{Syntactic Analysis} is
known as \emph{Parsing} (\S\ref{sec:parser}).

Syntactic methods can be carried very far; all of Formal Language
Theory as well as Proof Theory (Part \ref{sec:proof_theory}) are
carried out Syntactically.

Within a Formal Grammar the Symbols will be divided into two disjoint
subsets according to whether they are \emph{Terminal} or
\emph{Non-terminal Symbols}.

The definition of a Non-terminal Symbol in the context of Generative
Grammars is one for which a Production Rule exists with that Symbol
appearing in the input and replaced in the output.

\begin{description}

    \item[Symbol] \hfill \\
    an atomic unit of a Language

    \item[Alphabet ($\Sigma$)] \hfill \\
    a possibly infinite set of Symbols

    \item[Expression] \hfill \\
    a finite string of Symbols

    \item[Vocabulary ($\Sigma^{*}$)] \hfill \\
    set of all Expressions over an Alphabet of Symbols

    \item[Production] \hfill \\
    a Rewrite Rule specifying a Non-terminal Symbol substitution

    \item[Generative Grammar] \hfill \\
    a finite set of Productions over the Expressions of a Vocabulary

\end{description}
Two special Symbols are recognized:
\begin{description}

    \item[Empty Symbol ($\varepsilon$)] \hfill \\
    the Symbol of zero length and a Terminal Symbol

    \item[Start Symbol ($S$)] \hfill \\
    a unique Non-terminal Symbol

\end{description}



% --------------------------------------------------------------------
\subsection{Generative Grammar}\label{sec:generative_grammar}
% --------------------------------------------------------------------

A \emph{Generative Grammar} \emph{Generates} a Language by the
repeated application of its Production Rules beginning with the Start
Symbol. A sequence of rule applications is a \emph{Derivation} (cf.
\S\ref{sec:formal_proof} and \S\ref{sec:logical_argument}). A Grammar
may be formally defined as a 4-tuple:
\[
    G(N,T,P,S)
\]
where $N$ are Non-terminal Symbols, $T$ are Terminal Symbols, $P$ are
Production Rules and $S$ is the Start Symbol.

An unrestricted Production Rule has the form:
\[
    (N \cup T)^*N(N \cup T)^* \rightarrow (N \cup T)^*
\]
That is, a Production is a function from one Expression to
another, where the left Expression must contain at least one
Non-terminal Symbol. By convention, Non-terminal Symbols
will be denoted by capitals ($A,B,C,\cdots$), and Terminals by
lowercase ($a,b,c,\cdots$), and Expressions by Greek letters
($\alpha,\beta,\gamma$). Let:

\[
    \mathcal{A} = \{ Alphabets \},\: \mathcal{V} = \{ Vocabularies \}
\] \[
    \mathcal{G} = \{ Grammars \},\: \mathcal{L} = \{ Languages \}
\]

\begin{itemize}

\item Definition of the Kleene star over an Alphabet where $\circ$ is
  the operation to \emph{Concatenate} two Expressions:
\[
    \forall \: \Sigma \in \mathcal{A} \:
    \exists \: \Sigma^* \in \mathcal{V}
    : \Sigma^* = \bigcup_{i=0}^{|\Sigma|} \Sigma_i
    = (\Sigma,\circ)
\]

\item Definition of a Language in terms of a Vocabulary:
\[
    \forall \: L \in \mathcal{L} \:
    \exists \: \Sigma^* \in \mathcal{V}
    : L \subseteq \Sigma^*
\]

\item Existence of the Empty Symbol, $\varepsilon$:
\[
    \forall \: \Sigma^* \in \mathcal{V} \:
    \exists ! \: \varepsilon \in \Sigma^*
    : |\varepsilon|=0
\]

\end{itemize}



% --------------------------------------------------------------------
\subsection{Determinative Grammar}\label{sec:determinative_grammar}
% --------------------------------------------------------------------

A \emph{Determinative Grammar} is a Grammar through which a Member of
$\Sigma^*$ can be determined to belong to a Language $L \subseteq
\Sigma^*$ (\emph{Recognized}). Such systems are described by
\emph{Automata Theory} (\S\ref{sec:automata_theory})



% --------------------------------------------------------------------
\subsection{Chomsky Hierarchy}\label{sec:chomsky_hierarchy}\cite{chomsky56}
% --------------------------------------------------------------------

Grammars are classified by how restrictive the Production Rules are.
By convention, they may be organized into a hierarchy of Classes under
Proper Inclusion, where \emph{Type-0} is an Unrestricted Grammar,
covering all possible Formal Grammars.
\[
    Type-0 \supset Type-1 \supset Type-2 \supset Type-3
\]
These different levels in the hierarchy are \emph{Recognizable} by
different kinds of \emph{Automata} (\S\ref{sec:automata}).



% --------------------------------------------------------------------
\subsection{Type-0: Unrestricted Grammar}\label{sec:unrestricted_grammar}
% --------------------------------------------------------------------

\subsubsection{Semi-decidable}\label{sec:semidecidable}

Production Rules of an \emph{Unrestricted} Grammar have the form
\[
    \alpha \rightarrow \beta
\]
where $\alpha$ and $\beta$ are Expressions of $N \cup T$ and $\alpha
\neq \varepsilon$. Note that this means there is not necessarily a
distinction between Terminal and Non-terminal Symbols in an
Unrestricted Grammar.

A completely Unrestricted Grammar is called \emph{Recursively
  Enumerable} or \emph{Semi-decidable}
(\S\ref{sec:partial_recursive}). This means membership of the
Language can be decided by an Algorithm, but non-membership cannot,
and the Class of Languages having this property is called
$\mathsf{RE}$.

The complement of $\mathsf{RE}$ is the Class of Languages for which an
Algorithm may decide non-membership only and is termed
$\mathsf{coRE}$. The class of Automata capable of implementing these
Algorithms are \emph{Turing Machines}(\S\ref{sec:turing_machine}).



\subsubsection{Decidable}\label{sec:decidable_language}

A \emph{Decidable} or \emph{Recursive} Language is defined as the
intersection of $\mathsf{RE}$ and $\mathsf{coRE}$:
\[
    \mathsf{R} = \mathsf{RE} \cap \mathsf{coRE}
\]
That is, it can be decided whether a Symbol is a member or not by a
\emph{Total Computable Function} (\S\ref{sec:computable_function}).
Decidable Languages are Recognizable by a \emph{Decider} or
\emph{Total Turing Machine}\cite{kozen97} (however determining whether
an arbitrary Turing Machine gives an answer for every input is an
Undecidable Decision Problem).



% --------------------------------------------------------------------
\subsection{Type-1: Context-sensitive Grammar}\label{sec:context_sensitive}
% --------------------------------------------------------------------

\emph{Context-sensitive Grammars} have the restriction that the result
of a Production is not shorter than the input. Formally stated,
Productions are of the form
\[
    \alpha \Gamma \beta \rightarrow \alpha \gamma \beta
\]
where $|\Gamma| \leq |\gamma|$. In this formulation $\alpha$ and
$\beta$ form the \emph{Context} of $\Gamma$.

Requiring that $S$ does not appear on the right of any Production
and allowing the rule
\[
    S \rightarrow \varepsilon
\]
makes the Context-sensitive Languages a proper Superset of the
\emph{Context-free Languages} (\S\ref{sec:context_free}).

Context-sensitive Languages are equivalent to \emph{Linear
Bounded Automata} (\S\ref{sec:linear_bounded_automata}).



\subsubsection{Indexed Grammar}\label{subsubsection:indexed_grammar}

An \emph{Indexed Grammar} has an extra set of \emph{Index Symbols},
$F$, with Productions of three possible forms,
\[
    A[\sigma] \rightarrow \alpha[\sigma]
\]\[
    A[\sigma] \rightarrow B[f\sigma]
\]\[
    A[f\sigma] \rightarrow \alpha[\sigma]
\]
where $f \in F$ and $\sigma$ is a string of Index Symbols. The Index
Symbols are used to form a \emph{Stack} by the Production Rules where
Index Symbols are either pushed or popped from the Stack.

An Indexed Language can be Recognized by a \emph{Nested Stack
  Automaton} (\S\ref{sec:nested_stack_automaton}).\cite{aho69}



\subsubsection{Generalized Contex-free}\label{sec:generalized_context_free}

A \emph{Generalized Context-free Grammar} adds to the Production Rules
of a Context-free Grammar a set of non-Context-free \emph{Composition
  Functions} that combine tuples of Symbols:
\[
    f(\langle x_1,\cdots,x_m\rangle,\cdots,\langle
    y_1,\cdots,y_n\rangle)=\gamma
\]
where $\gamma$ is a single tuple or another Composition Function that
reduces to a single tuple.

Rules are of the form:
\[
    A \rightarrow f(X,Y,\cdots)
\]
where $X$,$Y$,$\cdots$ are string tuples or Non-terminal Symbols.

There are several weakly equivalent Grammars to the composition
formulation:

\begin{description}
\item[Linear Context-free Rewriting System] \hfill \\
    Weakly equivalent to \emph{Multi-component Tree-adjoining
      Grammars} where Composition Functions are both \emph{Linear} and
    \emph{Regular}. Can be Recognized by \emph{Thread
      Automata} (\S\ref{sec:thread_automaton})\cite{villemonte02}

\item[Tree-adjoining] \hfill \\
    Elementary rewriting unit is a \emph{Tree}
    (\S\ref{sec:graph_tree}) rather than a Symbol. Can be Recognized
    by \emph{Embedded Pushdown Automata}
    (\S\ref{sec:embedded_pushdown})\cite{vijayashanker88}

\item[Linear Indexed Grammar] \hfill \\
    A modified Indexed Grammar where only one Symbol receives the
    Stack.

\item[Combinatory Categorical Grammar] \hfill \\
    A type of \emph{Phrase Structure Grammar}
    (\S\ref{sec:context_free}) using \emph{Combinatory Logic}
    (\S\ref{sec:combinatory_logic}).

\item[Head grammar] \hfill \\
    A Subset of the Linear Context-free Rewriting System and a Phrase
    Structure Grammar.

\end{description}



% --------------------------------------------------------------------
\subsection{Type-2: Context-free Grammar}\label{sec:context_free}
% --------------------------------------------------------------------

\emph{Context-free Grammars} (\emph{CFG}s) have Production Rules of
the form:
\[
    V \rightarrow \alpha
\]
where $V$ is a single Non-terminal and $\alpha$ is a string of Terminals
and/or Non-terminals (or $\varepsilon$). Because $V$ is required to be a
single Non-terminal, the Production Rules can be applied regardless of
Context. Each Non-terminal in a Context-free Grammar, $G$, is said to
form a \emph{Sublanguage} of the Language defined by $G$.

Multiple Context-free Grammars may generate the same Language, so
properties of CFGs may be termed \emph{Extrinsic} while Language
properties are \emph{Intrinsic}. The question of equality between CFGs
is Undecidable.

A Context-free Language may also be called a \emph{Recursive
  Language}.

A popular notation for Context-free Grammars in Computer Science is
\emph{Backus-Naur form} (\emph{BNF}).

Context-free Grammars are equivalent to \emph{Non-deterministic
  Pushdown Automata}(\S\ref{sec:pushdown_automata}).



\subsubsection{Constituency Grammar}\label{sec:constituency_grammar}

In Linguistics, the term used for Context-free Grammar is \emph{Phrase
  Structure Grammar} which is also called \emph{Constituency Grammar}
due to the one-to-one-or-many correspondence between the Productions
(ultimately rooted in the \emph{Subject-Predicate Clause} derived from
\emph{Term Logic} (\S\ref{sec:term_logic})). A \emph{Parse Tree}
(\S\ref{sec:concrete_syntax}) may be constructed according to the
\emph{Constituency Relation} of a Constituency Grammar.



\subsubsection{Dependency Grammar}\label{sec:dependency_grammar}
An alternative formulation to Phrase Structure Grammar is
\emph{Dependency Grammar} in which the Verb is the root and there is a
one-to-one correspondence between Symbols and nodes in the Syntax
Structure (\S\ref{sec:concrete_syntax}).



\subsubsection{Deterministic}\label{sec:deterministic_cfg}
\emph{Deterministic Context-free Grammars} are derived from
\emph{Deterministic Pushdown
  Automata}(\S\ref{sec:deterministic_pda}) and are always
\emph{unambiguous}. They can be \emph{Parsed} (\S\ref{sec:parser}) in
Linear Time and a Parser can be automatically generated from
the Grammar by a \emph{Parser
  Generator}(\S\ref{sec:parser_generator}).



\subsubsection{Visibly Pushdown}
\emph{Visibly Pushdown Grammars} are described by the 4-tuple
\[
    G = (V=V^0 \cup V^1,T,P,S)
\]
where $V^0$ and $V^1$ are Disjoint Sets of Non-terminals and there
are three kinds of Production Rules:
\[
    X \rightarrow \varepsilon
\]\[
    X \rightarrow aY
\]\[
    X \rightarrow \langle aZb \rangle Y
\]
where $Z \in V^0$ and if $X \in V^0$ then $Y \in V^0$

The resulting Language is a \emph{Regular Language} with \emph{nested
  words}, described by a \emph{Monadic Second-order Logic}
(\S\ref{sec:monadic_second_order}).



% --------------------------------------------------------------------
\subsection{Type-3: Regular Grammar} \label{sec:regular_language}
% --------------------------------------------------------------------

\emph{Regular Languages} (or \emph{Token-level Languages}) are more
restricted than Context-free Languages and satisfy a number of Closure
Properties. For two Regular Languages, $K$ and $L$, the following
operations result in a Language that is also Regular:
\[
    K \cup L, \quad
    K \cap L, \quad
    \overline{L}, \quad
    K - L, \quad
    K \circ L, \quad
    L^*, \quad
    K / L, \quad
    L^R
\]
A common formulation of Regular Languages is the \emph{Regular
  Expression} and conversely it is sometimes said that a Regular
Language is one that can be defined by a Regular Expression.

An algebraic description is as follows:
\[
    L = \{ w \in \Sigma^* | f(w) \in N \}
\]
where $f : \Sigma^* \rightarrow M$ is a \emph{Monoid Homomorphism} of
\emph{Finite Monoid} (\S\ref{sec:monoid}) $M$ and $N \subseteq M$.



\subsubsection{Extended Regular}\label{sec:extended_regular}
\emph{Extended Regular Grammars} have Productions of either \emph{Right
Regular} or \emph{Left Regular} form.

Right:
\[
    B \rightarrow a
\]\[
    A \rightarrow B \nu
\]\[
    A \rightarrow \varepsilon
\]

Left:
\[
    A \rightarrow a
\]\[
    A \rightarrow B \nu
\]\[
    A \rightarrow \varepsilon
\]
where $a$ is a single Non-terminal and $\nu$ is an expression of only
Non-terminal characters.



\subsubsection{Strictly Regular}\label{sec:strictly_regular}
\emph{Strictly Regular Grammars} also have Productions of either Right
Regular or Left Regular form.

Right:
\[
    B \rightarrow a
\]\[
    B \rightarrow aC
\]\[
    B \rightarrow \varepsilon
\]

Left:
\[
    A \rightarrow a
\]\[
    A \rightarrow Ba
\]\[
    A \rightarrow \varepsilon
\]
where $a$ is a single Non-terminal.

There is a one-to-one correspondence between the rules of a
\emph{Strictly Left Regular Grammar} and those of a
\emph{Non-deterministic Finite Automaton}(\S\ref{sec:ndfa}).

The \emph{Pumping Lemma} states that the middle section of an
Expression within a Regular Language may be repeated an arbitrary
number of times to produce another Expression in that same Language.

\paragraph{k-Testable}\label{sec:k_testable}
A \emph{k-Testable Language} is one where membership of an Expression
depends on the first and last Symbol and a Set of Factors of length
$k$. An example is a \emph{Local Language} which is a \emph{2-Testable
  Language} described by the Regular Expression:
\[
    (Q\Sigma^* \cap \Sigma^*R)\setminus\Sigma^*F\Sigma^*
\]
where $Q,R \subseteq \Sigma$ and $F \subseteq \Sigma \times
\Sigma$. This requires for a \emph{Word} (Expression), $w$, that is a
member of a Local Language to have its first Symbol in $Q$, and its
second Symbol in $R$, and no factor of $w$ of length 2 is in $F$. A
Local Language is Recognized by a \emph{Local
  Automaton}(\S\ref{sec:dfa}).



\subsubsection{Star-free}\label{sec:starfree_grammar}

A \emph{Star-free Language} is one having a \emph{Generalized Star
  Height} equal to zero, that is, the minimal \emph{Star Height} of
all Expressions in the Language with the Star Height of an
Expression's \emph{Complement} being equal.

Star-free Languages are characterized as those with \emph{Aperiodic
  Syntactic Monoids}\cite{schutzenberger65}
(\S\ref{sec:syntactic_monoid}) and also as the \emph{Counter-free
  Languages}\cite{mcnaughton-papert71} by the \emph{Aperiodic
  Finite-state Automaton} (\S\ref{sec:aperiodic_automaton}) and
\emph{Linear Temporal Logic} (\S\ref{sec:linear_temporal}).



% --------------------------------------------------------------------
\subsection{Affix Grammar}\label{sec:affix_grammar}
% --------------------------------------------------------------------

\emph{Affix Grammars} are those of a Context-free Grammar with a
Subset of the Non-terminals used as \emph{Affix Arguments}. If the
same Affix appears multiple places in a Production, the value must be
the same.



% --------------------------------------------------------------------
\subsection{Two-Level Grammars}\label{sec:two_level_grammar}
% --------------------------------------------------------------------

\emph{Two-Level Grammars} are \emph{Grammar Generators} that may
generate Grammars with infinite rules. Allowing the values for Affixes
to be described by a Context-free Grammar results in a Two-Level
Grammar.


\begin{description}
\item[W-grammar] \emph{Van Wijngaarden Grammar} consists of a finite
  Set of \emph{Meta-rules} used to derive a possibly infinite set of
  Production Rules from a finite Set of \emph{Hyper-rules}.
\item[Extended Affix Grammar] is a restricted W-grammar.
\end{description}



% --------------------------------------------------------------------
\subsection{Attribute Grammar}\label{sec:attribute_grammar}
\cite{slonneger-kurtz95}
% --------------------------------------------------------------------

\emph{Attribute Grammars} can be seen as Context-free Grammars
extended to provide Context-sensitivity using a Set of
\emph{Attributes}, \emph{Attribute Values}, \emph{Evaluation Rules}
and \emph{Conditions}.

A finite, possibly empty set of Attributes is associated with each
distinct Symbol in the Grammar. Each Attribute has an associated
Domain of Values they may take on. Attributes are either
\emph{Synthesized Attributes} or \emph{Inherited Attributes}.

Production Rules have additional Conditions that must be met by the
Attribute Values at that Node in the \emph{Parse Tree}
(\S\ref{sec:concrete_syntax}).

Attribute Grammars allow Affixes (\S\ref{sec:affix_grammar}) from
arbitrary Domains and allows Functions to calculate values of Affixes.

In the context of Computer Science, Attribute Grammars can formally
express the rules for \emph{Type Checking} and declaration order, as
well as the \emph{Operational Semantics} of \emph{Programming
  Languages}.

\emph{L-attributed Grammar}

\emph{LR-attributed Grammar}

\emph{ECLR-attributed Grammar}

\emph{S-attributed Grammar}



\subsubsection{Synthesized Attributes}

A \emph{Synthesized Attribute} is passed from Child Node to Parent
Node.



\subsubsection{Inherited Attributes}

An \emph{Inherited Attribute} is passed from Parent Node to Child
Node.



% --------------------------------------------------------------------
\subsection{Analytic Grammar}\label{sec:analytic_grammar}
% --------------------------------------------------------------------

\emph{Analytic Grammars} are used in \emph{Parsing}
(\S\ref{sec:parser}). A few examples:

\begin{description}
\item[Top-Down Parsing Language] \hfill \\
Formal representation of \emph{Recursive Descent Parser}. Production
rules of the form
\[
    A \leftarrow \varepsilon
\]\[
    A \leftarrow f
\]\[
    A \leftarrow a
\]\[
    A \leftarrow BC/D
\]
\item[Parsing Expression Generator] \hfill \\
A more generalized Top-Down Parsing Language.
\item[Link Grammar] \hfill \\
Dependency Grammar
  (\S\ref{sec:context_free}) with directionality between Symbols.
\end{description}



% --------------------------------------------------------------------
\subsection{Adaptive Grammar}\label{sec:adaptive_grammar}
% --------------------------------------------------------------------

\emph{Adaptive Grammars} allow for Production Rules to be manipulated
within the Grammar, including addition, deletion, and modification of
Rules.



\subsubsection{Imperative Adaptive Grammar}

Global

Rule changes are based on global State changing over time.

\begin{itemize}
\item Extensible Context-Free Grammars
\item Top-down Modifiable Grammars
\item Bottom-up Modifiable Grammars
\end{itemize}



\subsubsection{Declarative Adaptive Grammar}

Local

Rule changes only affect the position in the Syntax Tree of the
generation of a string.

\begin{itemize}
\item Christiansen Grammars
\item Recursive Adaptive Grammars
\end{itemize}



\subsubsection{Time-space (Hybrid) Adaptive Grammar}

\begin{itemize}
\item \S-Calculus
\end{itemize}



\subsubsection{Dynamic Grammars}

Boullier\cite{boullier94}



% ====================================================================
\section{Abstract Reduction System}\label{sec:abstract_rewrite}
% ====================================================================

Formal Grammars may be abstracted as \emph{Abstract Reduction Systems}
(or \emph{Abstract Rewrite Systems}), abbreviated $ARS$. An ARS is
simply
    \[(A,\rightarrow)\]
where $A$ is a Set of objects (Expressions) and $\rightarrow \subseteq
A \times A$ is a Binary Relation on those objects called the
\emph{Reduction Relation}. The object appearing in the right-hand side
of a Reduction Relation is called a \emph{Reduct} (or
\emph{Expansion}) of the left-hand side. This is equivalent to an
\emph{Unlabelled State Transition System}
(\S\ref{sec:state_transition_system}).

An \emph{Indexed Abstract Reduction System} differentiates Reductions
into Classes so that $\rightarrow$ is the Indexed Union of these
relations:
    \[(A, \rightarrow_1, \rightarrow_2, \cdots)\]
This is identical to a \emph{Labeled Transition System}
(\S\ref{sec:state_transition_system}).

A \emph{Reduction Sequence}, $x \rightarrow y \rightarrow z$, is
denoted $x \stackrel{*}\rightarrow z$, where $\stackrel{*}\rightarrow$
is the Reflexive Transitive Closure of $\rightarrow$ (see below).

Given the Reduction Relation, $\rightarrow$, for an ARS, the following
Relations may be defined:

\begin{description}

\item [$\stackrel{*}\rightarrow$]: Reflexive Transitive Closure of
  $\rightarrow$, the smallest Preorder (Reflexive and Transitive
  Relation) containing $\rightarrow$, that is, the Transitive Closure
  of $\rightarrow \cup =$:
  \[ (\rightarrow \cup =)^+ =
    \bigcup_{i \in \{1,2,3,...\}} (\rightarrow \cup =)^i \]

\item [$\leftrightarrow$]: Symmetric Closure of $\rightarrow$:
  \[ \rightarrow \cup \rightarrow^{-1} \]

\item [$\stackrel{*}\leftrightarrow$]: Reflexive Transitive
  Symmetric Closure of $\rightarrow$ (smallest Equivalence Relation
  containing $\rightarrow$), that is, the Transitive Closure of
  $\leftrightarrow \cup =$:
  \[ (\leftrightarrow \cup =)^+ =
    \bigcup_{i \in \{1,2,3,...\}} (\leftrightarrow \cup =)^i \]

\end{description}



% --------------------------------------------------------------------
\subsection{Normal Form}\label{sec:normal_form}
% --------------------------------------------------------------------

An object of an Abstract Rewriting System $(A,\rightarrow)$ is in
\emph{Normal Form} (\emph{Irreducible}) if it cannot be Rewritten
further. That is, $x \in A$ is in Normal Form if $\nexists y \in A : x
\rightarrow y$. Otherwise $x$ is \emph{Reducible}.

For two objects $x,y \in A$, $y$ is a Normal Form of $x$ if $x
\stackrel{*}{\rightarrow} y$ and $y$ is Irreducible. A unique Normal
Form of $x$ is denoted $x \downarrow$.

An object of an ARS is \emph{Weakly Normalizing} if some Rewrite
sequence leads to a Normal Form and \emph{Strongly Normalizing} if
every Rewrite sequence leads to a Normal Form. If all objects of an
ARS are Strongly Normalizing, then the system itself is called
Strongly Normalizing (or \emph{Terminating} or \emph{Noetherian}) and
has no infinite Reduction Sequences.

An ARS is \emph{Normalizing} if every object has at least one Normal
Form, and likewise is Weakly or Strongly Normalizing if every object
is Weakly or Strongly Normalizing. A Strongly Normalizing ARS always
terminates and is therefore not Turing Complte and cannot be used as a
Self-interpreter (in programming languages).

\emph{Untyped $\lambda$-calculus} (\S\ref{sec:untyped_lambda}) lacks
the Normalization Property.

Systems of \emph{Typed $\lambda$-calculus} (\S\ref{sec:typed_lambda})
such as \emph{Simply-typed $\lambda$-calculus}
(\S\ref{sec:simply_typed}) and \emph{Calculus of Constructions}
(\S\ref{sec:coq}) are Strongly Normalizing.



% --------------------------------------------------------------------
\subsection{Joinability}\label{sec:rewrite_join}
% --------------------------------------------------------------------

For an ARS $(A, \rightarrow)$, two objects $x,y\in A$ are
\emph{Joinable} when:
\[
    \exists z \in A :
    x \stackrel{*}\rightarrow z \stackrel{*}\leftarrow y
\]
The \emph{Joinability Relation} can be defined as:
\[
    \stackrel{*}\rightarrow \circ \stackrel{*}\leftarrow
\]
where $\circ$ is Relation Composition
(\S\ref{sec:relation_composition}). As a Binary Relation, Joinability
is denoted $x \downarrow y$.

An ARS has the \emph{Church-Rosser Property} if and only if:
\[
    \forall x,y \in A, x \stackrel{*}\leftrightarrow y
    \Rightarrow x \downarrow y
\]
Equivalently, the Church-Rosser Property can be expressed as:
\[
    \stackrel{*}\leftrightarrow \subseteq \downarrow
\]
The Church-Rosser Property is equivalent to \emph{Confluence}
(\S\ref{sec:rewrite_confluence}).

In a Church-Rosser system, an object can have zero or one Normal
Forms. By the \emph{Church-Rosser Theorem}, $\lambda$-calculus
(\S\ref{sec:untyped_lambda}) has this property, which means that
evaluation ($beta$-reduction) can be carried out in any order.



% --------------------------------------------------------------------
\subsection{Confluence}\label{sec:rewrite_confluence}
% --------------------------------------------------------------------

An ARS $(A, \rightarrow)$ is \emph{Confluent} (or \emph{Globally
  Confluent}) if and only if:
\[
    \forall w,x,y \in A,
    x \stackrel{*}\leftarrow w \stackrel{*}\rightarrow y
    \Rightarrow x \downarrow y
\]
That is, for any two diverging paths, they eventually meet at a common
successor. An individual object can be said to be Confluent if it has
this property.

\emph{Semi-confluent}:
\[
    \forall w,x,y \in A,
    x \leftarrow w \stackrel{*}\rightarrow y
    \Rightarrow x \downarrow y
\]

\emph{Weakly Confluent} (or \emph{Locally Confluent}):
\[
    \forall w,x,y \in A,
    x \leftarrow w \rightarrow y \Rightarrow x \downarrow y
\]

\emph{Strongly Confluent}
\[
    \forall w,x,y \in A,
    x \leftarrow w \rightarrow y \Rightarrow
    \exists z \in A : x \stackrel{*}\rightarrow z \wedge
    (y \rightarrow z \vee y = z)
\]

A Semi-confluent system is necessarily Confluent. The Church-Rosser
Property, Confluence, and Semi-confluence are all equivalent
properties for an ARS. If Confluence is restricted to single
Reductions (rather than Reduction Sequences), the stronger property is
called the \emph{Diamond Property}.

A Reduction Relation $\rightarrow$ is Confluent if and only if
$\stackrel{*}\rightarrow$ is Locally Confluent.

By \emph{Newman's Lemma}, if an ARS is Locally Confluent and
Terminating, then it is Confluent.

Reduction of Polynomials Modulo an Ideal is a Confluent Rewrite System
when working with a Grobner Basis (\S\ref{sec:ring_ideal}).



% --------------------------------------------------------------------
\subsection{Convergence}\label{sec:rewrite_convergence}
% --------------------------------------------------------------------

An ARS that is both Confluent and Terminating is called
\emph{Convergent} (or \emph{Canonical}) and every object has a unique
Normal Form.



% --------------------------------------------------------------------
\subsection{Semi-Thue System}\label{sec:semithue_system}
% --------------------------------------------------------------------

or \emph{String Rewriting System} (\emph{SRS})



% --------------------------------------------------------------------
\subsection{Graph Rewriting System}\label{sec:graph_rewriting}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Term Rewriting System}\label{sec:term_rewriting}
% --------------------------------------------------------------------



% ====================================================================
\section{Parser} \label{sec:parser}
% ====================================================================

\emph{Parsing} is the process of \emph{Syntactic Analysis}. A
\emph{Parser} analyzes Expressions according to the rules of a Formal
Grammar, generating a \emph{Data Structure} (\S\ref{sec:f_algebra})
describing the Syntax of the input.

The Data Structure produced by the Parser may be a \emph{Concrete
  Syntax Tree} (\emph{Parse Tree}: \S\ref{sec:concrete_syntax}), an
\emph{Abstract Syntax Tree} (\emph{Syntax Tree}:
\S\ref{sec:abstract_syntax}), or other hierarchical structure.



% --------------------------------------------------------------------
\subsection{Concrete Syntax Tree}\label{sec:concrete_syntax}
% --------------------------------------------------------------------

A \emph{Concrete Syntax Tree} is an Ordered, Rooted Tree
(\S\ref{sec:graph_tree}) representing the \emph{Syntactic
  Structure} of a string according to a Context-free Grammar
(\S\ref{sec:context_free}) such as the \emph{Constituency Relation}
of a Constituency Grammar (\S\ref{sec:constituency_grammar}) or the
\emph{Depenency Relation} of a Dependency Grammar
(\S\ref{sec:dependency_grammar})



% --------------------------------------------------------------------
\subsection{Abstract Syntax Tree}\label{sec:abstract_syntax}
% --------------------------------------------------------------------

\emph{Abstract Syntax} is a representation of the Source (Concrete)
Syntax being Parsed.

\emph{First-order Abstract Syntax} (FOAS), \emph{Higher-order Abstract
  Syntax} (HOAS)



% --------------------------------------------------------------------
\subsection{Lexical Analysis}
% --------------------------------------------------------------------

A Parser may be preceded by a \emph{Lexical Analyzer} (also
\emph{Lexer} or \emph{Tokenizer}) which creates \emph{Tokens} from
input Expressions. A Token is a structure representing a \emph{Lexeme}
(that is, a string forming a Syntactic unit) and associated
\emph{Type} and \emph{Value} information.

A Lexical Analyzer is itself a Parser and usually the \emph{Lexical
  Syntax} is defined by a Regular Language
(\S\ref{sec:regular_language}). The Tokens are then analyzed by the
Parser according to the rules of the \emph{Phrase Syntax}, which is
usualy a Context-free Language (\S\ref{sec:context_free}). A Parser
without a separate Lexer is called a \emph{Scannerless Parser}.

Lexing may be divided into two stages: \emph{Scanning} and
\emph{Evaluation}. Prior to Tokenization, a \emph{Scanner} (usually a
Finite-state Machine \S\ref{sec:finite_automata}) may perform its own
Lexical Analysis, producing Lexemes categorized by Token class. An
\emph{Evaluator} then produces the final Tokens by either adding Value
information to the Type to produce a Type-Value pair, or returning
just the Type, or possibly also suppressing the Lexeme so the Parser
doesn't see it (e.g. whitespace or comments).

Token identification methods include \emph{Regular Expressions},
\emph{Flags} (specific sequences), \emph{Delimiters} (specific
separators), or explicit \emph{Dictionaries}.



% --------------------------------------------------------------------
\subsection{Syntactic Analysis}
% --------------------------------------------------------------------

The Parser determines if and how the input can be derived from the
Start Symbol of a Context-free Grammar. Parsing can proceed in two
directions:
\begin{description}
    \item [Top-down Parsing] starts with the highest level of the Parse
      Tree. Proceeds greedily and may be \emph{Exponential} with
      \emph{Backtracking}.
    \item [Bottom-up Parsing] starts with the lowest level of the
      Parse Tree.
\end{description}
The output of Syntactic Analysis is a Data Structure such as a
Concrete or Abstract Syntax Tree. Further Context-sensitive Parsing
may follow.



% --------------------------------------------------------------------
\subsection{Semantic Analysis}\label{sec:semantic_analysis}
% --------------------------------------------------------------------

\emph{Semantic Analysis} (or \emph{Symantic Parsing} or
\emph{Contextual Analysis}) may be performed after Syntactic Analysis.
Parsing for Context-sensitive Semantics, such as Type Checking or
declaration order, can be formally expressed with Attribute Grammars
(\S\ref{sec:attribute_grammar}).

A further example would be the in the resulting actions of
\emph{Evaluation} in an \emph{Interpreter} or \emph{Code Generation}
of a \emph{Compiler} for a \emph{Programming Language}, for which
purpose an Attribute Grammar may also be used.



% --------------------------------------------------------------------
\subsection{Top-down Parsers}
% --------------------------------------------------------------------

\subsubsection{Recursive Descent Parser}

\subsubsection{LL Parser}

\subsubsection{Early Parser}



% --------------------------------------------------------------------
\subsection{Bottom-up Parsers}
% --------------------------------------------------------------------

\subsubsection{Precedence Parser}

\subsubsection{LR Parser}

\emph{Canonical LR} LR(1)

\paragraph{SLR Parser}

\paragraph{LALR Parser}

\paragraph{GLR Parser}

\subsubsection{CYK Parser}

\subsubsection{Recursive Ascent Parser}



% --------------------------------------------------------------------
\subsection{Parser Generators}\label{sec:parser_generator}
% --------------------------------------------------------------------

A \emph{Parser Generator} takes as a Grammar (for example a BNF
Grammar) and outputs the source code of a Parser for the Language
specified by the Grammar.



% --------------------------------------------------------------------
\subsection{Template Processing}\label{sec:template_processing}
% --------------------------------------------------------------------



% ====================================================================
\section{Automata Theory}\label{sec:automata_theory}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Abstract Machine} \label{sec:abstract_machine}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{State Transition Systems} \label{sec:state_transition_system}
% --------------------------------------------------------------------

A \emph{State Transition System} can have an infinite number of
\emph{States} and \emph{Transitions}, represented as the pair
\[
    (S,\rightarrow)
\]
where $S$ is a set of States and $\rightarrow \subseteq S \times S$.
This is identical to an \emph{un-indexed Abstract Rewriting
  System}(\S\ref{sec:abstract_rewrite}).

\emph{Finite Automata} may be seen as State Transition Systems with an
initial State and a number of final \emph{Accept} states indicating
\emph{Word} (Expression) membership for a Language.

\emph{Labelled State Transition Systems} have an additional set of
\emph{Labels}, $\Lambda$
\[(S,\Lambda,\rightarrow)\]
and $\rightarrow \subseteq S \times \Lambda \times S$.

\emph{Action Programming Languages} add a set of \emph{Fluents}, $F$, and
\emph{Values}, $V$, and a function mapping $F \times S$ to $V$.



% --------------------------------------------------------------------
\subsection{Semiautomata}\label{sec:semiautomata}
% --------------------------------------------------------------------

A State Transition System may be formulated as a \emph{Semiautomata}
\[
    (Q,\Sigma,T)
\]
where $\Sigma$ is a non-empty \emph{input Symbols}, $Q$ is the set of
States, and $T$ is a \emph{transition function} $T:Q \times \Sigma
\rightarrow Q$.

A Semiautomaton induces a Monoid called the \emph{input Monoid}:
\[
    M(Q,\Sigma,T) = \{T_w | w \in \Sigma^*\}
\]



% --------------------------------------------------------------------
\subsection{Automata} \label{sec:automata}
% --------------------------------------------------------------------

An \emph{Automaton} reads input strings, \emph{Words} (Expressions),
and either accepts or rejects depending on whether a Word is a member
of the Language recognized by that Automaton. By convention the
Vocabulary of Expressions from Formal Languages will be re-cast as an
Alphabet of Words, $\Sigma$. The Set of all Words accepted by an
Automaton are those that are \emph{Recognized} by the Automaton.

Automata may be arranged in a hierarchy according to increasing power:
\[
    DFA = NFA \subset DPDA-I \subset NPDA-I \subset LBA \subset DPDA-II =
\]\[
    = NPDA-II = DTM = NTM = PTM = MDTM
\]
where
\begin{itemize}
\item DFA = Deterministic Finite Automata
\item NFA = Non-deterministic Finite Automata
\item DPDA = Deterministic Push Down Automata with 1
  or 2 push-down stores
\item NPDA = Non-deterministic Push Down Automata
  with 1 or 2 push-down stores
\item LBA = Linear Bounded Automata
\item DTM = Deterministic Turing Machine
\item NTM = Non-deterministic Turing Machine
\item PTM = Probabilistic Turing Machine
\item MDTM = Multidimensional Turing Machine
\end{itemize}



% --------------------------------------------------------------------
\subsection{Finite Automata}\label{sec:finite_automata}
% --------------------------------------------------------------------

\emph{Finite Automata} are \emph{Finite State Machines} and take a
finite input string of Symbols and either accepts or rejects the input
depending on the final State of the computation. Finite Automata are
able to recognize Regular Languages(\S\ref{sec:regular_language}).



\subsubsection{Deterministic Finite Automata}\label{sec:dfa}

\emph{Deterministic Finite Automata} have the restriction that an
input Symbol has a transition function to a single State.
Deterministic Finite Automata recognize Regular
Languages(\S\ref{sec:regular_language}).

Representation of a Deterministic Finite Automaton as a 5-tuple:
\[
    (Q,\Sigma,\delta,q_0,F)
\]
where
\begin{itemize}
\item $Q$ is a finite set of States
\item $\Sigma$ is the Alphabet
\item $\delta$ is the transition function $\delta: Q \times
  \Sigma \rightarrow Q$
\item $q_0 \in Q$ is the initial State
\item $F \subseteq Q$ is the set of final Accept States.
\end{itemize}

Running for a given input $w = a_1,a_2, \cdots , a_n \in \Sigma^*$
produces a sequence of States $q_0,q_1,q_2,\cdots , q_n$ where $q_i
\in Q$ such that $q_i = \delta (q_{i-1},a_i)$ and $w$ is accepted if
$q_n \in F$.

A recursive definition using \emph{composition} of transition
functions
\[
    \widehat{\delta}(q,\varepsilon) = q
\]\[
    \widehat{\delta}(q,wa) = \delta_a(\widehat{\delta}(q,w))
\]
where $w \in \Sigma^*$, $a \in \Sigma$ and $q \in Q$. Repeated
application describes the \emph{Transition Monoid} or
\emph{Transformation Semigroup}.

\paragraph{Local Automaton}\label{sec:local_automaton}
A \emph{Local Automaton} is a DFA for which all Edges with the same
Label lead to a single Vertex.

Local Automata Recognize Local Languages(\S\ref{sec:k_testable}).



\subsubsection{Nondeterministic Finite Automata}\label{sec:ndfa}

\emph{Nondeterministic Finite Automata} are Finite State Machines that
may transition from one State to a number of different states, given
as an element of the powerset of $Q$, $\mathcal{P}(Q)$.

Representation of a Nondeterministic Finite Automaton as a
5-tuple:
\[
    (Q,\Sigma,\Delta,q_0,F)
\]
where
\begin{itemize}
\item $Q$ is a finite set of States
\item $\Sigma$ is the Alphabet
\item $\Delta$ is a \emph{transition relation} $\Delta: Q \times
  \Sigma \rightarrow \mathcal{P}(Q)$
\item $q_0 \in Q$ is the initial State
\item $F \subseteq Q$ is the set of final Accept States.
\end{itemize}

A Word, $w=a_1,a_2,\cdots,a_n$, is accepted when there exists a
sequence of States, $r_0,r_1,\cdots,r_n$ such that
\begin{enumerate}
\item $r_0 = q_0$
\item $r_{i+1} \in \Delta(r_i, a_{i+1})$, for $i = 0, \cdots, n-1$
\item $r_n \in F$
\end{enumerate}

A DFA may be seen as a NFA which restricts transitions to allow only
one State, and can be constructed from a NFA with $n$ States using
\emph{powerset construction}, requiring up to $2^n$ States. Both types
recognize the same Regular Languages
(\S\ref{sec:regular_language}).

\paragraph{NFA-$\varepsilon$} is a NFA that allows transitions
without consuming input Symbols. A transition that changes state
without consuming input is an $\varepsilon$ $move$. Each State $q$
defines an $\varepsilon$-\emph{closure}, $E(q)$, which is the set of
States that are reachable by $\varepsilon$ moves.

The Languages recognized by NFA-$\varepsilon$ are the same as NFA/DFA.



\subsubsection{Aperiodic Finite-state Automaton}
\label{sec:aperiodic_automaton}



% --------------------------------------------------------------------
\subsection{Pushdown Automata}\label{sec:pushdown_automata}
% --------------------------------------------------------------------

\emph{Pushdown Automata} add to Finite Automata a \emph{Stack} as a
parameter for choice of States and can recognize Context-free
Languages(\S\ref{sec:context_free}).

Adding a second Stack makes a Pushdown Automata equal in power to a
Turing Machine.

Unlike Finite Automata, Deterministic PDA are not equivalent to
Nondeterministic PDA. The general representation for a PDA is
\[
    M = (Q, \Sigma, \Gamma, q_0, Z_0, F, \delta)
\]
where
\begin{itemize}
\item $Q$ is a finite set of States
\item $\Sigma$ is a finite set of input Symbols
\item $\Gamma$ is a finite set of Stack Symbols
\item $q_0 \in Q$ is the initial State
\item $Z_0 \in \Gamma$ is the initial Stack Symbol
\item $F \subseteq Q$ is the set of final Accept States
\item $\delta$ is the transition function $\delta: (Q \times (\Sigma
  \cup \{\varepsilon\}) \times \Gamma) \rightarrow \mathcal{P}(Q \times
  \Gamma^*)$
\end{itemize}

An element $(p,a,Z,q,\alpha)\in\delta$, with $M$ in State $p \in Q$,
input $a \in \Sigma \cup \{\varepsilon\}$, and top stack Symbol $Z \in
\Gamma$ results in the following:
\begin{enumerate}
\item read $a$
\item change state to $q$
\item pop $Z$
\item push $\alpha \in \Gamma^*$
\end{enumerate}



\subsubsection{Deterministic Pushdown Automata}\label{sec:deterministic_pda}

\emph{Deterministic Pushdown Automata} have the restriction of only
one derivation per accepted input Word. This allows recognition of a
subset of Context-free Languages termed
Deterministic(\S\ref{sec:deterministic_cfg}). Such Languages can be
parsed in linear time and Parsers for such Languages can be generated
automatically(\S\ref{sec:parser_generator}).

A Pushdown Automata is Deterministic iff both
\begin{enumerate}
\item $\forall q \in Q, a \in \Sigma \cup {\varepsilon}, x \in
  \Gamma \vdash |\delta(q,a,x)| \leq 1$
\item $\forall q \in Q, x \in \Gamma \vdash |\delta(q,\varepsilon,x)|
  \neq 0 \Rightarrow \forall a \in \Sigma \vdash |\delta(q,a,x)|=0$
\end{enumerate}



\subsubsection{Embedded Pushdown Automata}\label{sec:embedded_pushdown}

\subsubsection{Nested Stack Automaton}\label{sec:nested_stack_automaton}

\subsubsection{Thread Automaton}\label{sec:thread_automaton}



% --------------------------------------------------------------------
\subsection{Linear Bounded Automata} \label{sec:linear_bounded_automata}
% --------------------------------------------------------------------

\emph{Linear Bounded Automata} are Turing Machines restricted to an
input of finite length and are acceptors for Context-sensitive
Languages(\S\ref{sec:context_sensitive}) which require that
Production Rules do not increase the size of the Expression as a
result; therefore the size of the input is sufficient for calculation.



% --------------------------------------------------------------------
\subsection{Turing Machines}\label{sec:turing_machine}
% --------------------------------------------------------------------

A \emph{Turing Machine} operates on an infinite \emph{storage tape},
which acts as the read input as well as write storage. Pushdown
Automata with 2 Stacks are equivalent to Turing Machines.

A Turing Machine that halts for every input is a \emph{Total Turing
  Machine} or \emph{Decider}.



\subsubsection{Nondeterministic Turing Machines}

\emph{Nondeterministic Turing Machines} (\emph{NTM}s) can be defined
as
    \[
        M = (Q, \Sigma, q_0, \sqcup, A, \delta)
    \]
where
\begin{itemize}
\item $Q$ is a finite set of States
\item $\Sigma$ is the finite Alphabet
\item $q_0 \in Q$ is the initial State
\item $\sqcup \in \Sigma$ is the blank Symbol
\item $F \subseteq Q$ is the set of final Accept States
\item $\delta \subseteq (Q \setminus F \times \Sigma) \times (Q \times
  \Sigma \times \{L,R\})$ and $L$ and $R$ are left and right shift.
\end{itemize}

The operation of $M$ in State $q_i$ and current read input $a_j$ is a
transition function, $q_i a_j \rightarrow q_{i1} a_{j1} d_k$. Note
that for an NTM, $\delta$ is a relation and more than one function can
exist for each possible input/State combination. The result is to
write the new Symbol $a_{j1}$ in the current position and shift the
storage left or right as specified by $d_k$, afterwards assuming State
$q_{i1}$.



\subsubsection{Deterministic Turing Machines}

\emph{Deterministic Turing Machines} (\emph{DTM}s) have one possible
output transition per unique input/State combination, thus $\delta$ is
a \emph{partial function} rather than a \emph{relation}:
\[
    \delta : Q \setminus F \times \Sigma \rightarrow Q \times
    \Sigma \times {L,R}
\]
The computational power of DTMs and NTMs is equivalent (they can solve
the same problems) as NTMs include DTMs as a special case. An
equivalent accepting computation in a DTM is generally exponential to
the length of the shortest accepting computation of an NTM.



\subsubsection{Probabilistic Turing Machines}

A \emph{Probabilistic Turing Machine} adds to transitions a
probability distribution (or a tape with random Symbols). It is an
open question whether this is more powerful than a DTM
($\mathsf{BPP}=\mathsf{P}$ ?)  but it is useful in the definition of
\emph{interactive proof systems}. %FIXME ref



\subsubsection{Multidimensional Turing Machines}

\emph{Multidimensional Turing Machines} allow for tapes of varying
topologies. This requires additional shift directions (i.e. $\{L, R, U,
D\}$ for a 2-dimensional tape) but does not increase the computing
power; even an $\infty$-\emph{dimensional} Turing Machine can be
simulated by a DTM.



% ====================================================================
\section{Metalanguage}\label{sec:metalanguage}
% ====================================================================

A \emph{Metalanguage} is a Language used to describe another Language,
the \emph{Object Language}.

Metalanguage Expressions find use in:
\begin{itemize}
    \item Deductive Systems (\S\ref{sec:deductive_apparatus})
    \item Metavariables (\S\ref{sec:metavariable})
    \item Metatheory (\S\ref{sec:metatheory})
    \item Interpretations (\S\ref{sec:interpretation})
\end{itemize}

Note it is common to use Greek letters ($\Phi$, $\Psi$) for
Metavariables and Roman characters otherwise for Object Variables.



% --------------------------------------------------------------------
\subsection{Metasyntax}\label{sec:metasyntax}
% --------------------------------------------------------------------

The \emph{Metasyntax} describes the structure of valid Expressions in
a Metalanguage. There are three types of Symbols distinguished in
Metasyntax:
\begin{description}
    \item [Terminal Symbol] stand-alone Syntactic structure in the
      Object Language
    \item [Non-terminal Symbol] Syntactic category defining a Set of
      valid Expression structures in an $n$-element Subset
    \item [Metasymbol] Metalinguistic denotations
\end{description}
Depending on the Object Language, not all of these Categories may be
used. For example, Metasyntax for Regular Languages
(\S\ref{sec:regular_language}) does not have Non-terminals.



% --------------------------------------------------------------------
\subsection{Embedded Metalanguage}\label{sec:embedded_metalanguage}
% --------------------------------------------------------------------

The use of an Object Language to describe itself is an \emph{Embedded
  Metalanguage} (e.g. the English words \emph{noun} and \emph{verb}
are used to describe English itself).



% --------------------------------------------------------------------
\subsection{Ordered Metalanguage}\label{sec:ordered_metalanguage}
% --------------------------------------------------------------------

An \emph{Ordered Metalanguage} is constructed to discuss an
\emph{Object Language}, and a further Ordered Metalanguage may be
constructed to discuss the first.

analogous to \emph{Ordered Logic}
(\S\ref{sec:noncommutative_logic})



% --------------------------------------------------------------------
\subsection{Hierarchical Metalanguage}\label{sec:hierarchical_metalanguage}
% --------------------------------------------------------------------

an Ordered Hierarchy of Embedded Metalanguages



% ====================================================================
\section{Formal Theory}\label{sec:formal_theory}
% ====================================================================

For a Formal Language, $L$, a Formal Theory, $\mathcal{T}$, is a
Subset of a Conceptual Class of \emph{Elementary Expressions}
(\emph{Elementary Statements} or \emph{Well-formed Formulas}
\S\ref{sec:formal_system}), $\mathcal{E}$, consisting of
\emph{Elementary Theorems} considered to be \emph{True}. In a
\emph{Deductive Theory} (\S\ref{sec:deductive_theory}), $\mathcal{T}$
is an \emph{Inductive Class} such that some of the Elementary Theorems
are taken to be \emph{Axioms} and any Expressions Deducible by those
Axioms are also Elementary Theorems in $\mathcal{T}$.
