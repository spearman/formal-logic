%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Formal Language Theory}\label{part:formal_language}
\cite{hammel03}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Formal Language Theory} is the study of Formal Languages
(\S\ref{sec:formal_language}) and their Finite Representations:
Automata (\S\ref{sec:automata_theory}). These topics are closely
related to the question of Decidability
(\S\ref{sec:computable_function}), an important concept in Recursion
Theory (Part \ref{part:recursion_theory}).
%FIXME xref finite representations ?



% ====================================================================
\section{Alphabet}\label{sec:alphabet}
% ====================================================================

An \emph{Alphabet}:
\[
  \Sigma = \{ \sigma_1, \ldots, \sigma_n \}
\]
is a Non-empty Finite Set (\S\ref{sec:set}) of \emph{Symbols}
(Signifiers \S\ref{sec:signifier}), usually \emph{Characters} or
\emph{Letters}.

The Alphabet of a String (\S\ref{sec:string}), $s$, is the Set of
Symbols occuring in that String:
\[
  \mathrm{Alph}(s)
\]

The Alphabet of a Language (\S\ref{sec:formal_language}), $L$, is the
Set of all Symbols occuring in any String of that Language:
\[
  \mathrm{Alph}(L) = \bigcup_{s \in L} \mathrm{Alph}(s)
\]



% ====================================================================
\section{String}\label{sec:string}
% ====================================================================

A \emph{String} (or \emph{Word}), $s$, is a Finite Sequence
(\S\ref{sec:sequence}) of Symbols over an Alphabet $\Sigma$:
\[
  s = \langle c_1 \cdots c_k \rangle
\]
where $c_1, \ldots, c_k \in \Sigma$.

The \emph{Length} of a String, $|s| = k$, is equal to the number of
Symbols in $s$.

The \emph{Empty String} $\varepsilon$ is the unique String over
$\Sigma$ of Length 0, and acts as an Identity Element for String
Concatenation (\S\ref{sec:string_concatenation}).

The Set of all Strings of Length $n$ over $\Sigma$ is denoted
$\Sigma^n$.

The Infinite Set of all possible Strings of all Finite Lengths,
$\Sigma^*$, is the Free Monoid (\S\ref{sec:free_monoid}) or
\emph{Kleene Closure} of $\Sigma$, i.e. the smallest Superset
(\S\ref{sec:subset}) of $\Sigma$ that is closed under String
Concatenation (\S\ref{sec:string_concatenation}):
\[
  \Sigma^* = \bigcup_{n\in\mathbb{N}_0} \Sigma^n
\]

A Formal Language (\S\ref{sec:formal_language}) is a possibly Infinite
Subset of $\Sigma^*$.

The Set of all Infinite Words over $\Sigma$ is denoted $\Sigma^\omega$
and the Set of all Finite \emph{and} Infinite Words is denoted
$\Sigma^\infty$

An $\omega$-language (\S\ref{sec:omega_language}) is a Subset of
$\Sigma^\omega$



% --------------------------------------------------------------------
\subsection{String Concatenation}\label{sec:string_concatenation}
% --------------------------------------------------------------------

The \emph{Concatenation} of two Strings, $s$ and $t$, is an
Associative Operation denoted $s \cdot t$ or $st$, giving the String
resulting from appending the right-hand String to the end of the
left-hand String.

The Empty String $\varepsilon$ acts as an Identity Element:
\[
  s \cdot \varepsilon = s = \varepsilon \cdot s
\]

With a third String, $u$, the Associative Property gives:
\[
  s \cdot (t \cdot u) = (s \cdot t) \cdot u
\]

Free Monoid (\S\ref{sec:free_monoid})



% --------------------------------------------------------------------
\subsection{Substring}\label{sec:substring}
% --------------------------------------------------------------------

A \emph{Substring} (or \emph{Subword} or \emph{Factor}) is a
Subsequence (\S\ref{sec:subsequence}) of consecutive Symbols that
occur within a given String. For an $n$-length String, there are
$n(n+1) \over 2$ Non-empty Substrings.



\subsubsection{Prefix \& Suffix}\label{sec:prefix_suffix}

A \emph{Prefix} is a Substring that occurs at the beginning of a
String, and a \emph{Suffix} is a Substring that occurs at the end of a
String. A Prefix or Suffix that is not equal to the entire String
itself is called a \emph{Proper Prefix} or \emph{Proper Suffix}
respectively.

The Prefix Relation is sometimes denoted $t \sqsubseteq s$ for Prefix
$t$ of $s$, and defines a kind of Prefix Order
(\S\ref{sec:prefix_order}). For related Data Structures, see
\emph{Prefix Tree}\S\ref{sec:prefix_tree} and \emph{Suffix
  Tree}\S\ref{sec:suffix_tree}.

The Function $\mathrm{Pref}_L(s)$ is defined as the Set of all
Prefixes of a String $s$, with respect to a Language $L$:
\[
  \mathrm{Pref}_L(s) =
    \{ t\;|\;s = tu; t,u \in \mathrm{Alph}(L)^* \}
\]
For two Strings $s$ and $t$, the Prefix Relation
(\S\ref{sec:prefix_suffix}) can be defined in terms of the
$\mathrm{Pref}$ Function as:
\[
  s \sqsubseteq t \Leftrightarrow s \in \mathrm{Pref}_L(t)
\]
This Function is also used in the definition of the \emph{Prefix
  Closure} (\S\ref{sec:prefix_closure}) of a Language.



\subsubsection{Border}\label{sec:string_border}

A \emph{Border} is a Substring that is both a Prefix and a Suffix of
the same String.



\subsubsection{Superstring}\label{sec:superstring}

A \emph{Superstring}, $s$, for a Set of Strings, $P$, is a single
String that contains every String in $P$ as a Substring. For example,
the trivial Superstring of $P$ would be given by the concatenation of
all the Strings in $P$ in any order.



% --------------------------------------------------------------------
\subsection{String Substitution}\label{sec:string_substitution}
% --------------------------------------------------------------------

A \emph{String Substitution} is a Function, $f$, from Symbols in the
Alphabet, $s \in \Sigma$, of a Language, $L$, to another Language,
$L_s$, possibly with a different Alphabet, $\Delta$:
\[
  f(s) = L_s
\]
where $L_s \subseteq \Delta^*$. It is extended to Strings by:
\[
  f(\varepsilon) = \varepsilon
\]
for Empty String $\varepsilon$ and:
\[
  f(st) = f(s)f(t)
\]
for Strings $s,t \in L$. This can be extended to give the Substitution
Closure of a Language (\S\ref{sec:substitution_closure}). Regular
(\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are closed under String
Substitution.



\subsubsection{String Homomorphism}\label{sec:string_homomorphism}

A \emph{String Homomorphism} is a String Substitution such that each
Symbol in an Alphabet $\Sigma$ is replaced by a Single String. A
String Homomorphism is a Monoid Morphism on the Free Monoid of
$\Sigma$ (preserving String Concatenation). For a String Homomorphism
$f$ on Alphabet $\Sigma_1$:
\[
  f : \Sigma_1^* \rightarrow \Sigma_2^*
\]
where $\forall u,v \in \Sigma_1, f(uv) = f(u)f(v)$.

A String Homomorphism $f$ is \emph{$\varepsilon$-free} if $\forall a
\in \Sigma, f(a) \neq \varepsilon$.

A String Homomorphism can be used to define a Homomorphism Closure for
Languages (\S\ref{sec:language_homomorphism}). Regular
(\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are Closed under String
Homomorphisms.



% --------------------------------------------------------------------
\subsection{String Projection}\label{sec:string_projection}
% --------------------------------------------------------------------

For a String $s$ and Alphabet $\Sigma$ (not necessarily
$\text{Alph}(s)$), a \emph{String Projection}, $\pi_{\Sigma}(s)$, is
the String resulting from removal of all Symbols of $s$ that are not
in $\Sigma$:
\[
  \pi_{\Sigma}(s) =
  \begin{cases}
    \varepsilon       & \quad \text{if $s = \varepsilon$}\\
    \pi_{\Sigma}(t)   & \quad \text{if $s = ta$ and $a \notin \Sigma$}\\
    \pi_{\Sigma}(t)a  & \quad \text{if $s = ta$ and $a \in \Sigma$}\\
  \end{cases}
\]
\fist Cf. Projection in Relational Algebra
\S\ref{sec:relational_projection}. String Projection may be used to
define a Projection over a Language (\S\ref{sec:language_projection}).



% --------------------------------------------------------------------
\subsection{Quotient}\label{sec:string_quotient}
% --------------------------------------------------------------------

The \emph{Right Quotient} $S / b$ of a String $S$ and Symbol $b$
defined as:
\[
  (Sa)/b =
  \begin{cases}
    S           & \quad \text{if $a = b$}\\
    \varepsilon & \quad \text{if $a \neq b$}\\
  \end{cases}
\]
Likewise the \emph{Left Quotient} $b \backslash S$ is defined as:
\[
  b\backslash(aS) =
  \begin{cases}
    S           & \quad \text{if $a = b$}\\
    \varepsilon & \quad \text{if $a \neq b$}\\
  \end{cases}
\]
\fist Cf. Syntactic Quotient \S\ref{sec:syntactic_quotient}
(Syntactic Monoid)



% --------------------------------------------------------------------
\subsection{Right Cancellation}\label{sec:right_cancellation}
% --------------------------------------------------------------------

\emph{Right Cancellation}, $s \div a$, of a Symbol $a$ from a String
$S$ is the removal of the first occurance of $a$ from $S$, starting
from the right-hand side:
\[
  (sb) \div a =
  \begin{cases}
    s           & \quad \text{if $a = b$}\\
    (s \div a)b & \quad \text{if $a \neq b$}\\
  \end{cases}
\]



% --------------------------------------------------------------------
\subsection{Metacharacter}\label{sec:metacharacter}
% --------------------------------------------------------------------

%FIXME relation to formal languages ?

e.g. Wildcard Characters, Statement Delimiters, etc.



% ====================================================================
\section{Formal Language}\label{sec:formal_language}
% ====================================================================

A \emph{Formal Language}, $L$, is a (possibly Infinite) Subset of
$\Sigma^*$, the Set of all possible Strings (\S\ref{sec:string}) over
an Alphabet (\S\ref{sec:alphabet}) $\Sigma$. The Strings belonging to
$L$ are called \emph{Expressions} (\S\ref{sec:expression}), or
\emph{Phrases} (\S\ref{sec:lexical_analysis}) or \emph{Well-formed
  Formulas} (\S\ref{sec:formula}) depending on the context.

The entire content of a Formal Language is uniquely determined by
either:
\begin{itemize}
  \item the Set of all Terminal Expressions that are Generated by the
    Production Rules of a \emph{Generative Grammar}
    (\S\ref{sec:generative_grammar})
\end{itemize}
or
\begin{itemize}
  \item the Set of Expressions Recognized by a \emph{Determinative
    Grammar} (\S\ref{sec:determinative_grammar})
\end{itemize}

For two Languages $L_1$ and $L_2$ over a common Alphabet $\Sigma$:
\begin{itemize}
  \item $L_1 \cup L_2$ is a Language (Set Union)
  \item $L_1 \cap L_2$ is a Language (Set Intersection)
  \item $L_1 - L_2$ is a Language (Set Difference)
  \item $(L_1 - L_2 \cup L_2 - L_1)$ is a Language (Symmetric Difference)
  \item $L_1 \times L_2$ is a Language (Cartesian Product)
\end{itemize}
If $L_2 \subset L_1$ then $L_2$ is a \emph{Sublanguage} of $L_1$.

For a given Language $L \subseteq \Sigma^*$ there exists a
\emph{Complement Language} $L^C = \Sigma^* - L$.

The Language consisting of just the Empty String $\{\varepsilon\}$ is
distinguished from the Empty Language $\{\}$.



% --------------------------------------------------------------------
\subsection{Expression}\label{sec:expression}
% --------------------------------------------------------------------

An \emph{Expression} is a String (\S\ref{sec:string}) belonging to a
Formal Language.

\url{https://en.wikipedia.org/wiki/Closed-form_expression#Comparison_of_different_classes_of_expressions}:

\emph{
Arithmetic Expressions $\subset$ Polynomial Expressions $\subset$ Algebraic
Expressions $\subset$ Closed-form Expressions $\subset$ Analytic Expressions
$\subset$ Mathematical Expressions
}

% TODO: xrefs



% --------------------------------------------------------------------
\subsection{Language Concatenation}\label{sec:language_concatenation}
% --------------------------------------------------------------------

The \emph{Concatenation} of two Languages, $S$ and $T$, is an
Associative Operation that gives a third Language, $S \cdot T$,
defined as:
\[
  S \cdot T = \{s \cdot t\;|\; s \in S \wedge t \in T\}
\]
The Language consisting of just the Empty String, $\{\varepsilon\}$,
acts as an Identity Element:
\[
  S \cdot \{\varepsilon\} = S = \{\varepsilon\} \cdot S
\]
and the Empty Language, $\{\}$, acts as a Zero Element:
\[
  S \cdot \{\} = \{\} = \{\} \cdot S
\]
With a third Language, $U$, the Associative Property gives:
\[
  S \cdot (T \cdot U) = (S \cdot T) \cdot U
\]



% --------------------------------------------------------------------
\subsection{Prefix Closure}\label{sec:prefix_closure}
% --------------------------------------------------------------------

The \emph{Prefix Closure} of a Language $L$ is given by the Function
$\mathrm{Pref}(L)$:
\[
  \mathrm{Pref}(L) = \bigcup_{s \in L} \mathrm{Pref}_L(s) =
  \{ t\;|\;s = tu; s \in L; t,u \in \mathrm{Alph}(L)^* \}
\]
where $\mathrm{Pref}_L(s)$ is the Set of Prefixes
(\S\ref{sec:prefix_suffix}) of String $s$ with respect to $L$.

A Language is \emph{Prefix Closed} if $\mathrm{Pref}(L) = L$.
$\mathrm{Pref}$ is Idempotent:
\[
  \mathrm{Pref}(\mathrm{Pref}(L)) = \mathrm{Pref}(L)
\]

Example:

$L = \{abc\}$

$\mathrm{Pref}(L) = \{\varepsilon, a, ab, abc\}$



% --------------------------------------------------------------------
\subsection{Substitution Closure}\label{sec:substitution_closure}
% --------------------------------------------------------------------

The \emph{Substitution Closure} of a Language $L$ is defined as:
\[
  f(L) = \bigcup_{s \in L} f(s)
\]
where $f(s)$ is the String Substitution Function
(\S\ref{sec:string_substitution}). Regular
(\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are Closed under String
Substitution.



% --------------------------------------------------------------------
\subsection{Language Homomorphism}\label{sec:language_homomorphism}
% --------------------------------------------------------------------

A \emph{Homomorphism}, $h$, of a Language, $L$, with Alphabet,
$\Sigma$, is defined in terms of a String Homomorphism
(\S\ref{sec:string_homomorphism}) $f(s)$, where $s \in L$:
\[
  h(L) = \bigcup_{s \in L} f(s)
\]
and the Inverse Homomorphic Image is then:
\[
  h^{-1}(L) = \{ s\;|\; f(s) \in L \}
\]
In General $h(h^{-1}(L)) \neq L$, but:
\[
  h (h^{-1}(L)) \subseteq L
\] and:
\[
  L \subseteq h^{-1}(h(L))
\]
Regular (\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are Closed under Homomorphisms
and Inverse Homomorphisms.



% --------------------------------------------------------------------
\subsection{Language Projection}\label{sec:language_projection}
% --------------------------------------------------------------------

Using a String Projection (\S\ref{sec:string_projection}),
$\pi_{\Sigma}(s)$, on Strings, $s$, the \emph{Language Projection} for
a Language, $L$, is defined as:
\[
  \pi_{\Sigma}(L) = \{\pi_{\Sigma}(s)\;|\; s \in L\}
\]



% ====================================================================
\section{Formal Grammar}\label{sec:formal_grammar}
% ====================================================================

The term \emph{Syntax} may be used as a synonym for \emph{Grammar}:
i.e. a Set of Rules through which the Strings of a Language can be
either \emph{Generated} (\S\ref{sec:generative_grammar}) or
\emph{Recognized} (\S\ref{sec:determinative_grammar}).

Syntax is the aspect of Formal Languages that refers only to the
literal Strings of Symbols of a Language with no regard to the Meaning
(\S\ref{sec:meaning}) or Interpretation (\S\ref{sec:interpretation});
only the condition that they can be identified and differentiated from
one-another is required. The process of Syntactic Analysis is known as
\emph{Parsing} (\S\ref{sec:parser}).



% --------------------------------------------------------------------
\subsection{Generative Grammar}\label{sec:generative_grammar}
% --------------------------------------------------------------------

A \emph{Generative Grammar} \emph{Generates} a Language by the
repeated application of \emph{Production Rules} beginning with a
unique \emph{Non-terminal Symbol} called the Start Symbol, here
denoted $S$.

The definition of a Non-terminal Symbol in the context of a Generative
Grammar is one for which a Production Rule exists with that Symbol
appearing in the input and replaced in the output. A \emph{Terminal
  Symbol} is one for which no Production Rule exists with that Symbol
occuring in the input.

A sequence of rule applications is a \emph{Derivation} (cf.
\S\ref{sec:formal_proof} and \S\ref{sec:logical_argument}).

A Generative Grammar may be formally defined as a 4-tuple:
\[
  G(N,T,P,S)
\]
where $N$ are Non-terminal Symbols, $T$ are Terminal Symbols, $P$ are
Production Rules and $S$ is the Start Symbol.

An unrestricted Production Rule has the form:
\[
  (N \cup T)^*N(N \cup T)^* \rightarrow (N \cup T)^*
\]
That is, a Production is a function from one Expression to
another, where the left Expression must contain at least one
Non-terminal Symbol. By convention, Non-terminal Symbols
will be denoted by capitals ($A,B,C,\cdots$), and Terminals by
lowercase ($a,b,c,\cdots$), and Expressions by Greek letters
($\alpha,\beta,\gamma$).



% --------------------------------------------------------------------
\subsection{Determinative Grammar}\label{sec:determinative_grammar}
% --------------------------------------------------------------------

A \emph{Determinative Grammar} is a Grammar through which a Member of
$\Sigma^*$ can be determined to belong to a Language $L \subseteq
\Sigma^*$ (\emph{Recognized}, note this is different from the use of
\emph{Recognizable} (\S\ref{sec:recognizable}) in the case of
Monoids). Such systems are described by \emph{Automata Theory}
(\S\ref{sec:automata_theory})

Syntactic Monoid (\S\ref{sec:syntactic_monoid})



% --------------------------------------------------------------------
\subsection{Chomsky Hierarchy}\label{sec:chomsky_hierarchy}
\cite{chomsky56}
% --------------------------------------------------------------------

Grammars are classified by how restrictive the Production Rules are.
By convention, they may be organized into a hierarchy of Classes under
Proper Inclusion, where \emph{Type-0} is an Unrestricted Grammar,
covering all possible Formal Grammars:

  Type-0 $\supset$ Type-1 $\supset$ Type-2 $\supset$ Type-3 \\
These different levels in the hierarchy are Recognizable
(\S\ref{sec:determinative_grammar}) by different kinds of Automata
(\S\ref{sec:automaton}).

Type-0: Unrestricted (\S\ref{sec:unrestricted_grammar})

Type-1: Context-sensitive (\S\ref{sec:context_sensitive})

Type-2: Context-free (\S\ref{sec:context_free})

Type-3: Regular (\S\ref{sec:regular_language})

underlying the Hierarchy at a more restricted level than Regular
Grammars is \emph{Combinational Logic}
(\S\ref{sec:combinational_logic}) or ``Time-independent Logic'': a
type of ``Digital Logic'' implemented by ``Boolean Circuits'' where
the Output is a \emph{Pure Function} of the present Input
\emph{only}-- unlike Sequential Logic (\S\ref{sec:sequential_logic})
in which the Output depends on the present Input and also the
\emph{history} of past Input, i.e. \emph{Memory}

a \emph{Regular Language} can be Recognized by a \emph{Finite
  Automaton} (\S\ref{sec:finite_automaton})

a \emph{Context-free Language} can be Recognized by a Finite Automaton
with a Stack: a \emph{Pushdown Automaton}
(\S\ref{sec:pushdown_automaton})

a \emph{Context-sensitive Language} can be Recognized by a Finite
Automaton with two Stacks of bounded size: a \emph{Linear Bounded
  Automaton} (\S\ref{sec:linear_bounded_automaton})

an \emph{Unrestricted Language} can be Recognized by a Finite
Automaton with two Stacks of unbounded size: a \emph{Turing Machine}
(\S\ref{sec:turing_machine})



% --------------------------------------------------------------------
\subsection{Type-0: Unrestricted Grammar}\label{sec:unrestricted_grammar}
% --------------------------------------------------------------------

\subsubsection{Semi-decidable Language}\label{sec:semidecidable}

Production Rules of an \emph{Unrestricted Grammar} have the form
\[
  \alpha \rightarrow \beta
\]
where $\alpha$ and $\beta$ are Expressions of $N \cup T$ and $\alpha
\neq \varepsilon$. Note that this means there is not necessarily a
distinction between Terminal and Non-terminal Symbols in an
Unrestricted Grammar.

A completely Unrestricted Grammar generates a Language called
\emph{Recursively Enumerable} or \emph{Semi-decidable}
(\S\ref{sec:partial_computable}). This means membership of the
Language can be decided by an Algorithm, but non-membership cannot,
and the Class of Languages having this property is called
$\mathsf{RE}$.

The complement of $\mathsf{RE}$ is the Class of Languages for which an
Algorithm may decide non-membership only and is termed
$\mathsf{coRE}$. The Class of Automata capable of implementing these
Algorithms are Turing Machines (\S\ref{sec:turing_machine}).



\subsubsection{Decidable Language}\label{sec:decidable_language}

A \emph{Decidable} or \emph{Recursive} Language is defined as the
intersection of $\mathsf{RE}$ and $\mathsf{coRE}$:
\[
  \mathsf{R} = \mathsf{RE} \cap \mathsf{coRE}
\]
That is, it can be decided whether a Symbol is a member or not by a
Total Computable Function (\S\ref{sec:computable_function}). Decidable
Languages are Recognizable by a \emph{Decider} or Total Turing Machine
(\S\ref{sec:turing_machine}).\cite{kozen97}



% --------------------------------------------------------------------
\subsection{Type-1: Context-sensitive Grammar}
\label{sec:context_sensitive}
% --------------------------------------------------------------------

\emph{Context-sensitive Grammars} have the restriction that the result
of a Production is not shorter than the input. Formally stated,
Productions are of the form
\[
  \alpha \Gamma \beta \rightarrow \alpha \gamma \beta
\]
where $|\Gamma| \leq |\gamma|$. In this formulation $\alpha$ and
$\beta$ form the \emph{Context} of $\Gamma$.

Requiring that $S$ does not appear on the right of any Production
and allowing the rule
\[
  S \rightarrow \varepsilon
\]
makes the Context-sensitive Languages a proper Superset of the
Context-free Languages (\S\ref{sec:context_free}).

Context-sensitive Languages are equivalent to Linear Bounded Automata
(\S\ref{sec:linear_bounded_automaton}).



\subsubsection{Indexed Grammar}\label{subsubsection:indexed_grammar}

An \emph{Indexed Grammar} has an extra set of \emph{Index Symbols},
$F$, with Productions of three possible forms,
\[
  A[\sigma] \rightarrow \alpha[\sigma]
\]\[
  A[\sigma] \rightarrow B[f\sigma]
\]\[
  A[f\sigma] \rightarrow \alpha[\sigma]
\]
where $f \in F$ and $\sigma$ is a String of Index Symbols. The Index
Symbols are used to form a \emph{Stack} by the Production Rules where
Index Symbols are either pushed or popped from the Stack.

An Indexed Language can be Recognized by a Nested Stack Automaton
(\S\ref{sec:nested_stack_automaton}).\cite{aho69}



\subsubsection{Generalized Contex-free}
\label{sec:generalized_context_free}

A \emph{Generalized Context-free Grammar} adds to the Production Rules
of a Context-free Grammar a set of Non-context-free \emph{Composition
  Functions} that combine tuples of Symbols:
\[
  f(\langle x_1,\cdots,x_m\rangle,\cdots,\langle
  y_1,\cdots,y_n\rangle)=\gamma
\]
where $\gamma$ is a single tuple or another Composition Function that
reduces to a single tuple.

Rules are of the form:
\[
  A \rightarrow f(X,Y,\cdots)
\]
where $X$,$Y$,$\cdots$ are String tuples or Non-terminal Symbols.

There are several weakly equivalent Grammars to the composition
formulation:

\begin{description}
\item[Linear Context-free Rewriting System] \hfill \\
  Weakly equivalent to \emph{Multi-component Tree-adjoining Grammars}
  where Composition Functions are both \emph{Linear} and
  \emph{Regular}. Can be Recognized by Thread Automata
  (\S\ref{sec:thread_automaton})\cite{villemonte02}
  %FIXME may need to link to concepts of Linear and Regular

\item[Tree-adjoining] \hfill \\
  Elementary rewriting unit is a Tree (\S\ref{sec:tree_graph}) rather
  than a Symbol. Can be Recognized by Embedded Pushdown Automata
  (\S\ref{sec:embedded_pushdown})\cite{vijayashanker88}

\item[Linear Indexed Grammar] \hfill \\
  A modified Indexed Grammar where only one Symbol receives the
  Stack.

\item[Combinatory Categorical Grammar] \hfill \\
  A type of Phrase Structure Grammar (\S\ref{sec:context_free}) using
  Combinatory Logic (\S\ref{sec:combinatory_logic}).

\item[Head grammar] \hfill \\
  A Subset of the Linear Context-free Rewriting System and a Phrase
  Structure Grammar.
\end{description}



\subsubsection{Square Pattern}\label{subsubsection:square_pattern}

a String constructed of repeated Substrings, e.g. ``papa'',
``WikiWiki''

in POSIX Regular Expressions, the pattern for Squares is:
\[ \mono{(.+)\backslash1} \]
but this is \emph{not} a feature of Regular Languages as the Language
of Squares is not Context-free due to the \emph{Pumping Lemma}
%FIXME xref pumping lemma



% --------------------------------------------------------------------
\subsection{Type-2: Context-free Grammar}\label{sec:context_free}
% --------------------------------------------------------------------

\emph{Context-free Grammars} (\emph{CFG}s) have Production Rules of
the form:
\[
  V \rightarrow \alpha
\]
where $V$ is a single Non-terminal and $\alpha$ is a String of Terminals
and/or Non-terminals (or $\varepsilon$). Because $V$ is required to be a
single Non-terminal, the Production Rules can be applied regardless of
Context. Each Non-terminal in a Context-free Grammar, $G$, is said to
form a \emph{Sublanguage} of the Language defined by $G$.

A Context-free Language is Closed under String Substitution
(\S\ref{sec:string_substitution}).

Multiple Context-free Grammars may generate the same Language, so
Properties of CFGs may be termed \emph{Extrinsic} while Language
Properties are \emph{Intrinsic}. The question of Equality between CFGs
is Undecidable.

A Context-free Language may also be called a \emph{Recursive
  Language}.

A popular notation for Context-free Grammars in Computer Science is
\emph{Backus-Naur form} (\emph{BNF}).

Context-free Grammars are equivalent to Non-deterministic Pushdown
Automata (\S\ref{sec:pushdown_automaton}).

Spivak 16:

every Context Free Grammar is an Operad (\S\ref{sec:operad})



\subsubsection{Constituency Grammar}\label{sec:constituency_grammar}

In Linguistics, the term used for Context-free Grammar is \emph{Phrase
  Structure Grammar} which is also called \emph{Constituency Grammar}
due to the one-to-one-or-many correspondence between the Productions
(ultimately rooted in the \emph{Subject-Predicate Clause} derived from
Term Logic \S\ref{sec:term_logic}). A Parse Tree
(\S\ref{sec:concrete_syntax}) may be constructed according to the
\emph{Constituency Relation} of a Constituency Grammar.



\paragraph{Transformational Grammar}\label{sec:transformational_grammar}
\hfill



\paragraph{Categorial Grammar}\label{sec:categorial_grammar}\hfill

Montague Grammar (\S\ref{sec:montague_grammar})

Non-symmetric Compact Closed Category
(\S\ref{sec:nonsymmetric_compact_closed})

\fist Grammatical Inference (\S\ref{sec:grammatical_inference})

\fist Simply-typed $\lambda$-calculus (\S\ref{sec:simply_typed})



\subparagraph{Pregroup Grammar}\label{sec:pregroup_grammar}\hfill

\url{https://johncarlosbaez.wordpress.com/2018/02/11/linguistics-using-category-theory/}



\subparagraph{Lambek Grammar}\label{sec:lambek_grammar}\hfill

\emph{Lambek Calculus}

Type-logical Semantics (\S\ref{sec:typelogical_semantics})

Symmetric Monoidal Categories (\S\ref{sec:symmetric_monoidal})

\fist Non-commutative Logic (\S\ref{sec:noncommutative_logic}) is an
extension of Linear Logic (\S\ref{sec:linear_logic}) combining the
Commutative Connectives of Linear Logic with the Noncommutative
Multiplicative Connectives of Lambek Calculus



\subparagraph{Combinatory Categorial Grammar}\hfill
\label{sec:combinatory_categorial}



\paragraph{Lexical Functional Grammar}\label{sec:lexical_functional}
\hfill

Glue Semantics (\S\ref{sec:glue_semantics})



\subsubsection{Dependency Grammar}\label{sec:dependency_grammar}

An alternative formulation to Phrase Structure Grammar is
\emph{Dependency Grammar} in which the Verb is the root and there is a
one-to-one correspondence between Symbols and nodes in the Syntax
Structure (\S\ref{sec:concrete_syntax}).



\subsubsection{Deterministic}\label{sec:deterministic_cfg}

\emph{Deterministic Context-free Grammars} are derived from
Deterministic Pushdown Automata (\S\ref{sec:deterministic_pda}) and
are always unambiguous. They can be Parsed in Linear Time and a Parser
can be automatically generated from the Grammar by a Parser Generator
(\S\ref{sec:parser_generator}).



\subsubsection{Visibly Pushdown}\label{sec:visibly_pushdown}

\emph{Visibly Pushdown Grammars} are described by the 4-tuple
\[
  G = (V=V^0 \cup V^1,T,P,S)
\]
where $V^0$ and $V^1$ are Disjoint Sets of Non-terminals and there
are three kinds of Production Rules:
\[
  X \rightarrow \varepsilon
\]\[
  X \rightarrow aY
\]\[
  X \rightarrow \langle aZb \rangle Y
\]
where $Z \in V^0$ and if $X \in V^0$ then $Y \in V^0$

The resulting Language is a \emph{Regular Language} with \emph{Nested
  Words}, described by a Monadic Second-order Logic
(\S\ref{sec:monadic_secondorder}).
%FIXME possibly reference or explain nested words



% --------------------------------------------------------------------
\subsection{Type-3: Regular Grammar} \label{sec:regular_language}
% --------------------------------------------------------------------

\emph{Regular Languages} (or \emph{Token-level Languages}) are more
restricted than Context-free Languages (\S\ref{sec:context_free}) and
satisfy a number of Closure Properties. For two Regular Languages, $K$
and $L$, the following operations result in a Language that is also
Regular:
\[
  K \cup L, \quad
  K \cap L, \quad
  \overline{L}, \quad
  K - L, \quad
  K \circ L, \quad
  L^*, \quad
  K / L, \quad
  L^R
\]
Like Context-free Languages, Regular Languages are Closed under String
Substitution (\S\ref{sec:string_substitution}).

A common formulation of Regular Languages is using \emph{Regular
  Expressions} (\S\ref{sec:regular_expression}) and conversely it is
sometimes said that a Regular Language is one that can be defined by a
Regular Expression. If there is at least one Regular Expression that
matches a Set of Strings, then there exists an Infinite number of
other Regular Expressions that also match that Set.

An algebraic description is as follows:
\[
  L = \{ w \in \Sigma^* | f(w) \in N \}
\]
where $f : \Sigma^* \rightarrow M$ is a Monoid Homomorphism of Finite
Monoid (\S\ref{sec:monoid}) $M$ and $N \subseteq M$.



\subsubsection{Regular Expression}\label{sec:regular_expression}

\emph{Regular Expression}

if there is at least one Regular Expression that matches a Set of
Strings, then there exists an Infinite number of other Regular
Expressions that also match that Set



\paragraph{Star Height}\label{sec:star_height}\hfill

\emph{Star Height}



\subsubsection{Regular Language Induction}
\label{sec:regular_language_induction}

not all Regular Languages can be Induced



\subsubsection{Extended Regular}\label{sec:extended_regular}

\emph{Extended Regular Grammars} have Productions of either \emph{Right
Regular} or \emph{Left Regular} form.

Right:
\[
  B \rightarrow a
\]\[
  A \rightarrow B \nu
\]\[
  A \rightarrow \varepsilon
\]

Left:
\[
  A \rightarrow a
\]\[
  A \rightarrow B \nu
\]\[
  A \rightarrow \varepsilon
\]
where $a$ is a single Non-terminal and $\nu$ is an expression of only
Non-terminal characters.



\subsubsection{Strictly Regular}\label{sec:strictly_regular}

\emph{Strictly Regular Grammars} also have Productions of either Right
Regular or Left Regular form.

Right:
\[
  B \rightarrow a
\]\[
  B \rightarrow aC
\]\[
  B \rightarrow \varepsilon
\]

Left:
\[
  A \rightarrow a
\]\[
  A \rightarrow Ba
\]\[
  A \rightarrow \varepsilon
\]
where $a$ is a single Non-terminal.

There is a one-to-one correspondence between the rules of a
\emph{Strictly Left Regular Grammar} and those of a Non-deterministic
Finite Automaton (\S\ref{sec:ndfa}).

The \emph{Pumping Lemma} states that the middle section of an
Expression within a Regular Language may be repeated an arbitrary
number of times to produce another Expression in that same Language;
cf. Squares (\S\ref{sec:square_pattern}) in Context-sensitive
Languages



\paragraph{k-Testable}\label{sec:k_testable}\hfill

A \emph{k-Testable Language} is one where membership of an Expression
depends on the first and last Symbol and a Set of Factors of length
$k$. An example is a \emph{Local Language} which is a \emph{2-Testable
  Language} described by the Regular Expression:
\[
  (Q\Sigma^* \cap \Sigma^*R)\setminus\Sigma^*F\Sigma^*
\]
where $Q,R \subseteq \Sigma$ and $F \subseteq \Sigma \times \Sigma$.
This requires for an Expression, $w$, that is a member of a Local
Language to have its first Symbol in $Q$, and its second Symbol in
$R$, and no factor of $w$ of length 2 is in $F$. A Local Language is
Recognized by a \emph{Local Automaton} (\S\ref{sec:dfa}).



\subsubsection{Star-free}\label{sec:starfree_grammar}

A \emph{Star-free Language} is one having a Generalized Star Height
(\S\ref{sec:star_height}) equal to zero, that is, the minimal Star
Height of all Expressions in the Language with the Star Height of an
Expression's \emph{Complement} being equal.
% FIXME complement: ref or explain

Star-free Languages are characterized as those with Aperiodic
Syntactic Monoids
(\S\ref{sec:syntactic_monoid})\cite{schutzenberger65} and also as the
\emph{Counter-free Languages}\cite{mcnaughton-papert71} by the
Aperiodic Finite-state Automaton (\S\ref{sec:aperiodic_automaton}) and
Linear Temporal Logic (\S\ref{sec:linear_temporal}).



\subsubsection{Regular Tree Grammar}\label{sec:regular_tree_grammar}

generalizes Regular Grammars from Strings to Trees



% --------------------------------------------------------------------
\subsection{Affix Grammar}\label{sec:affix_grammar}
% --------------------------------------------------------------------

\emph{Affix Grammars} are those of a Context-free Grammar
(\S\ref{sec:context_free}) with a Subset of the Non-terminals used as
\emph{Affix Arguments}. If the same Affix appears multiple places in a
Production, the value must be the same.



\subsubsection{Two-level Grammars}\label{sec:two_level_grammar}

Allowing the values for Affixes to be described by a Context-free
Grammar (\S\ref{sec:context_free}) results in a Two-level Grammar.
Two-level Grammars are \emph{Grammar Generators} that may
\emph{Generate} Grammars with infinite rules.



\begin{description}
\item[W-grammar] \emph{Van Wijngaarden Grammar} consists of a finite
  Set of \emph{Meta-rules} used to derive a possibly infinite set of
  Production Rules from a finite Set of \emph{Hyper-rules}.
\item[Extended Affix Grammar] is a restricted W-grammar.
\end{description}



\subsubsection{Attribute Grammar}\label{sec:attribute_grammar}
\cite{slonneger-kurtz95}

\emph{Attribute Grammars} allow for Affixes (called \emph{Attributes})
to take Values from arbitrary Domains and for Functions to calculate
Values of those Attributes. Attribute Grammars can be seen as
Context-free Grammars (\S\ref{sec:context_free}) extended to provide
Context-sensitivity using a Set of \emph{Attributes}, \emph{Attribute
  Values}, \emph{Evaluation Rules} and \emph{Conditions}.

A finite, possibly empty Set of Attributes is associated with each
distinct Symbol in the Grammar. Each Attribute has an associated
Domain of Values they may take on. Attributes are either
\emph{Synthesized Attributes} (\S\ref{sec:synthesized_attribute}) or
\emph{Inherited Attributes} (\S\ref{sec:inherited_attribute}).

Production Rules have additional Conditions that must be met by the
Attribute Values at that Node in the Parse Tree
(\S\ref{sec:concrete_syntax}).

In the context of Computer Science, Attribute Grammars can formally
express the rules for Type Checking and Declaration Order, as well as
the Operational Semantics of Programming Languages
(\S\ref{sec:operational_semantics}).

\emph{L-attributed Grammar}

\emph{LR-attributed Grammar}

\emph{ECLR-attributed Grammar}

\emph{S-attributed Grammar}



\paragraph{Synthesized Attribute}\label{sec:synthesized_attribute}\hfill

A \emph{Synthesized Attribute} is passed from Child Node to Parent
Node.



\paragraph{Inherited Attribute}\label{sec:inherited_attribute}\hfill

An \emph{Inherited Attribute} is passed from Parent Node to Child
Node.



% --------------------------------------------------------------------
\subsection{Analytic Grammar}\label{sec:analytic_grammar}
% --------------------------------------------------------------------

\emph{Analytic Grammars} are used in Parsing (\S\ref{sec:parser}). A
few examples:

\begin{description}
\item[Top-Down Parsing Language] \hfill \\
  Formal representation of Recursive Descent Parser
  (\S\ref{sec:recursive_descent}). Production rules of the form
  \[
    A \leftarrow \varepsilon
  \]\[
    A \leftarrow f
  \]\[
    A \leftarrow a
  \]\[
    A \leftarrow BC/D
  \]
  \emph{Packrat Parsing} is a method for implementing \emph{Linear-time
    Parsers} for Grammars defined in Top-down Parsing Language -- \fist see
  \url{https://pdos.csail.mit.edu/~baford/packrat/thesis/},
  \url{http://scymtym.github.io/esrap/} (Common Lisp implementation)
\item[Parsing Expression Generator] (\S\ref{sec:parsing_expression_grammar})
  \hfill \\ A more generalized Top-Down Parsing Language
  (\S\ref{sec:topdown_parser}).
\item[Link Grammar] \hfill \\
  Dependency Grammar (\S\ref{sec:dependency_grammar}) with
  directionality between Symbols.
\end{description}



\subsubsection{Parsing Expression Grammar}\label{sec:parsing_expression_grammar}

Top-down Parsing Languages



% --------------------------------------------------------------------
\subsection{Adaptive Grammar}\label{sec:adaptive_grammar}
% --------------------------------------------------------------------

\emph{Adaptive Grammars} allow for Production Rules to be manipulated
within the Grammar, including addition, deletion, and modification of
Rules.



\subsubsection{Imperative Adaptive Grammar}
\label{sec:imperative_adaptive}

Global

Rule changes are based on global State changing over time.

\begin{itemize}
  \item Extensible Context-Free Grammars
  \item Top-down Modifiable Grammars
  \item Bottom-up Modifiable Grammars
\end{itemize}



\subsubsection{Declarative Adaptive Grammar}
\label{sec:declarative_adaptive}

Local

Rule changes only affect the position in the Syntax Tree of the
generation of a String.

\begin{itemize}
  \item Christiansen Grammars
  \item Recursive Adaptive Grammars
\end{itemize}



\subsubsection{Time-space (Hybrid) Adaptive Grammar}
\label{sec:timespace_adaptive}

\begin{itemize}
  \item \S-Calculus
\end{itemize}



\subsubsection{Dynamic Grammars}\label{sec:dynamic_grammar}

Boullier\cite{boullier94}



% ====================================================================
\section{Abstract Reduction System}\label{sec:abstract_rewrite}
% ====================================================================

Formal Grammars may be abstracted as \emph{Abstract Reduction Systems}
(or \emph{Abstract Rewrite Systems}), abbreviated $ARS$. An ARS is
simply:
\[
  (A,\rightarrow)
\]
where $A$ is a Set of Objects (Expressions) and $\rightarrow \subseteq
A \times A$ is a Binary Relation on those Objects called the
\emph{Reduction Relation} (\S\ref{sec:reduction_relation}). The Object
appearing in the right-hand side of a Reduction Relation is called a
\emph{Reduct} (or \emph{Expansion}) of the left-hand side. This is
equivalent to an Unlabelled State Transition System
(\S\ref{sec:state_transition}). Indexed Abstract Reduction Systems
(\S\ref{sec:indexed_rewrite}) coincide with Labelled State
Transition Systems (\S\ref{sec:labelled_transition})

A \emph{Reduction Sequence}, $x \rightarrow y \rightarrow z$, is
denoted $x \stackrel{*}\rightarrow z$, where $\stackrel{*}\rightarrow$
is the Reflexive Transitive Closure of $\rightarrow$ (see below).

Given the Reduction Relation, $\rightarrow$, for an ARS, the following
Relations may be defined:

\begin{description}

\item [$\stackrel{*}\rightarrow$]: Reflexive Transitive Closure of
  $\rightarrow$, the smallest Preorder (Reflexive and Transitive
  Relation) containing $\rightarrow$, that is, the Transitive Closure
  of $\rightarrow \cup =$:
  \[ (\rightarrow \cup =)^+ =
  \bigcup_{i \in \{1,2,3,...\}} (\rightarrow \cup =)^i \]

\item [$\leftrightarrow$]: Symmetric Closure of $\rightarrow$:
  \[ \rightarrow \cup \rightarrow^{-1} \]

\item [$\stackrel{*}\leftrightarrow$]: Reflexive Transitive
  Symmetric Closure of $\rightarrow$ (smallest Equivalence Relation
  containing $\rightarrow$), that is, the Transitive Closure of
  $\leftrightarrow \cup =$:
  \[ (\leftrightarrow \cup =)^+ =
  \bigcup_{i \in \{1,2,3,...\}} (\leftrightarrow \cup =)^i \]

\end{description}



% --------------------------------------------------------------------
\subsection{Indexed Abstract Reduction System}
\label{sec:indexed_rewrite}
% --------------------------------------------------------------------

An \emph{Indexed Abstract Reduction System} differentiates Reductions
into Classes so that $\rightarrow$ is the Indexed Union of these
relations:
\[
  (A, \rightarrow_1, \rightarrow_2, \cdots)
\]
This is identical to a Labelled Transition System
(\S\ref{sec:labelled_transition}).



% --------------------------------------------------------------------
\subsection{Reduction Relation}\label{sec:reduction_relation}
% --------------------------------------------------------------------

Operational Semantics (\S\ref{sec:operational_semantics})



\subsubsection{Rewrite Relation}\label{sec:rewrite_relation}

\emph{Rewrite Relation}

\emph{Rewrite Preorder}

\emph{Reduction Preorder}

\emph{Rewrite Closure}



% --------------------------------------------------------------------
\subsection{Normal Form}\label{sec:normal_form}
% --------------------------------------------------------------------

An Object of an Abstract Rewriting System $(A,\rightarrow)$ is in
\emph{Normal Form} (\emph{Irreducible}) if it cannot be Rewritten
further. That is, $x \in A$ is in Normal Form if $\nexists y \in A : x
\rightarrow y$. Otherwise $x$ is \emph{Reducible}.

For two Objects $x,y \in A$, $y$ is a Normal Form of $x$ if $x
\stackrel{*}{\rightarrow} y$ and $y$ is Irreducible. A unique Normal
Form of $x$ is denoted $x \downarrow$.



% --------------------------------------------------------------------
\subsection{Normalization}\label{sec:normalization}
% --------------------------------------------------------------------

An Object of an ARS is \emph{Weakly Normalizing} if some Rewrite
sequence leads to a Normal Form and \emph{Strongly Normalizing} if
every Rewrite sequence leads to a Normal Form.

An ARS is \emph{Normalizing} if every Object has at least one Normal
Form, and likewise is Weakly or Strongly Normalizing if every Object
is Weakly or Strongly Normalizing. A Strongly Normalizing ARS always
Terminates (\S\ref{sec:rewrite_termination}) and is therefore not Turing
Complete and cannot be used as a Self-interpreter (in Programming
Languages).

Untyped $\lambda$-calculus (\S\ref{sec:untyped_lambda}) lacks the
Normalization Property.

Systems of Typed $\lambda$-calculus such as Simply-typed
$\lambda$-calculus (\S\ref{sec:simply_typed}) and Calculus of
Constructions (\S\ref{sec:coc}) are Strongly Normalizing.

Barendregt-Guevers-Klop Conjecture: every Weakly Normalizing Pure Type
System (\S\ref{sec:pts}) has the Strong Normalization Property



\subsubsection{Termination}\label{sec:rewrite_termination}

If all Objects of an ARS are Strongly Normalizing, then the system
itself is called \emph{Strongly Normalizing} (or \emph{Terminating} or
\emph{Noetherian}, cf. Noetherian Relation
\S\ref{sec:noetherian_relation}) and has no infinite Reduction
Sequences.



% --------------------------------------------------------------------
\subsection{Joinability}\label{sec:rewrite_join}
% --------------------------------------------------------------------

For an ARS $(A, \rightarrow)$, two Objects $x,y\in A$ are
\emph{Joinable} when:
\[
  \exists z \in A :
  x \stackrel{*}\rightarrow z \stackrel{*}\leftarrow y
\]
The \emph{Joinability Relation} can be defined as:
\[
  \stackrel{*}\rightarrow \circ \stackrel{*}\leftarrow
\]
where $\circ$ is Relation Composition
(\S\ref{sec:relation_composition}). As a Binary Relation, Joinability
is denoted $x \downarrow y$.

An ARS has the \emph{Church-Rosser Property} if and only if:
\[
  \forall x,y \in A, x \stackrel{*}\leftrightarrow y
  \Rightarrow x \downarrow y
\]
Equivalently, the Church-Rosser Property can be expressed as:
\[
  \stackrel{*}\leftrightarrow \subseteq \downarrow
\]
The Church-Rosser Property is equivalent to Confluence
(\S\ref{sec:rewrite_confluence}).

In a Church-Rosser system, an Object can have zero or one Normal
Forms. By the \emph{Church-Rosser Theorem}, $\lambda$-calculus
(\S\ref{sec:untyped_lambda}) has this property, which means that
Evaluation ($\beta$-reduction) can be carried out in any order. See
also Rosser's Trick (\S\ref{sec:rossers_trick}).



% --------------------------------------------------------------------
\subsection{Confluence}\label{sec:rewrite_confluence}
% --------------------------------------------------------------------

An ARS $(A, \rightarrow)$ is \emph{Confluent} (or \emph{Globally
  Confluent}) if and only if:
\[
  \forall w,x,y \in A,
  x \stackrel{*}\leftarrow w \stackrel{*}\rightarrow y
  \Rightarrow x \downarrow y
\]
That is, for any two diverging paths, they eventually meet at a common
successor. An individual Object can be said to be Confluent if it has
this property.

\emph{Semi-confluent}:
\[
  \forall w,x,y \in A,
  x \leftarrow w \stackrel{*}\rightarrow y
  \Rightarrow x \downarrow y
\]

\emph{Weakly Confluent} (or \emph{Locally Confluent}):
\[
  \forall w,x,y \in A,
  x \leftarrow w \rightarrow y \Rightarrow x \downarrow y
\]

\emph{Strongly Confluent}
\[
  \forall w,x,y \in A,
  x \leftarrow w \rightarrow y \Rightarrow
  \exists z \in A : x \stackrel{*}\rightarrow z \wedge
  (y \rightarrow z \vee y = z)
\]

A Semi-confluent system is necessarily Confluent. The Church-Rosser
Property, Confluence, and Semi-confluence are all equivalent
properties for an ARS. If Confluence is restricted to single
Reductions (rather than Reduction Sequences), the stronger property is
called the \emph{Diamond Property}.

A Reduction Relation $\rightarrow$ is Confluent if and only if
$\stackrel{*}\rightarrow$ is Locally Confluent.

By \emph{Newman's Lemma}, if an ARS is Locally Confluent and
Terminating, then it is Confluent.

Reduction of Polynomials Modulo an Ideal is a Confluent Rewrite System
when working with a Grobner Basis (\S\ref{sec:ring_ideal}).



% --------------------------------------------------------------------
\subsection{Convergence}\label{sec:rewrite_convergence}
% --------------------------------------------------------------------

An ARS that is both Confluent (\S\ref{sec:rewrite_confluence}) and
Terminating (\S\ref{sec:rewrite_termination}) is called \emph{Convergent} (or
\emph{Canonical}) and every Object has a unique Normal Form
(\S\ref{sec:normal_form}).



% --------------------------------------------------------------------
\subsection{String Rewriting System}\label{sec:string_rewriting}
% --------------------------------------------------------------------

A \emph{String Rewriting System} (\emph{SRS}, also \emph{Semi-Thue
  System}) is defined by a Tuple $(\Sigma, R)$ with Alphabet $\Sigma$
and Binary Relation $R$ corresponding to the Rewrite Rules between
some fixed Strings of Alphabet Symbols.

Monoid Presentation (\S\ref{sec:presentation})

Markov Algorithm (\S\ref{sec:markov_algorithm})



% --------------------------------------------------------------------
\subsection{Graph Rewriting System}\label{sec:graph_rewriting}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Term Rewriting System}\label{sec:term_rewriting}
% --------------------------------------------------------------------

A \emph{Term Rewriting System} is an Abstract Rewriting System with
\emph{Terms} (Formal Logic\S\ref{sec:term}) as Objects and a Rewrite
Relation, $\rightarrow_R$, given by the Set of \emph{Term Rewriting
  Rules}, $R$, of the form:
\[
  l \rightarrow r
\]
where $l$ and $r$ are a pair of Terms. Such a Rule is \emph{Applied}
to a Term $s$ if $l$ matches a Subterm of $s$.

Termination (\S\ref{sec:rewrite_termination}) is Decidable for Finite Ground
Systems, but Undecidable for Systems with a single Rule consisting of
a Linear (\S\ref{sec:linear_term}) left-hand side, or for Systems with
only Unary Function Symbols. % FIXME clarify finite ground system



\subsubsection{Co-hygiene}\label{sec:cohygiene}

\url{https://fexpr.blogspot.com/2014/04/why-is-beta-substitution-like-higgs.html}

\url{https://fexpr.blogspot.com/2015/06/thinking-outside-quantum-box.html#sec-qt-hyg}

\url{https://fexpr.blogspot.com/2016/06/the-co-hygiene-principle.html}

\url{http://fexpr.blogspot.com/2017/06/co-hygiene-and-quantum-gravity.html}

analogy between \emph{Variable Substitution} (Formal Logic \S
Substitution \ref{sec:substitution}, $\lambda$-calculus \S
Substitution \ref{sec:labmda_substitution}) in Term Rewriting Systems
and \emph{Fundamental Forces} in Physics

Vau Calculus (\S\ref{sec:vau_calculus})



% --------------------------------------------------------------------
\subsection{Parallel Rewriting System}\label{sec:parallel_rewriting}
% --------------------------------------------------------------------

\subsubsection{L-system}\label{sec:l_system}

or \emph{Lindenmayer System}

Fractals (\S\ref{sec:fractal})



% ====================================================================
\section{Parser} \label{sec:parser}
% ====================================================================

\emph{Parsing} is the process of \emph{Syntactic Analysis}. A
\emph{Parser} analyzes Expressions according to the rules of a Formal
Grammar, generating a Data Structure (\S\ref{sec:f_algebra})
describing the Syntax of the input.

The Data Structure produced by the Parser may be a \emph{Concrete
  Syntax Tree} (Parse Tree \S\ref{sec:concrete_syntax}), an
\emph{Abstract Syntax Tree} (Syntax Tree \S\ref{sec:abstract_syntax}),
or other hierarchical structure.



% --------------------------------------------------------------------
\subsection{Concrete Syntax Tree}\label{sec:concrete_syntax}
% --------------------------------------------------------------------

A \emph{Concrete Syntax Tree} (or \emph{Parse Tree} or
\emph{Derivation Tree}) is an Ordered, Rooted Tree
(\S\ref{sec:tree_graph}) representing the \emph{Syntactic Structure}
of a String according to a Context-free Grammar
(\S\ref{sec:context_free}) such as the Constituency Relation of a
Constituency Grammar (\S\ref{sec:constituency_grammar}) or the
Depenency Relation of a Dependency Grammar
(\S\ref{sec:dependency_grammar})


2016 - Shulman - ``What is a Formal Proof'': %FIXME

In Type Theory, Derivation Trees (Concrete Syntax Trees
\S\ref{sec:concrete_syntax}) are Inductively Defined Structures that
can be used to Prove a Soundness Theorem.

Terms as One-dimensional Syntactic Representations of Derivation Trees
(or parts of them)

Notion of ``Term'' plays the role of Informal ``Argument'';
``Type-checking'' is about getting from Argument to Proof

``Well-typed Terms'' (those that represent Derivation Trees) are
distinguished from larger Class of ``Untyped Terms''

Untyped Terms are a sort of Argument that may or may not give rise to
a Proof

\emph{Decidable Static Type-Checking}: Decision Algorithm on Untyped
Terms determining whether the Term represents a Derivation Tree or not
in a Finite amount of time

``Decidable Checking'' is not a fundamental Property of a Proofs in a
Formal System but a Property of a certain Class (???) of Arguments
that might or might not represent Proofs.



% --------------------------------------------------------------------
\subsection{Abstract Syntax Tree}\label{sec:abstract_syntax}
% --------------------------------------------------------------------

\emph{Abstract Syntax} is a representation of the Source (Concrete)
Syntax being Parsed.

\emph{First-order Abstract Syntax} (FOAS), \emph{Higher-order Abstract
  Syntax} (HOAS)

\fist Zephyr Abstract Syntax Description Language (ASDL) --
(Wang,Appel,Korn,Serra97)



\subsubsection{First-order Abstract Syntax}\label{sec:foas}

\emph{FOAS}

Abstract Structure

Concrete Names (Identifiers): requiring Name Resolution
(\S\ref{sec:name_resolution})

relation between Binding Site and Use indicated by using the same
Identifier



\subsubsection{Higher-order Abstract Syntax}\label{sec:hoas}

\emph{HOAS}

Abstract Structure

Abstract Names

Languages with Variable Binders %FIXME

each Use refers directly to Binding Site

$\alpha$-equivalent Programs have identical representations in HOAS

de Bruijn Indices (\S\ref{sec:debruijn_index})



\paragraph{Parametric Higher-order Abstract Syntax}
\label{sec:phoas}\hfill



\subsubsection{Name Binding}\label{sec:name_binding}

%FIXME
Free Variable (\S\ref{sec:free_variable})



\paragraph{Name Resolution}\label{sec:name_resolution}\hfill

%FIXME merge with name binding? wikipedia



\subsubsection{Homoiconicity}\label{sec:homoiconicity}

A Language for which the Abstract Syntax Tree and the Syntax are
Isomorphic is called \emph{Homoiconic}.

Circular Defenition (\S\ref{sec:circular_definition})

Reflection (\S\ref{sec:reflective_subcategory})



% --------------------------------------------------------------------
\subsection{Syntactic Analysis}\label{sec:syntactic_analysis}
% --------------------------------------------------------------------

\emph{Syntactic Analysis} is the process of Parsing input. The Parser
determines if and how the input can be derived from the Start Symbol
of a Context-free Grammar. Parsing can proceed in two directions:
\begin{description}
\item [Top-down Parsing] (\S\ref{sec:topdown_parser}) starts with
  the highest level of the Parse Tree (\S\ref{sec:concrete_syntax}).
  Proceeds greedily and may be \emph{Exponential} with
  \emph{Backtracking}.
\item [Bottom-up Parsing] (\S\ref{sec:bottomup_parser}) starts with
  the lowest level of the Parse Tree.
\end{description}
The output of Syntactic Analysis is a Data Structure
(\S\ref{sec:f_algebra}) such as a Concrete or Abstract Syntax Tree.
Further Context-sensitive Parsing may follow
(\S\ref{sec:semantic_analysis}).



\subsubsection{Lexical Analysis}\label{sec:lexical_analysis}

A Parser may be preceded by a \emph{Lexical Analyzer} (also
\emph{Lexer} or \emph{Tokenizer}) which creates \emph{Tokens} from
input Expressions. A Token is a structure representing a \emph{Lexeme}
(that is, a String forming a Syntactic unit) and associated
\emph{Type} and \emph{Value} information.

A Lexical Analyzer is itself a Parser and usually the \emph{Lexical
  Syntax} is defined by a Regular Language
(\S\ref{sec:regular_language}). The Tokens are then analyzed by the
Parser according to the rules of the \emph{Phrase Syntax}, which is
usualy a Context-free Language (\S\ref{sec:context_free}). A Parser
without a separate Lexer is called a \emph{Scannerless Parser}.

Lexing may be divided into two stages: \emph{Scanning} and
\emph{Evaluation}. Prior to Tokenization, a \emph{Scanner} (usually a
Finite-state Machine \S\ref{sec:finite_automaton}) may perform its own
Lexical Analysis, producing Lexemes categorized by Token class. An
\emph{Evaluator} then produces the final Tokens by either adding Value
information to the Type to produce a Type-Value pair, or returning
just the Type, or possibly also suppressing the Lexeme so the Parser
doesn't see it (e.g. whitespace or comments).

Token identification methods include Regular Expressions
(\S\ref{sec:regular_expression}), \emph{Flags} (specific sequences),
\emph{Delimiters} (specific separators), or explicit
\emph{Dictionaries}.



\subsubsection{Semantic Analysis}\label{sec:semantic_analysis}

\emph{Semantic Analysis} (or \emph{Symantic Parsing} or
\emph{Contextual Analysis}) may be performed after Syntactic Analysis.
Parsing for Context-sensitive Semantics, such as Type Checking or
declaration order, can be formally expressed with Attribute Grammars
(\S\ref{sec:attribute_grammar}).

A further example would be the in the resulting actions of Evaluation
in an Interpreter or Code Generation of a Compiler for a Programming
Language, for which purpose an Attribute Grammar may also be used.



% --------------------------------------------------------------------
\subsection{Top-down Parser}\label{sec:topdown_parser}
% --------------------------------------------------------------------

\subsubsection{Recursive Descent Parser}\label{sec:recursive_descent}

Mutual (Indirect) Recursion (\S\ref{sec:indirect_recursion})



\subsubsection{LL Parser}\label{sec:ll_parser}

\subsubsection{Early Parser}\label{sec:early_parser}



% --------------------------------------------------------------------
\subsection{Bottom-up Parsers}\label{sec:bottomup_parser}
% --------------------------------------------------------------------

\subsubsection{Precedence Parser}\label{sec:precedence_parser}

\subsubsection{LR Parser}\label{sec:lr_parser}

\emph{Canonical LR} LR(1)

\paragraph{SLR Parser}\label{sec:slr_parser}\hfill

\paragraph{LALR Parser}\label{sec:lalr_parser}\hfill

\paragraph{GLR Parser}\label{sec:glr_parser}\hfill

Generalized LR

Unbounded Look-ahead

supports arbitrary Context-free Grammars and compositions thereof

Syntax Definition Formalism (SDF) -- used in specification of Nix expression
language



\subsubsection{CYK Parser}\label{sec:cyk_parser}

\subsubsection{Recursive Ascent Parser}\label{sec:recursive_ascent}



% --------------------------------------------------------------------
\subsection{Chart Parser}\label{sec:chart_parser}
% --------------------------------------------------------------------

uses Dynamic Programming (\S\ref{sec:dynamic_optimization}) which eliminates
Backtracking and prevents a Combinatorial Explosion



\subsubsection{Earley Parser}\label{sec:earley_parser}

can parse all Context-free Languages

general case: Cubic Time $O(n^3)$ where $n$ is length of Parsed String

unambiguous Grammars: Quadratic Time $O(n^2)$

Linear Time for ``almost all'' $LR(k)$ Grammars %FIXME xref

performs well for Left-recursive Rules



% --------------------------------------------------------------------
\subsection{Parser Generator}\label{sec:parser_generator}
% --------------------------------------------------------------------

A \emph{Parser Generator} takes as a Grammar (for example a BNF
Grammar) and outputs the source code of a Parser for the Language
specified by the Grammar.



% --------------------------------------------------------------------
\subsection{Template Processing}\label{sec:template_processing}
% --------------------------------------------------------------------



% ====================================================================
\section{Automata Theory}\label{sec:automata_theory}
% ====================================================================

%FIXME section needs a rewrite or cleanup

Combinational Logic (\S\ref{sec:combinational_logic})
$\subset$ Finite State Machine (\S\ref{sec:finite_automaton})
$\subset$ Pushdown Automaton (\S\ref{sec:pushdown_automaton})
$\subset$ Turing Machine (\S\ref{sec:turing_machine})

Sequential Logic (\S\ref{sec:sequential_logic})

%FIXME does sequential logic subsume all the automatons?



% --------------------------------------------------------------------
\subsection{Combinational Logic}\label{sec:combinational_logic}
% --------------------------------------------------------------------

or \emph{Time-independent Logic}

Digital Logic

Output depends only on present Input

in Sequential Logic (\S\ref{sec:sequential_logic}) the Output also
depends on the history of Input (Memory)

%FIXME move to formal logic?



% --------------------------------------------------------------------
\subsection{Sequential Logic}\label{sec:sequential_logic}
% --------------------------------------------------------------------

%FIXME does sequential logic subsume all the automatons?

Finite State Machines (\S\ref{sec:finite_automaton})

Digital Circuit Theory: Output depends on present Value of Input
Signals and on the Sequence of past Inputs (the Input History)

\emph{State} (``Memory'')



\subsubsection{Synchronous Logic}\label{sec:synchronous_logic}

State only changes at discrete Times in response to ``Clock Signal''



\subsubsection{Asynchronous Logic}\label{sec:asynchronous_logic}

State can change at any Time in response to changing Inputs



% --------------------------------------------------------------------
\subsection{Abstract Machine} \label{sec:abstract_machine}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{State Transition Systems}\label{sec:state_transition}
% --------------------------------------------------------------------

A \emph{State Transition System} can have an infinite number of
\emph{States} and \emph{Transitions}, represented as the pair:
\[
  (S,\rightarrow)
\]
where $S$ is a set of States and $\rightarrow \subseteq S \times S$.
This is identical to an Un-indexed Abstract Rewriting System
(\S\ref{sec:abstract_rewrite}).

``Infinite State Machine''; \fist Cf. Finite State Machines (Finite
Automata \S\ref{sec:finite_automaton})

\emph{Labelled State Transition Systems}
(\S\ref{sec:labelled_transition}) have an additional set of
\emph{Labels}, $\Lambda$:
\[
  (S,\Lambda,\rightarrow)
\]
and $\rightarrow \subseteq S \times \Lambda \times S$. Labelled State
Transition Systems coincide with Indexed Abstract Reduction Systems
(\S\ref{sec:indexed_rewrite}).

\emph{Action Programming Languages} add a set of \emph{Fluents}, $F$, and
\emph{Values}, $V$, and a function mapping $F \times S$ to $V$.

the Automata-theoretic version of a State Transition System is a
\emph{Semiautomaton} (\S\ref{sec:semiautomaton})

Finite Automata (\S\ref{sec:finite_automaton}) may be seen as State
Transition Systems with an initial State and a number of final
\emph{Accept} states indicating \emph{Word} (Expression) membership
for a Language.

Petri Net (\S\ref{sec:petri_net}): State Transition System of
Distributed Processes

Model Checking (\S\ref{sec:model_checking}): State Transition System
including additional Labelling Function for States giving a Kripke
Structure (\S\ref{sec:kripke_structure})


\asterism


\cite{abramsky-gay-nagarajan96}:

Interaction Categories (\S\ref{sec:interaction_category})

The Inclusion Functor of Synchronization Trees
(\S\ref{sec:synchronization_tree}) into State Transition Systems has a
Left-adjoint Unfolding (???, Non-well-founded Set Theory
\S\ref{sec:non_wellfounded}) to Synchronization Trees



\subsubsection{Labelled Transition System}
\label{sec:labelled_transition}

Labelled Transition Systems coincide with Indexed Abstract Reduction
Systems (\S\ref{sec:indexed_rewrite})

Hennessy-Milner Logic (\S\ref{sec:hennessy_milner})

%FIXME some of this may need to be moved

Operational Semantics (\S\ref{sec:operational_semantics}) for Process
Calculi (\S\ref{sec:process_calculus})

Non-well-founded Set Theory (\S\ref{sec:non_wellfounded})

System (\S\ref{sec:system})

Transition Systems Labelled by Elements of Set $Act$ are Coalgebras
relative to the Standard Functor (???) $\pow(Act \times \cdots)$
\cite{aczel88}

Abstract Behavior: Bisimulation (\S\ref{sec:bisimulation}) Relation on
a Labelled Transition System \cite{aczel88}

Atomic Actions: externally observable

successive states are internal to the Process: distinct States of a
Process may have the same external Behavior \cite{aczel88}

$(X,\alpha)$

$\alpha : X \rightarrow \pow(Act \times X)$
\[
  \alpha x = \{(a,y) \in Act \times X | x \xrightarrow{a} y\}
\]

$\alpha : X \rightarrow \Theta X$ where:
\[
  \Theta = \pow \circ (K_{Act} \times Id)
\]
Transition System $TS$ is a Coalgebra (\S\ref{sec:coalgebra}) relative
to the Functor $\Theta$

Final Coalgebra Theorem gives the existence of a Final Coalgebra for
$\Theta$, called a \emph{Complete $TS$}

States of SCCS are States of a Complete $TS$, $\class{P}$, the
Greatest Fixed Point of $\Theta$ (equivalently the largest Class such
that if $P \in \class{P}$ then $P$ is a Subset of $Act \times
\class{P}$

$\class{P}$ has Transition Relations $\xrightarrow{a}$ for $a \in Act$
given by:
\[
  P \xrightarrow{a} Q \Leftrightarrow (a,Q) \in P
\]
for all $P,Q \in \class{P}$

\emph{Behavior Map} -- Unique Map $(X,\alpha) \rightarrow \class{P}$
for each $TS$ $(X,\alpha)$, is $\pi : X \rightarrow \class{P}$ such
that for all $x \in X$:
\[
  \pi x = \{(a,\pi y) | x \xrightarrow{a} y \in (X,\alpha)\}
\]

if a $TS$ is an Operational Semantics for a Programming Language then
the Behavior Map gives a Canonical Representation of the Abstract
Behaviors of the Programs of the Language

$\class{P}$: Domain of Mathematical Objects that can be the
Denotations (\S\ref{sec:denotation}) of Programs for such a
Programming Language

Operations on $\class{P}$: Action, Summation, Restriction, Product



\subsubsection{Simulation} \label{sec:simulation}

\subsubsection{Bisimulation} \label{sec:bisimulation}

Universal Coalgebra (\S\ref{sec:universal_coalgebra})

Bisimilar Object (\S\ref{sec:bisimilar_object})

Non-well-founded Set Theory (\S\ref{sec:non_wellfounded})

System (\S\ref{sec:system})

Abstract Behavior: Bisimulation Relation on a Labelled Transition
System (\S\ref{sec:labelled_transition}) \cite{aczel88}

Maximal

Regular

Weak Bisimulation: Observational (Extensional) Equality
(\S\ref{sec:extensional_equality})



% --------------------------------------------------------------------
\subsection{Semiautomaton}\label{sec:semiautomaton}
% --------------------------------------------------------------------

A State Transition System (\S\ref{sec:state_transition}) may be
formulated as a \emph{Semiautomaton}:
\[
  (Q,\Sigma,T)
\]
where $\Sigma$ is a Non-empty Set of \emph{Input Symbols}, $Q$ is the
Set of States, and $T$ is a \emph{Transition Function}:
\[
  T:Q \times \Sigma \rightarrow Q
\]

When $Q$ is finite, a Semiautomaton may be seen as a Deterministic
Finite Automaton (DFA \S\ref{sec:dfa}) without an Initial State or set
of Accept States

\fist Essentially \emph{Functors} (\S\ref{sec:functor}) from Category
Theory

A Semiautomaton induces a Monoid called the \emph{Transition Monoid}
(\S\ref{sec:transition_monoid}):
\[
  M(Q,\Sigma,T) = \{T_w | w \in \Sigma^*\}
\]

A Semiautomaton induces a Monoid Action (\S\ref{sec:monoid_action})

Transformation Semigroup (\S\ref{sec:transformation_semigroup})

the construction of all possible Transitions driven by Strings in the
Free Group can be depicted graphically as a De Bruijn Graph
(\S\ref{sec:debruijn_graph})

%FIXME clarify which free group above



% --------------------------------------------------------------------
\subsection{Automaton} \label{sec:automaton}
% --------------------------------------------------------------------

An \emph{Automaton} reads \emph{Input Strings} (\emph{Words}) and
either \emph{Accepts} or \emph{Rejects} the Words depending on whether
they are a member of the Language \emph{Recognized} by that Automaton.
By convention the Set of Expressions from Formal Languages will be
re-cast as Alphabets of Words, $\Sigma$. The Set of all Words Accepted
by an Automaton are those belonging to a Formal Language.

Automata may be arranged in a hierarchy according to increasing
power:\\
DFA = NFA $\subset$ DPDA-I $\subset$ NPDA-I $\subset$ LBA
$\subset$ DPDA-II = NPDA-II = DTM = NTM = PTM = MDTM

where
\begin{itemize}
\item DFA = Deterministic Finite Automata (\S\ref{sec:dfa})
\item NFA = Non-deterministic Finite Automata (\S\ref{sec:ndfa})
\item DPDA = Deterministic Pushdown Automata
  (\S\ref{sec:deterministic_pda}) with 1 or 2 push-down stores
\item NPDA = Non-deterministic Pushdown Automata
  (\S\ref{sec:pushdown_automaton}) with 1 or 2 push-down stores
\item LBA = Linear Bounded Automata
  (\S\ref{sec:linear_bounded_automaton})
\item DTM = Deterministic Turing Machine
  (\S\ref{sec:deterministic_turing_machine})
\item NTM = Non-deterministic Turing Machine
  (\S\ref{sec:nondeterministic_turing_machine})
\item PTM = Probabilistic Turing Machine
  (\S\ref{sec:probabilistic_turing_machine})
\item MDTM = Multidimensional Turing Machine
  (\S\ref{sec:multidimensional_turing_machine})
\end{itemize}

The Syntactic Monoid (\S\ref{sec:syntactic_monoid}) of a Formal
Language is Isomorphic to the Transition Monoid
(\S\ref{sec:transition_monoid}) of the minimal Automaton Accepting the
Language.



% --------------------------------------------------------------------
\subsection{Finite Automaton}\label{sec:finite_automaton}
% --------------------------------------------------------------------

\emph{Finite Automata} are \emph{Finite-state Machines} and take a
finite Input String of Symbols and either Accept or Reject the Input
depending on the final State of the computation.

Finite Automata are able to recognize Regular
Languages(\S\ref{sec:regular_language}).

\fist Cf. ``Infinite State Machines'' (State Transition Systems
\S\ref{sec:state_transition}), $\omega$-automata
(\S\ref{sec:omega_automaton}): Finite Automata with Infinite Input
Strings



\subsubsection{Deterministic Finite Automaton}\label{sec:dfa}

\emph{Deterministic Finite Automata} have the restriction that an
Input Symbol has a Transition Function to a single State.
Deterministic Finite Automata recognize Regular Languages
(\S\ref{sec:regular_language}).

a DFA is an Subsequential Finite-state Transducer (SFT
\S\ref{sec:sft}) with an Endmarked Output Alphabet $\hat{\Gamma} =
(\varnothing, \{a,r\})$ so that valid Output Strings are only $a$ or
$r$ which Transduces its Input to the Output String $a$ or $r$ to
indicate Acceptance or Rejection of the Input, respectively

Representation of a Deterministic Finite Automaton as a 5-tuple:
\[
  (Q,\Sigma,\delta,q_0,F)
\]
where:
\begin{itemize}
  \item $Q$ is a Finite Set of States
  \item $\Sigma$ is the Alphabet
  \item $\delta$ is the transition function $\delta: Q \times
    \Sigma \rightarrow Q$
  \item $q_0 \in Q$ is the Initial State
  \item $F \subseteq Q$ is the Set of final Accept States.
\end{itemize}

Running for a given input $w = a_1,a_2, \cdots , a_n \in \Sigma^*$
produces a sequence of States $q_0,q_1,q_2,\cdots , q_n$ where $q_i
\in Q$ such that $q_i = \delta (q_{i-1},a_i)$ and $w$ is accepted if
$q_n \in F$.

A recursive definition using Composition of Transition Functions:
\[
  \widehat{\delta}(q,\varepsilon) = q
\]\[
  \widehat{\delta}(q,wa) = \delta_a(\widehat{\delta}(q,w))
\]
where $w \in \Sigma^*$, $a \in \Sigma$ and $q \in Q$. Repeated
application describes the Transition Monoid
(\S\ref{sec:transition_monoid}).



\paragraph{Minimum DFA}\label{sec:minimum_dfa}\hfill

For each Regular Language accepted by a DFA, there is a \emph{Minimum
  DFA} which is unique up to Isomorphism with a minimum number of
States. \emph{Minimization} from the original DFA is achieved by
removing \emph{Unreachable States} and merging
\emph{Nondistinguishable States}.



\paragraph{Local Automaton}\label{sec:local_automaton}\hfill

A \emph{Local Automaton} is a DFA for which all Edges with the same
Label lead to a single Vertex.

Local Automata Recognize Local Languages (\S\ref{sec:k_testable}).



\subsubsection{Non-deterministic Finite Automaton}\label{sec:ndfa}

\emph{Nondeterministic Finite Automata} are Finite-state Machines that
may transition from one State to a number of different states, given
as an Element of the Powerset of $Q$, $\pow(Q)$.

Representation of a Nondeterministic Finite Automaton as a
5-tuple:
\[
  (Q,\Sigma,\Delta,q_0,F)
\]
where:
\begin{itemize}
  \item $Q$ is a Finite Set of States
  \item $\Sigma$ is the Alphabet
  \item $\Delta$ is a \emph{Transition Relation} $\Delta: Q \times
    \Sigma \rightarrow \pow(Q)$
  \item $q_0 \in Q$ is the Initial State
  \item $F \subseteq Q$ is the Set of final Accept States.
\end{itemize}

A Word, $w=a_1,a_2,\cdots,a_n$, is Accepted when there exists a
Sequence of States, $r_0,r_1,\cdots,r_n$ such that:
\begin{enumerate}
  \item $r_0 = q_0$
  \item $r_{i+1} \in \Delta(r_i, a_{i+1})$, for $i = 0, \cdots, n-1$
  \item $r_n \in F$
\end{enumerate}

A DFA (\S\ref{sec:dfa}) may be seen as a NFA which restricts
transitions to allow only one State, and can be constructed from a NFA
with $n$ States using \emph{Powerset Construction}, requiring up to
$2^n$ States. Both types recognize the same Regular Languages
(\S\ref{sec:regular_language}).



\paragraph{NFA-$\varepsilon$}\label{sec:nfa_e}\hfill

An \emph{NFA-$\varepsilon$} is a NFA that allows transitions without
consuming Input Symbols. A Transition that changes state without
consuming Input is an \emph{$\varepsilon$-move}. Each State $q$
defines an $\varepsilon$-\emph{closure}, $E(q)$, which is the set of
States that are reachable by $\varepsilon$-moves.

The Languages recognized by NFA-$\varepsilon$ are the same as NFA/DFA.



\subsubsection{Aperiodic Finite-state Automaton}
\label{sec:aperiodic_automaton}

\subsubsection{Finite State Transducer}\label{sec:fst}

$(Q,\Sigma,\Gamma,I,F,\delta)$

$Q$ -- Finite Set of States

$\Sigma$ -- Finite Input Alphabet

$\Gamma$ -- Finite Output Alphabet

$I$ -- Initial States: $I \subseteq Q$

$F$ -- Final States: $F \subseteq Q$

$\delta$ -- Transition Relation: $\delta \subseteq Q \times (\Sigma
\cup \{\varepsilon\}) \times (\Gamma \cup \{\varepsilon\}) \times Q$


\asterism


\cite{abramsky-gay-nagarajan96}:

Interaction Categories (\S\ref{sec:interaction_category})

Functions extended in Time: Space of Deterministic Transducers

Relations extended in Time: Space of Non-deterministic Transducers



\paragraph{Subsequential Finite-state Transducer}\label{sec:sft}\hfill

Sch\"utzenberger - \emph{Sur Une Variante des Fonctions Sequentielles}

(SFT)

DeYoung-Pfenning16 -- Curry-Howard Isomorphism between \emph{Fixed-cut
  Proofs} in $\oplus,\mathbf{1}$-$\mu$-subsingleton Logic
(\S\ref{sec:modal_mu_logic}) and a generalization of Deterministic
Finite-state Transducers (Subsequential Finite-state Transducers) that
also ``captures'' Deterministic Automata; relates Proofs to Automata
and Proof Reduction to State Transitions

Linear Communicating Automata (\S\ref{sec:lca}) -- Subsequential
Finite-state String Transducer Chains

Curry-Howard Isomorphism between SFTs and a Class of Cut-free
Subsingleton Proofs:
\begin{itemize}
  \item Propositions as Languages
  \item Proofs as SFTs
  \item Cut Reductions are SFT Computation Steps
\end{itemize}

SFT $T$ as a 6-tuple:
\[
  T = (Q, \hat{\Sigma}, \hat{\Gamma}, \delta, \sigma, q_0)
\]
with:
\begin{itemize}
  \item $Q$ -- Set of \emph{States}, Partitioned into Sets of
    \emph{Read States}, $Q^r$, \emph{Write States}, $Q^w$, and
    \emph{Halt States} $Q^h$
  \item $\hat{\Sigma} = (\Sigma_i,\Sigma_e)$ -- \emph{Endmarked Input
    Alphabet} of Disjoint Finite Alphabets of \emph{Internal Symbols}
    (possibly Empty) $\Sigma_i$ and \emph{Endmarkers} (Nonempty)
    $\Sigma_e$
  \item $\hat{\Gamma} = (\Gamma_i,\Gamma_e)$ -- \emph{Endmarked Output
    Alphabet} of Disjoint Finite Alphabets of \emph{Internal Symbols}
    (possibly Empty) $\Gamma_i$ and \emph{Endmarkers} (Nonempty)
    $\Gamma_e$
  \item $\delta : \Sigma \times Q^r \rightarrow Q$ -- Total Transition
    Function on Read States
  \item $\sigma : Q^w \rightarrow Q \times \Gamma$ -- Total Output
    Function on Write States
  \item $q_0 \in Q$ -- Initial State
\end{itemize}

\emph{Configurations}, $\longrightarrow$ TODO

\emph{Normal Form SFTs} avoid getting ``stuck'' -- having a Read State
reachable after an Endmarker signalling end of Input has been Read,
having a Write State reachable after having written an Endmarker, or
having Halt State reachable before an Endmarker signalling end of
Input has been Read

three Properties must be met for an SFT to be Normal Form:
\begin{enumerate}
  \item $\forall e \in \Sigma_e, q \in Q^r$, no Read State is
    Reachable from $\delta(e,q)$
  \item $\forall e \in \Gamma_e, q_e \in Q^w$, no Write State is
    Reachable from $q_e$ if $\sigma(q) = (q_e,e)$
  \item $\forall q \in Q^h$, all paths from the Initial State $q_0$ to
    $q$ pass through $(e_q')$ for \emph{some} Endmarker $e \in
    \Sigma_e$ and Read State $q' \in Q^r$
\end{enumerate}

when allowing Alphabets with more than one Endmarker, SFTs subsume
Deterministic Finite Automata (DFAs \S\ref{sec:dfa}), where a DFA is
an SFT with an Endmarked Output Alphabet $\hat{\Gamma} = (\varnothing,
\{a,r\})$ so that valid Output Strings are only $a$ or $r$ which
Transduces its Input to the Output String $a$ or $r$ to indicate
Acceptance or Rejection of the Input, respectively


\textbf{SFT Chains}

Linear Communicating Automata (\S\ref{sec:lca})

an \emph{SFT Chain} $(T_i)^n_{i=1}$ is a Finite Family of $n$ SFTs:
\[
  T_i = (Q_i,\hat{\Sigma}_i,\hat{\Gamma}_i,\delta_i,\sigma_i,q_i)
\]
such that $\hat{\Gamma} = \hat{\Sigma}_{i+1}$ for each $i < n$

SFT Chain Configurations

Composition accomplished by Concatenating the States of individual SFTs



\subsubsection{Extended Finite State Machine}\label{sec:extended_state}

(or \emph{EFSM})

$7$-tuple:
\[
  M = (I,O,S,D,F,U,T)
\]
where:
\begin{itemize}
  \item $S$ -- Set of State Symbols
  \item $I$ -- Set of Input Symbols
  \item $O$ -- Set of Output Symbols
  \item $D$ -- $n$-dimensional Vector Space $D_1 \times \ldots \times
    D_n$
  \item $F$ -- Set of \emph{Enabling Functions} $f_i : D \rightarrow
    {0,1}$
  \item $U$ -- Set of \emph{Update Functions} $u_i : D \rightarrow D$
  \item $T$ -- Transition Relation $T : S \times F \times I
    \rightarrow S \times U \times O$
\end{itemize}



\subsubsection{Moore Machine}\label{sec:moore_machine}

%FIXME

a Kripke Structure (\S\ref{sec:kripke_structure}) having only one
Initial State may be identified with a Moore Machine with a singleton
Input Alphabet and Output Function given by the structure's Labelling
Function

Entry, Exit Actions associated with States (not Transitions) -- (UML
State Machines)



\subsubsection{Mealy Machine}\label{sec:mealy_machine}

%FIXME

Actions depending on the State of the System as well as the Triggering
Event -- (UML State Machines)



\subsubsection{Timed Finite Automaton}\label{sec:timed_fsm}

Alur-Dill94

\fist Timed Multiparty Session Types (\S\ref{sec:timed_multiparty})



\subsubsection{Communicating Finite Automaton}
\label{sec:communicating_fsm}

Gouda, Manning, Yu - 1984 -
\emph{On the progress of communication between two finite state machines}

Muscholl10 - \emph{Analysis of Communicating Automata}

DeYoung-Pfenning16 - \emph{Substructural Proofs as Automata} -- Linear
Communicating Automata (\S\ref{sec:lca}): weak Linear Logic augmented
with Least Fixed-points (\S\ref{sec:fixedpoint_logic})

correspondence between Communicating Automata and Session Types
(\S\ref{sec:session_type}):

Danielou-Yoshida13 - \emph{Multiparty Compatibility in
  Communicating Automata: Synthesis and Characterisation of Multiparty
  Session Types}

Multiparty Session Types (\S\ref{sec:multiparty_session})

Equivalence of Local Types (\S\ref{sec:local_type}) with Communicating
Automata

Automata/Model Checking approach to Protocol
Specification/Verification/Implementation -- Specification and
Description Language (SDL)

Choreography Specifications

WS-CDL (Web Service Choreography Description Language)

Choreography BPMN (Business Process Model and Notation)

\emph{Communicating Finite State Machine} $M$ -- 5-tuple:
\[
  M = (Q,C,q_0,\Lambda,\delta)
\]

$Q$ -- Finite Set of States

$C = \{pq \in P^2 \ |\ p \neq q\}$ -- Set of Channels constructed from
Set of Participants $P = \{ p, q, \ldots \}$

$q_0 \in Q$ -- Initial State

$\Lambda$ -- Finite Alphabet of Messages (Action Labels)

$\delta \subseteq Q \times (C \times \{!,?\} \times \Lambda) \times Q$
-- Finite Set of Transitions

$pq!a$ -- Send an Action of $a$ from Process $p$ to Process $q$

$pq?a$ -- Receive an Action of $a$ from Process $p$ by Process $q$

$\ell, \ell', \ldots$ -- Actions (Transition Labels)

Subject of an Action $\ell$:
\[
  subj(pq!a) = subj(qp?a) = p
\]

a Sending State has only Labelled Send Actions as Outgoing Transitions

a Receiving State has only Labelled Receive Actions as Outgoing
Transitions

a Final State does not have any Outgoing Transitions

a Mixed State has both Receiving and Sending Outgoing Transitions

a Directed State has only Sending or only Receiving Actions to a
single other Participant

\emph{Path} -- a Finite Sequence of States $q_0, \ldots q_n$ such that
$(q_i, \ell, q_{i+1}) \in \delta$ and $q \xrightarrow{\ell} q'$ if
$(q,\ell,q') \in \delta$

a Path is \emph{Connected} if for every State $q \neq q_0$ there is a
Path from $q_0$ to $q$

\emph{Deterministic} if for all States $q \in Q$ and all Actions
$\ell$, $(q,\ell,q'),(q,\ell,q'') \in \delta$ Implies $q' = q''$

a \emph{Communicating System} $\class{S}$ is a Tuple:
\[
  \class{S} = (M_p)_{p \in P}
\]
such that:
\[
  M_p = (Q_p, C, q_{0p}, \Lambda, \delta_p)
\]

\fist cf. Communicating System (Systems Theory
\S\ref{sec:communicating_system}) %FIXME

Danielou-Yoshida13 define a \emph{Basic} Communicating FSM to be one
which is Deterministic, Directed, and has no Mixed States; any Basic
CFSM can be translated into a Local Type (\S\ref{sec:local_type});
Basic CFSMs provide a genralization of Half-duplex Communication
Systems; \emph{note} in general the notion of Safety defined for
Multiparty Session Types is Undecidable for Basic CFSMs: the
\emph{Compatibility} Property is defined to enforce Behavior of CFSMs
to be as if they were Projections of Global Types
(\S\ref{sec:global_type})


Lange-Tuosto-Yoshida15:

Global Graph (Graphical Choreography \S\ref{sec:global_graph})

CFSM as a $4$-tuple:
\[
  M = (Q, q_0, A, \delta)
\]

$Q$ -- Finite Set of \emph{States}

$q_0 \in Q$ -- \emph{Initial State}

$\delta \subseteq Q \times Act \times Q$ -- Set of \emph{Transitions}



\paragraph{Linear Communicating Automaton (LCA)}\label{sec:lca} \hfill

DeYoung-Pfenning16 - \emph{Substructural Proofs as Automata}

``Linear'': Communicating Automata arranged in a row such that each
Automaton only Communicates with its Left and Right Neighbors

Well-behaved LCAs corresponding to Circular Proofs in
$\oplus$,$\mathbf{1}$-$\mu$-subsingleton Logic
(\S\ref{sec:modal_mu_logic}) are free from Deadlock and Race
Conditions

Subsequential Finite-state String Transducer (SFT \S\ref{sec:sft})
Chains


\textbf{Model of Linear Communicating Automata}

if conditions on Circular Proofs are relaxed so that $\mu$ is a
\emph{general}--not Least--Fixed Point, then Proofs are as powerful as
Turing Machines

generalization of Turing Machines:
\begin{itemize}
  \item ability to insert and delete Cells from the Tape
  \item ability to ``spawn'' multiple machine Heads that operate
    Concurrently
\end{itemize}

\emph{Linear Communicating Automata (LCA)}, $M$, as an $8$-tuple:
\[
  M = (Q,\Sigma,\delta^{rL},\delta^{rR},\sigma^{wL},\sigma^{wR},\rho,q_0)
\]
with:
\begin{itemize}
  \item $Q$ -- Finite Set of States, Partitioned into (possibly
    Empty) Sets of:
    \begin{itemize}
      \item $Q^{rL}$ and $Q^{rR}$ -- Left- and Right-reading States
      \item $Q^{wL}$ and $Q^{wR}$ -- Left- and Right-writing States
      \item $Q^s$ -- Spawn States
      \item $Q^h$ -- Halt States
    \end{itemize}
  \item $\Sigma$ -- Finite Alphabet
  \item $\delta^{rL} : \Sigma \times Q^{rL} \rightarrow Q$ -- Total
    Function on Left-reading States
  \item $\delta^{rR} : Q^{rR} \times \Sigma \rightarrow Q$ -- Total
    Function on Right-reading States
  \item $\sigma^{wL} : \Sigma \times Q^{wL} \rightarrow Q$ -- Total
    Function on Left-writing States
  \item $\sigma^{wR} : Q^{wR} \times \Sigma \rightarrow Q$ -- Total
    Function on Right-writing States
  \item $\rho : Q^s \rightarrow Q \times Q$ -- Total Function on Spawn
    States
  \item $q_0 \in Q$ -- Initial State
\end{itemize}

\emph{Configurations} of an LCA $M$ are Strings $w$ and $v$ from the Set
$(\Sigma^*Q)*\Sigma^*$

each State in the Configuration represents a Read/Write Head

$\longrightarrow$ -- Binary Relation on Configurations:
(Read-L, Read-R, Write-L, Write-R, Spawn, Halt) %TODO

\emph{Turing Completeness of LCAs}

$\&$,$\bot$,$\oplus$,$\One$-$\mu$-subsingleton Logic --
\emph{Well-behaved LCAs}

$\mathsf{readL}$ and $\mathsf{readR}$ become Left- and Right-reading
States

$\mathsf{writeL}$ and $\mathsf{writeR}$ become Left- and Right-writing
States

Cuts, represented by the $\rhd$ Operation which creates a new
Read/Write Head, become Spawning States

the $Id$ Rule, represented by the $\leftrightarrowtriangle$ Operation,
becomes a Halting State

LCA Transitions are mached by Proof Reductions

corresponding LCAs cannot Deadlock becaues Cut Elimination can always
make Progress (Fortier-Santocanale13) and do not have Races because
$\mathsf{readR}$ and $\mathsf{readL}$ have different Types and as a
result cannot be neighbors


\emph{Turing Completeness}

allowing general occurrences of $Cut$ in order to simulate Turing
Machines; to allow for Non-halting Turing Machines, $\mu$ is relaxed
to be a \emph{general} Recursive Type (instead of an Inductive Type)
by dropping the requirement that every Cycle in a Circular Proof is a
Left $\mu$-trace

Turing Machines can also be simulated without $\&$ by replacing
occurrences of $\&$, $\mathsf{readR}$, and $\mathsf{writeL}$ with
$\oplus$ (and its constructs) in a Continuation-passing Style-- this
indicates that Turing Completeness depends on interaction of general
Cuts and general Recursion, not on interaction between $\oplus$ and
$\&$



\subsubsection{Communicating Timed Automaton}\label{sec:communicating_timed_fsm}

Krcal-Yi06

Bocchi-Yang-Yoshida14 - \emph{Timed Multiparty Session Types}

Timed Multiparty Session Types (\S\ref{sec:timed_multiparty})



% --------------------------------------------------------------------
\subsection{Pushdown Automaton}\label{sec:pushdown_automaton}
% --------------------------------------------------------------------

\emph{Pushdown Automata} add to Finite Automata a \emph{Stack} as a
\emph{Parameter for Choice} of States and can recognize Context-free
Languages(\S\ref{sec:context_free}).

Adding a second Stack makes a Pushdown Automaton equal in power to a
Turing Machine.

Unlike Finite Automata, Deterministic PDA are not equivalent to
Nondeterministic PDA. The general representation for a PDA is:
\[
  M = (Q, \Sigma, \Gamma, q_0, Z_0, F, \delta)
\]
where:
\begin{itemize}
  \item $Q$ is a Finite Set of States
  \item $\Sigma$ is a Finite Set of input Symbols
  \item $\Gamma$ is a Finite Set of Stack Symbols
  \item $q_0 \in Q$ is the Initial State
  \item $Z_0 \in \Gamma$ is the Initial Stack Symbol
  \item $F \subseteq Q$ is the Set of final Accept States
  \item $\delta$ is the Transition Function $\delta: (Q \times (\Sigma
    \cup \{\varepsilon\}) \times \Gamma) \rightarrow \pow(Q \times
    \Gamma^*)$
\end{itemize}

An Element $(p,a,Z,q,\alpha)\in\delta$, with $M$ in State $p \in Q$,
Input $a \in \Sigma \cup \{\varepsilon\}$, and top Stack Symbol $Z \in
\Gamma$ results in the following:
\begin{enumerate}
  \item read $a$
  \item change state to $q$
  \item pop $Z$
  \item push $\alpha \in \Gamma^*$
\end{enumerate}



\subsubsection{Deterministic Pushdown Automaton}
\label{sec:deterministic_pda}

\emph{Deterministic Pushdown Automata} have the restriction of only
one Derivation per Accepted input Word. This allows recognition of a
Subset of Context-free Languages termed Deterministic
(\S\ref{sec:deterministic_cfg}). Such Languages can be parsed in
Linear Time and Parsers for such Languages can be generated
automatically (\S\ref{sec:parser_generator}).

A Pushdown Automata is Deterministic iff both:
\begin{enumerate}
\item $\forall q \in Q, a \in \Sigma \cup {\varepsilon}, x \in
  \Gamma \vdash |\delta(q,a,x)| \leq 1$
\item $\forall q \in Q, x \in \Gamma \vdash |\delta(q,\varepsilon,x)|
  \neq 0 \Rightarrow \forall a \in \Sigma \vdash |\delta(q,a,x)|=0$
\end{enumerate}



\subsubsection{Embedded Pushdown Automaton}\label{sec:embedded_pushdown}

\subsubsection{Nested Stack Automaton}\label{sec:nested_stack_automaton}

\subsubsection{Thread Automaton}\label{sec:thread_automaton}



% --------------------------------------------------------------------
\subsection{Linear Bounded Automaton} \label{sec:linear_bounded_automaton}
% --------------------------------------------------------------------

\emph{Linear Bounded Automata} are Turing Machines (i.e. Pushdown
Automata \S\ref{sec:pushdown_atomaton} with two Stacks), restricted to
an Input of finite length and are Acceptors for Context-sensitive
Languages (\S\ref{sec:context_sensitive}) which require that
Production Rules do not increase the size of the Expression as a
result; therefore the size of the Input is sufficient for calculation.



% --------------------------------------------------------------------
\subsection{Turing Machine}\label{sec:turing_machine}
% --------------------------------------------------------------------

A \emph{Turing Machine} operates on an infinite \emph{Storage Tape},
which acts as the Read Input as well as Write Storage. Pushdown
Automata (\S\ref{sec:pushdown_automaton}) with 2 Stacks are equivalent
to Turing Machines.

A Turing Machine that halts for every Input is a \emph{Total Turing
  Machine} or \emph{Decider}.



\subsubsection{Non-deterministic Turing Machine}
\label{sec:nondeterministic_turing_machine}

\fist Bounded Non-determinsm (\S\ref{sec:bounded_nondeterminism})

\emph{Non-deterministic Turing Machines} (\emph{NTM}s) can be defined
as:
\[
  M = (Q, \Sigma, q_0, \sqcup, A, \delta)
\]
where:
\begin{itemize}
  \item $Q$ is a finite set of States
  \item $\Sigma$ is the finite Alphabet
  \item $q_0 \in Q$ is the initial State
  \item $\sqcup \in \Sigma$ is the blank Symbol
  \item $F \subseteq Q$ is the set of final Accept States
  \item $\delta \subseteq (Q \setminus F \times \Sigma) \times (Q
    \times \Sigma \times \{L,R\})$ and $L$ and $R$ are left and right
    shift.
\end{itemize}

The operation of $M$ in State $q_i$ and current Read Input $a_j$ is a
Transition Function, $q_i a_j \rightarrow q_{i1} a_{j1} d_k$. Note
that for an NTM, $\delta$ is a Relation and more than one Function can
exist for each possible Input/State combination. The result is to
Write the new Symbol $a_{j1}$ in the current position and shift the
Storage left or right as specified by $d_k$, afterwards assuming State
$q_{i1}$.



\subsubsection{Deterministic Turing Machine}
\label{sec:deterministic_turing_machine}

\emph{Deterministic Turing Machines} (\emph{DTM}s) have one possible
Output Transition per unique Input/State combination, thus $\delta$ is
a Partial Function rather than a Relation:
\[
  \delta : Q \setminus F \times \Sigma \rightarrow Q \times
  \Sigma \times {L,R}
\]
The computational power of DTMs and NTMs is equivalent (they can solve
the same problems) as NTMs include DTMs as a special case. An
equivalent Accepting Computation in a DTM is generally Exponential to
the length of the shortest Accepting Computation of an NTM.



\subsubsection{Probabilistic Turing Machine}
\label{sec:probabilistic_turing_machine}

A \emph{Probabilistic Turing Machine} adds to Transitions a
Probability Distribution (or a Tape with random Symbols). It is an
open question whether this is more powerful than a DTM
($\mathsf{BPP}=\mathsf{P}$ ?)  but it is useful in the definition of
Interactive Proof Systems. %FIXME refs



\subsubsection{Multidimensional Turing Machine}
\label{sec:multidimensional_turing_machine}

\emph{Multidimensional Turing Machines} allow for Tapes of varying
Topologies (Part \ref{sec:topology}). This requires additional Shift
directions (i.e. $\{L, R, U, D\}$ for a 2-dimensional tape) but does
not increase the computing power; even an $\infty$-dimensional Turing
Machine can be simulated by a DTM.



% --------------------------------------------------------------------
\subsection{Zeno Machine}\label{sec:zeno_machine}
% --------------------------------------------------------------------

Countably Infinite Operations in a Finite Time

\fist Physically impossible according to Law of Discreteness (Actor
Model Theory \S\ref{sec:discreteness_law})



% ====================================================================
\section{Nominal Automaton}\label{sec:nominal_automaton}
% ====================================================================

\url{https://www.youtube.com/watch?v=b38uoZccGuU} Silva 16

Automata Theory over Nominal Sets (\S\ref{sec:nominal_set}): Infinite
Sets with Finite Representations

Closed-ness and Consistency are dual concepts

Epi-Mono Factorization (\S\ref{sec:epimono_factorization})



% ====================================================================
\section{$\omega$-language}\label{sec:omega_language}
% ====================================================================

An $\omega$-language (\S\ref{sec:omega_language}) is a Subset of
$\Sigma^\omega$

a Word on a Path in a Kripke Structure (\S\ref{sec:kripke_structure})
is an $\omega$-word over the Alphabet $2^{AP}$ where $AP$ are the
Atomic Propositions of the Kripke Structure

Operations:

Intersection, Union, Left Catenation, Omega (Infinite Iteration),
Finite Prefixes, Limits

Distance, Metric Space %FIXME



% --------------------------------------------------------------------
\subsection{$\omega$-regular language}\label{sec:omega_regular}
% --------------------------------------------------------------------

Recognized by B\"uchi Automata (\S\ref{sec:buchi_automaton})

precisely the $\omega$-languages definable a particular Monadic
Second-order Logic called $\mathsf{S2S}$



% ====================================================================
\section{$\omega$-automaton}\label{sec:omega_automaton}
% ====================================================================

variant of Finite Automaton (\S\ref{sec:finite_automaton}) that runs
on Infinite (rather than Finite) Input Strings (Words)

not expected to Terminate: different ``Acceptance Conditions''
required %FIXME

may be Deterministic or Non-deterministic

all except B\"uchi Automata recognize precisely the Regular
$\omega$-languages and differ only in Acceptance Conditions; some may
be more succinct in their representations of a given $\omega$-language



% --------------------------------------------------------------------
\subsection{B\"uchi Automaton}\label{sec:buchi_automaton}
% --------------------------------------------------------------------

Recognizer for Subset of $\omega$-regular Languages
(\S\ref{sec:omega_regular}); strictly weaker than other
$\omega$-automata

used in Model Checking (\S\ref{sec:model_checking}, Kripke Structures
\S\ref{sec:kripke_structure}) as an Automata-theoretic version of a
Formula in Linear Temporal Logic (\S\ref{sec:linear_temporal})



% --------------------------------------------------------------------
\subsection{Rabin Automaton}\label{sec:rabin_automaton}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Streett Automaton}\label{sec:streett_automaton}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Parity Automaton}\label{sec:parity_automaton}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Muller Automaton}\label{sec:muller_automaton}
% --------------------------------------------------------------------



% ====================================================================
\section{Metalanguage}\label{sec:metalanguage}
% ====================================================================

A \emph{Metalanguage} is a Language used to describe another Language,
the \emph{Object Language}.

Metalanguage Expressions find use in:
\begin{itemize}
  \item Deductive Systems (\S\ref{sec:deductive_apparatus})
  \item Metavariables (\S\ref{sec:metavariable})
  \item Metatheory (\S\ref{sec:metatheory})
  \item Interpretations (\S\ref{sec:interpretation})
\end{itemize}

Note it is common to use Greek letters ($\Phi$, $\Psi$) for
Metavariables and Roman characters otherwise for Object Variables.



% --------------------------------------------------------------------
\subsection{Metasyntax}\label{sec:metasyntax}
% --------------------------------------------------------------------

The \emph{Metasyntax} describes the structure of valid Expressions in
a Metalanguage. There are three types of Symbols distinguished in
Metasyntax:
\begin{description}
  \item [Terminal Symbol] stand-alone Syntactic structure in the
    Object Language
  \item [Non-terminal Symbol] Syntactic category defining a Set of
    valid Expression structures in an $n$-element Subset
  \item [Metasymbol] Metalinguistic denotations
\end{description}
Depending on the Object Language, not all of these Categories may be
used. For example, Metasyntax for Regular Languages
(\S\ref{sec:regular_language}) does not have Non-terminals.



% --------------------------------------------------------------------
\subsection{Embedded Metalanguage}\label{sec:embedded_metalanguage}
% --------------------------------------------------------------------

The use of an Object Language to describe itself is an \emph{Embedded
  Metalanguage} (e.g. the English words \emph{noun} and \emph{verb}
are used to describe English itself).



% --------------------------------------------------------------------
\subsection{Ordered Metalanguage}\label{sec:ordered_metalanguage}
% --------------------------------------------------------------------

An \emph{Ordered Metalanguage} is constructed to discuss an
\emph{Object Language}, and a further Ordered Metalanguage may be
constructed to discuss the first.

analogous to \emph{Ordered Logic}
(\S\ref{sec:noncommutative_logic})



% --------------------------------------------------------------------
\subsection{Hierarchical Metalanguage}\label{sec:hierarchical_metalanguage}
% --------------------------------------------------------------------

an Ordered Hierarchy of Embedded Metalanguages
