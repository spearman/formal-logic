%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Formal Language Theory}\label{part:formal_language}
\cite{hammel03}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Formal Language Theory} is the study of Formal Languages
(\S\ref{sec:formal_language}) and their finite representations,
Automata (\S\ref{sec:automata_theory}). These topics are closely
related to the question of Decidability
(\S\ref{sec:computable_function}), an important concept in Recursion
Theory (Part \ref{part:recursion_theory}).



% ====================================================================
\section{Alphabet}\label{sec:alphabet}
% ====================================================================

An \emph{Alphabet}:
\[
  \Sigma = \{ \sigma_1, \ldots, \sigma_n \}
\]
is a Non-empty Finite Set (\S\ref{sec:set}) of \emph{Symbols}
(Signifiers \S\ref{sec:signifier}), usually \emph{Characters} or
\emph{Letters}.

The Alphabet of a String (\S\ref{sec:string}), $s$, is the Set of
Symbols occuring in that String:
\[
  \mathrm{Alph}(s)
\]

The Alphabet of a Language (\S\ref{sec:formal_language}), $L$, is the
Set of all Symbols occuring in any String of that Language:
\[
  \mathrm{Alph}(L) = \bigcup_{s \in L} \mathrm{Alph}(s)
\]



% ====================================================================
\section{String}\label{sec:string}
% ====================================================================

A \emph{String} (or \emph{Word}), $s$, is a Finite Sequence
(\S\ref{sec:sequence}) of Symbols over an Alphabet $\Sigma$:
\[
  s = \langle c_1 \cdots c_k \rangle
\]
where $c_1, \ldots, c_k \in \Sigma$.

The \emph{Length} of a String, $|s| = k$, is equal to the number of
Symbols in $s$.

The \emph{Empty String} $\varepsilon$ is the unique String over
$\Sigma$ of Length 0, and acts as an Identity Element for String
Concatenation (\S\ref{sec:string_concatenation}).

The Set of all Strings of Length $n$ over $\Sigma$ is denoted
$\Sigma^n$.

The Infinite Set of all possible Strings of all Finite Lengths,
$\Sigma^*$, is the Free Monoid (\S\ref{sec:free_monoid}) or
\emph{Kleene Closure} of $\Sigma$, i.e. the smallest Superset
(\S\ref{sec:subset}) of $\Sigma$ that is closed under String
Concatenation (\S\ref{sec:string_concatenation}):
\[
  \Sigma^* = \bigcup_{n\in\mathbb{N}_0} \Sigma^n
\]



% --------------------------------------------------------------------
\subsection{String Concatenation}\label{sec:string_concatenation}
% --------------------------------------------------------------------

The \emph{Concatenation} of two Strings, $s$ and $t$, is an
Associative Operation denoted $s \cdot t$ or $st$, giving the String
resulting from appending the right-hand String to the end of the
left-hand String.

The Empty String $\varepsilon$ acts as an Identity Element:
\[
  s \cdot \varepsilon = s = \varepsilon \cdot s
\]

With a third String, $u$, the Associative Property gives:
\[
  s \cdot (t \cdot u) = (s \cdot t) \cdot u
\]

Free Monoid (\S\ref{sec:free_monoid})



% --------------------------------------------------------------------
\subsection{Substring}\label{sec:substring}
% --------------------------------------------------------------------

A \emph{Substring} (or \emph{Subword} or \emph{Factor}) is a
Subsequence (\S\ref{sec:subsequence}) of consecutive Symbols that
occur within a given String. For an $n$-length String, there are
$n(n+1) \over 2$ Non-empty Substrings.



\subsubsection{Prefix \& Suffix}\label{sec:prefix_suffix}

A \emph{Prefix} is a Substring that occurs at the beginning of a
String, and a \emph{Suffix} is a Substring that occurs at the end of a
String. A Prefix or Suffix that is not equal to the entire String
itself is called a \emph{Proper Prefix} or \emph{Proper Suffix}
respectively.

The Prefix Relation is sometimes denoted $t \sqsubseteq s$ for Prefix
$t$ of $s$, and defines a kind of Prefix Order
(\S\ref{sec:prefix_order}). For related Data Structures, see
\emph{Prefix Tree}\S\ref{sec:prefix_tree} and \emph{Suffix
  Tree}\S\ref{sec:suffix_tree}.

The Function $\mathrm{Pref}_L(s)$ is defined as the Set of all
Prefixes of a String $s$, with respect to a Language $L$:
\[
  \mathrm{Pref}_L(s) =
    \{ t\;|\;s = tu; t,u \in \mathrm{Alph}(L)^* \}
\]
For two Strings $s$ and $t$, the Prefix Relation
(\S\ref{sec:prefix_suffix}) can be defined in terms of the
$\mathrm{Pref}$ Function as:
\[
  s \sqsubseteq t \Leftrightarrow s \in \mathrm{Pref}_L(t)
\]
This Function is also used in the definition of the \emph{Prefix
  Closure} (\S\ref{sec:prefix_closure}) of a Language.



\subsubsection{Border}\label{sec:string_border}

A \emph{Border} is a Substring that is both a Prefix and a Suffix of
the same String.



\subsubsection{Superstring}\label{sec:superstring}

A \emph{Superstring}, $s$, for a Set of Strings, $P$, is a single
String that contains every String in $P$ as a Substring. For example,
the trivial Superstring of $P$ would be given by the concatenation of
all the Strings in $P$ in any order.



% --------------------------------------------------------------------
\subsection{String Substitution}\label{sec:string_substitution}
% --------------------------------------------------------------------

A \emph{String Substitution} is a Function, $f$, from Symbols in the
Alphabet, $s \in \Sigma$, of a Language, $L$, to another Language,
$L_s$, possibly with a different Alphabet, $\Delta$:
\[
  f(s) = L_s
\]
where $L_s \subseteq \Delta^*$. It is extended to Strings by:
\[
  f(\varepsilon) = \varepsilon
\]
for Empty String $\varepsilon$ and:
\[
  f(st) = f(s)f(t)
\]
for Strings $s,t \in L$. This can be extended to give the Substitution
Closure of a Language (\S\ref{sec:substitution_closure}). Regular
(\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are closed under String
Substitution.



\subsubsection{String Homomorphism}\label{sec:string_homomorphism}

A \emph{String Homomorphism} is a String Substitution such that each
Symbol in an Alphabet $\Sigma$ is replaced by a Single String. A
String Homomorphism is a Monoid Morphism on the Free Monoid of
$\Sigma$ (preserving String Concatenation). For a String Homomorphism
$f$ on Alphabet $\Sigma_1$:
\[
  f : \Sigma_1^* \rightarrow \Sigma_2^*
\]
where $\forall u,v \in \Sigma_1, f(uv) = f(u)f(v)$.

A String Homomorphism $f$ is \emph{$\varepsilon$-free} if $\forall a
\in \Sigma, f(a) \neq \varepsilon$.

A String Homomorphism can be used to define a Homomorphism Closure for
Languages (\S\ref{sec:language_homomorphism}). Regular
(\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are Closed under String
Homomorphisms.



% --------------------------------------------------------------------
\subsection{String Projection}\label{sec:string_projection}
% --------------------------------------------------------------------

For a String $s$ and Alphabet $\Sigma$ (not necessarily
$\text{Alph}(s)$), a \emph{String Projection}, $\pi_{\Sigma}(s)$, is
the String resulting from removal of all Symbols of $s$ that are not
in $\Sigma$:
\[
  \pi_{\Sigma}(s) =
  \begin{cases}
    \varepsilon       & \quad \text{if $s = \varepsilon$}\\
    \pi_{\Sigma}(t)   & \quad \text{if $s = ta$ and $a \notin \Sigma$}\\
    \pi_{\Sigma}(t)a  & \quad \text{if $s = ta$ and $a \in \Sigma$}\\
  \end{cases}
\]
\HandRight\; Cf. Projection in Relational Algebra
\S\ref{sec:relational_projection}. String Projection may be used to
define a Projection over a Language (\S\ref{sec:language_projection}).



% --------------------------------------------------------------------
\subsection{Quotient}\label{sec:string_quotient}
% --------------------------------------------------------------------

The \emph{Right Quotient} $S / b$ of a String $S$ and Symbol $b$
defined as:
\[
  (Sa)/b =
  \begin{cases}
    S           & \quad \text{if $a = b$}\\
    \varepsilon & \quad \text{if $a \neq b$}\\
  \end{cases}
\]
Likewise the \emph{Left Quotient} $b \backslash S$ is defined as:
\[
  b\backslash(aS) =
  \begin{cases}
    S           & \quad \text{if $a = b$}\\
    \varepsilon & \quad \text{if $a \neq b$}\\
  \end{cases}
\]
\HandRight\; Cf. Syntactic Quotient \S\ref{sec:syntactic_quotient}
(Syntactic Monoid)



% --------------------------------------------------------------------
\subsection{Right Cancellation}\label{sec:right_cancellation}
% --------------------------------------------------------------------

\emph{Right Cancellation}, $s \div a$, of a Symbol $a$ from a String
$S$ is the removal of the first occurance of $a$ from $S$, starting
from the right-hand side:
\[
  (sb) \div a =
  \begin{cases}
    s           & \quad \text{if $a = b$}\\
    (s \div a)b & \quad \text{if $a \neq b$}\\
  \end{cases}
\]



% ====================================================================
\section{Formal Language}\label{sec:formal_language}
% ====================================================================

A \emph{Formal Language}, $L$, is a (possibly Infinite) Subset of
$\Sigma^*$, the Set of all possible Strings (\S\ref{sec:string}) over
an Alphabet (\S\ref{sec:alphabet}) $\Sigma$. The Strings belonging to
$L$ are called \emph{Expressions} (\S\ref{sec:expression}), or
\emph{Phrases} (\S\ref{sec:lexical_analysis}) or \emph{Well-formed
  Formulas} (\S\ref{sec:formula}) depending on the context.

The entire content of a Formal Language is uniquely determined by
either:
\begin{itemize}
  \item the Set of all Terminal Expressions that are Generated by the
    Production Rules of a \emph{Generative Grammar}
    (\S\ref{sec:generative_grammar})
\end{itemize}
or
\begin{itemize}
  \item the Set of Expressions Recognized by a \emph{Determinative
    Grammar} (\S\ref{sec:determinative_grammar})
\end{itemize}

For two Languages $L_1$ and $L_2$ over a common Alphabet $\Sigma$:
\begin{itemize}
  \item $L_1 \cup L_2$ is a Language (Set Union)
  \item $L_1 \cap L_2$ is a Language (Set Intersection)
  \item $L_1 - L_2$ is a Language (Set Difference)
  \item $(L_1 - L_2 \cup L_2 - L_1)$ is a Language (Symmetric Difference)
  \item $L_1 \times L_2$ is a Language (Cartesian Product)
\end{itemize}
If $L_2 \subset L_1$ then $L_2$ is a \emph{Sublanguage} of $L_1$.

For a given Language $L \subseteq \Sigma^*$ there exists a
\emph{Complement Language} $L^C = \Sigma^* - L$.

The Language consisting of just the Empty String $\{\varepsilon\}$ is
distinguished from the Empty Language $\{\}$.



% --------------------------------------------------------------------
\subsection{Expression}\label{sec:expression}
% --------------------------------------------------------------------

An \emph{Expression} is a String (\S\ref{sec:string}) belonging to a
Formal Language.



% --------------------------------------------------------------------
\subsection{Language Concatenation}\label{sec:language_concatenation}
% --------------------------------------------------------------------

The \emph{Concatenation} of two Languages, $S$ and $T$, is an
Associative Operation that gives a third Language, $S \cdot T$,
defined as:
\[
  S \cdot T = \{s \cdot t\;|\; s \in S \wedge t \in T\}
\]
The Language consisting of just the Empty String, $\{\varepsilon\}$,
acts as an Identity Element:
\[
  S \cdot \{\varepsilon\} = S = \{\varepsilon\} \cdot S
\]
and the Empty Language, $\{\}$, acts as a Zero Element:
\[
  S \cdot \{\} = \{\} = \{\} \cdot S
\]
With a third Language, $U$, the Associative Property gives:
\[
  S \cdot (T \cdot U) = (S \cdot T) \cdot U
\]



% --------------------------------------------------------------------
\subsection{Prefix Closure}\label{sec:prefix_closure}
% --------------------------------------------------------------------

The \emph{Prefix Closure} of a Language $L$ is given by the Function
$\mathrm{Pref}(L)$:
\[
  \mathrm{Pref}(L) = \bigcup_{s \in L} \mathrm{Pref}_L(s) =
  \{ t\;|\;s = tu; s \in L; t,u \in \mathrm{Alph}(L)^* \}
\]
where $\mathrm{Pref}_L(s)$ is the Set of Prefixes
(\S\ref{sec:prefix_suffix}) of String $s$ with respect to $L$. A
Language is \emph{Prefix Closed} if $\mathrm{Pref}(L) = L$.
$\mathrm{Pref}$ is Idempotent:
\[
  \mathrm{Pref}(\mathrm{Pref}(L)) = \mathrm{Pref}(L)
\]



% --------------------------------------------------------------------
\subsection{Substitution Closure}\label{sec:substitution_closure}
% --------------------------------------------------------------------

The \emph{Substitution Closure} of a Language $L$ is defined as:
\[
  f(L) = \bigcup_{s \in L} f(s)
\]
where $f(s)$ is the String Substitution Function
(\S\ref{sec:string_substitution}). Regular
(\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are Closed under String
Substitution.



% --------------------------------------------------------------------
\subsection{Language Homomorphism}\label{sec:language_homomorphism}
% --------------------------------------------------------------------

A \emph{Homomorphism}, $h$, of a Language, $L$, with Alphabet,
$\Sigma$, is defined in terms of a String Homomorphism
(\S\ref{sec:string_homomorphism}) $f(s)$, where $s \in L$:
\[
  h(L) = \bigcup_{s \in L} f(s)
\]
and the Inverse Homomorphic Image is then:
\[
  h^{-1}(L) = \{ s\;|\; f(s) \in L \}
\]
In General $h(h^{-1}(L)) \neq L$, but:
\[
  h (h^{-1}(L)) \subseteq L
\] and:
\[
  L \subseteq h^{-1}(h(L))
\]
Regular (\S\ref{sec:regular_language}) and Context-free
(\S\ref{sec:context_free}) Languages are Closed under Homomorphisms
and Inverse Homomorphisms.



% --------------------------------------------------------------------
\subsection{Language Projection}\label{sec:language_projection}
% --------------------------------------------------------------------

Using a String Projection (\S\ref{sec:string_projection}),
$\pi_{\Sigma}(s)$, on Strings, $s$, the \emph{Language Projection} for
a Language, $L$, is defined as:
\[
  \pi_{\Sigma}(L) = \{\pi_{\Sigma}(s)\;|\; s \in L\}
\]



% ====================================================================
\section{Formal Grammar}\label{sec:formal_grammar}
% ====================================================================

The term \emph{Syntax} may be used as a synonym for \emph{Grammar}:
i.e. a Set of Rules through which the Strings of a Language can be
either \emph{Generated} (\S\ref{sec:generative_grammar}) or
\emph{Recognized} (\S\ref{sec:determinative_grammar}).

Syntax is the aspect of Formal Languages that refers only to the
literal Strings of Symbols of a Language with no regard to the Meaning
(\S\ref{sec:meaning}) or Interpretation (\S\ref{sec:interpretation});
only the condition that they can be identified and differentiated from
one-another is required. The process of Syntactic Analysis is known as
\emph{Parsing} (\S\ref{sec:parser}).



% --------------------------------------------------------------------
\subsection{Generative Grammar}\label{sec:generative_grammar}
% --------------------------------------------------------------------

A \emph{Generative Grammar} \emph{Generates} a Language by the
repeated application of \emph{Production Rules} beginning with a
unique \emph{Non-terminal Symbol} called the Start Symbol, here
denoted $S$.

The definition of a Non-terminal Symbol in the context of a Generative
Grammar is one for which a Production Rule exists with that Symbol
appearing in the input and replaced in the output. A \emph{Terminal
  Symbol} is one for which no Production Rule exists with that Symbol
occuring in the input.

A sequence of rule applications is a \emph{Derivation} (cf.
\S\ref{sec:formal_proof} and \S\ref{sec:logical_argument}).

A Generative Grammar may be formally defined as a 4-tuple:
\[
  G(N,T,P,S)
\]
where $N$ are Non-terminal Symbols, $T$ are Terminal Symbols, $P$ are
Production Rules and $S$ is the Start Symbol.

An unrestricted Production Rule has the form:
\[
  (N \cup T)^*N(N \cup T)^* \rightarrow (N \cup T)^*
\]
That is, a Production is a function from one Expression to
another, where the left Expression must contain at least one
Non-terminal Symbol. By convention, Non-terminal Symbols
will be denoted by capitals ($A,B,C,\cdots$), and Terminals by
lowercase ($a,b,c,\cdots$), and Expressions by Greek letters
($\alpha,\beta,\gamma$).



% --------------------------------------------------------------------
\subsection{Determinative Grammar}\label{sec:determinative_grammar}
% --------------------------------------------------------------------

A \emph{Determinative Grammar} is a Grammar through which a Member of
$\Sigma^*$ can be determined to belong to a Language $L \subseteq
\Sigma^*$ (\emph{Recognized}, note this is different from the use of
\emph{Recognizable} (\S\ref{sec:recognizable}) in the case of
Monoids). Such systems are described by \emph{Automata Theory}
(\S\ref{sec:automata_theory})

Syntactic Monoid (\S\ref{sec:syntactic_monoid})



% --------------------------------------------------------------------
\subsection{Chomsky Hierarchy}\label{sec:chomsky_hierarchy}
\cite{chomsky56}
% --------------------------------------------------------------------

Grammars are classified by how restrictive the Production Rules are.
By convention, they may be organized into a hierarchy of Classes under
Proper Inclusion, where \emph{Type-0} is an Unrestricted Grammar,
covering all possible Formal Grammars:

  Type-0 $\supset$ Type-1 $\supset$ Type-2 $\supset$ Type-3 \\
These different levels in the hierarchy are Recognizable
(\S\ref{sec:determinative_grammar}) by different kinds of Automata
(\S\ref{sec:automaton}).



% --------------------------------------------------------------------
\subsection{Type-0: Unrestricted Grammar}\label{sec:unrestricted_grammar}
% --------------------------------------------------------------------

\subsubsection{Semi-decidable Language}\label{sec:semidecidable}

Production Rules of an \emph{Unrestricted Grammar} have the form
\[
  \alpha \rightarrow \beta
\]
where $\alpha$ and $\beta$ are Expressions of $N \cup T$ and $\alpha
\neq \varepsilon$. Note that this means there is not necessarily a
distinction between Terminal and Non-terminal Symbols in an
Unrestricted Grammar.

A completely Unrestricted Grammar generates a Language called
\emph{Recursively Enumerable} or \emph{Semi-decidable}
(\S\ref{sec:partial_computable}). This means membership of the
Language can be decided by an Algorithm, but non-membership cannot,
and the Class of Languages having this property is called
$\mathsf{RE}$.

The complement of $\mathsf{RE}$ is the Class of Languages for which an
Algorithm may decide non-membership only and is termed
$\mathsf{coRE}$. The Class of Automata capable of implementing these
Algorithms are Turing Machines (\S\ref{sec:turing_machine}).



\subsubsection{Decidable Language}\label{sec:decidable_language}

A \emph{Decidable} or \emph{Recursive} Language is defined as the
intersection of $\mathsf{RE}$ and $\mathsf{coRE}$:
\[
  \mathsf{R} = \mathsf{RE} \cap \mathsf{coRE}
\]
That is, it can be decided whether a Symbol is a member or not by a
Total Computable Function (\S\ref{sec:computable_function}). Decidable
Languages are Recognizable by a \emph{Decider} or Total Turing Machine
(\S\ref{sec:turing_machine}).\cite{kozen97}



% --------------------------------------------------------------------
\subsection{Type-1: Context-sensitive Grammar}
\label{sec:context_sensitive}
% --------------------------------------------------------------------

\emph{Context-sensitive Grammars} have the restriction that the result
of a Production is not shorter than the input. Formally stated,
Productions are of the form
\[
  \alpha \Gamma \beta \rightarrow \alpha \gamma \beta
\]
where $|\Gamma| \leq |\gamma|$. In this formulation $\alpha$ and
$\beta$ form the \emph{Context} of $\Gamma$.

Requiring that $S$ does not appear on the right of any Production
and allowing the rule
\[
  S \rightarrow \varepsilon
\]
makes the Context-sensitive Languages a proper Superset of the
Context-free Languages (\S\ref{sec:context_free}).

Context-sensitive Languages are equivalent to Linear Bounded Automata
(\S\ref{sec:linear_bounded_automaton}).



\subsubsection{Indexed Grammar}\label{subsubsection:indexed_grammar}

An \emph{Indexed Grammar} has an extra set of \emph{Index Symbols},
$F$, with Productions of three possible forms,
\[
  A[\sigma] \rightarrow \alpha[\sigma]
\]\[
  A[\sigma] \rightarrow B[f\sigma]
\]\[
  A[f\sigma] \rightarrow \alpha[\sigma]
\]
where $f \in F$ and $\sigma$ is a String of Index Symbols. The Index
Symbols are used to form a \emph{Stack} by the Production Rules where
Index Symbols are either pushed or popped from the Stack.

An Indexed Language can be Recognized by a Nested Stack Automaton
(\S\ref{sec:nested_stack_automaton}).\cite{aho69}



\subsubsection{Generalized Contex-free}\label{sec:generalized_context_free}

A \emph{Generalized Context-free Grammar} adds to the Production Rules
of a Context-free Grammar a set of Non-context-free \emph{Composition
  Functions} that combine tuples of Symbols:
\[
  f(\langle x_1,\cdots,x_m\rangle,\cdots,\langle
  y_1,\cdots,y_n\rangle)=\gamma
\]
where $\gamma$ is a single tuple or another Composition Function that
reduces to a single tuple.

Rules are of the form:
\[
  A \rightarrow f(X,Y,\cdots)
\]
where $X$,$Y$,$\cdots$ are String tuples or Non-terminal Symbols.

There are several weakly equivalent Grammars to the composition
formulation:

\begin{description}
\item[Linear Context-free Rewriting System] \hfill \\
  Weakly equivalent to \emph{Multi-component Tree-adjoining Grammars}
  where Composition Functions are both \emph{Linear} and
  \emph{Regular}. Can be Recognized by Thread Automata
  (\S\ref{sec:thread_automaton})\cite{villemonte02}
  %FIXME may need to link to concepts of Linear and Regular

\item[Tree-adjoining] \hfill \\
  Elementary rewriting unit is a Tree (\S\ref{sec:tree_graph}) rather
  than a Symbol. Can be Recognized by Embedded Pushdown Automata
  (\S\ref{sec:embedded_pushdown})\cite{vijayashanker88}

\item[Linear Indexed Grammar] \hfill \\
  A modified Indexed Grammar where only one Symbol receives the
  Stack.

\item[Combinatory Categorical Grammar] \hfill \\
  A type of Phrase Structure Grammar (\S\ref{sec:context_free}) using
  Combinatory Logic (\S\ref{sec:combinatory_logic}).

\item[Head grammar] \hfill \\
  A Subset of the Linear Context-free Rewriting System and a Phrase
  Structure Grammar.
\end{description}



% --------------------------------------------------------------------
\subsection{Type-2: Context-free Grammar}\label{sec:context_free}
% --------------------------------------------------------------------

\emph{Context-free Grammars} (\emph{CFG}s) have Production Rules of
the form:
\[
  V \rightarrow \alpha
\]
where $V$ is a single Non-terminal and $\alpha$ is a String of Terminals
and/or Non-terminals (or $\varepsilon$). Because $V$ is required to be a
single Non-terminal, the Production Rules can be applied regardless of
Context. Each Non-terminal in a Context-free Grammar, $G$, is said to
form a \emph{Sublanguage} of the Language defined by $G$.

A Context-free Language is Closed under String Substitution
(\S\ref{sec:string_substitution}).

Multiple Context-free Grammars may generate the same Language, so
Properties of CFGs may be termed \emph{Extrinsic} while Language
Properties are \emph{Intrinsic}. The question of Equality between CFGs
is Undecidable.

A Context-free Language may also be called a \emph{Recursive
  Language}.

A popular notation for Context-free Grammars in Computer Science is
\emph{Backus-Naur form} (\emph{BNF}).

Context-free Grammars are equivalent to Non-deterministic Pushdown
Automata (\S\ref{sec:pushdown_automaton}).



\subsubsection{Constituency Grammar}\label{sec:constituency_grammar}

In Linguistics, the term used for Context-free Grammar is \emph{Phrase
  Structure Grammar} which is also called \emph{Constituency Grammar}
due to the one-to-one-or-many correspondence between the Productions
(ultimately rooted in the \emph{Subject-Predicate Clause} derived from
Term Logic \S\ref{sec:term_logic}). A Parse Tree
(\S\ref{sec:concrete_syntax}) may be constructed according to the
\emph{Constituency Relation} of a Constituency Grammar.



\paragraph{Transformational Grammar}\label{sec:transformational_grammar}



\paragraph{Categorial Grammar}\label{sec:categorial_grammar}

Montague Grammar (\S\ref{sec:montague_grammar})

\subparagraph{Lambek Grammar}\label{sec:lambek_grammar}

\emph{Lambek Calculus}

Type-logical Semantics (\S\ref{sec:typelogical_semantics})

%FIXME xref symmetric monoidal categories

\subparagraph{Combinatory Categorial Grammar}
\label{sec:combinatory_categorial}



\paragraph{Lexical Functional Grammar}\label{sec:lexical_functional}

Glue Semantics (\S\ref{sec:glue_semantics})



\subsubsection{Dependency Grammar}\label{sec:dependency_grammar}

An alternative formulation to Phrase Structure Grammar is
\emph{Dependency Grammar} in which the Verb is the root and there is a
one-to-one correspondence between Symbols and nodes in the Syntax
Structure (\S\ref{sec:concrete_syntax}).



\subsubsection{Deterministic}\label{sec:deterministic_cfg}

\emph{Deterministic Context-free Grammars} are derived from
Deterministic Pushdown Automata (\S\ref{sec:deterministic_pda}) and
are always unambiguous. They can be Parsed in Linear Time and a Parser
can be automatically generated from the Grammar by a Parser Generator
(\S\ref{sec:parser_generator}).



\subsubsection{Visibly Pushdown}\label{sec:visibly_pushdown}

\emph{Visibly Pushdown Grammars} are described by the 4-tuple
\[
  G = (V=V^0 \cup V^1,T,P,S)
\]
where $V^0$ and $V^1$ are Disjoint Sets of Non-terminals and there
are three kinds of Production Rules:
\[
  X \rightarrow \varepsilon
\]\[
  X \rightarrow aY
\]\[
  X \rightarrow \langle aZb \rangle Y
\]
where $Z \in V^0$ and if $X \in V^0$ then $Y \in V^0$

The resulting Language is a \emph{Regular Language} with \emph{Nested
  Words}, described by a Monadic Second-order Logic
(\S\ref{sec:monadic_secondorder}).
%FIXME possibly reference or explain nested words



% --------------------------------------------------------------------
\subsection{Type-3: Regular Grammar} \label{sec:regular_language}
% --------------------------------------------------------------------

\emph{Regular Languages} (or \emph{Token-level Languages}) are more
restricted than Context-free Languages (\S\ref{sec:context_free}) and
satisfy a number of Closure Properties. For two Regular Languages, $K$
and $L$, the following operations result in a Language that is also
Regular:
\[
  K \cup L, \quad
  K \cap L, \quad
  \overline{L}, \quad
  K - L, \quad
  K \circ L, \quad
  L^*, \quad
  K / L, \quad
  L^R
\]
Like Context-free Languages, Regular Languages are Closed under String
Substitution (\S\ref{sec:string_substitution}).

A common formulation of Regular Languages is using \emph{Regular
  Expressions} (\S\ref{sec:regular_expression}) and conversely it is
sometimes said that a Regular Language is one that can be defined by a
Regular Expression.

An algebraic description is as follows:
\[
  L = \{ w \in \Sigma^* | f(w) \in N \}
\]
where $f : \Sigma^* \rightarrow M$ is a Monoid Homomorphism of Finite
Monoid (\S\ref{sec:monoid}) $M$ and $N \subseteq M$.



\subsubsection{Regular Expression}\label{sec:regular_expression}

\emph{Regular Expression}



\paragraph{Star Height}\label{sec:star_height} \hfill \\

\emph{Star Height}



\subsubsection{Extended Regular}\label{sec:extended_regular}

\emph{Extended Regular Grammars} have Productions of either \emph{Right
Regular} or \emph{Left Regular} form.

Right:
\[
  B \rightarrow a
\]\[
  A \rightarrow B \nu
\]\[
  A \rightarrow \varepsilon
\]

Left:
\[
  A \rightarrow a
\]\[
  A \rightarrow B \nu
\]\[
  A \rightarrow \varepsilon
\]
where $a$ is a single Non-terminal and $\nu$ is an expression of only
Non-terminal characters.



\subsubsection{Strictly Regular}\label{sec:strictly_regular}

\emph{Strictly Regular Grammars} also have Productions of either Right
Regular or Left Regular form.

Right:
\[
  B \rightarrow a
\]\[
  B \rightarrow aC
\]\[
  B \rightarrow \varepsilon
\]

Left:
\[
  A \rightarrow a
\]\[
  A \rightarrow Ba
\]\[
  A \rightarrow \varepsilon
\]
where $a$ is a single Non-terminal.

There is a one-to-one correspondence between the rules of a
\emph{Strictly Left Regular Grammar} and those of a Non-deterministic
Finite Automaton (\S\ref{sec:ndfa}).

The \emph{Pumping Lemma} states that the middle section of an
Expression within a Regular Language may be repeated an arbitrary
number of times to produce another Expression in that same Language.



\paragraph{k-Testable}\label{sec:k_testable} \hfill \\

A \emph{k-Testable Language} is one where membership of an Expression
depends on the first and last Symbol and a Set of Factors of length
$k$. An example is a \emph{Local Language} which is a \emph{2-Testable
  Language} described by the Regular Expression:
\[
  (Q\Sigma^* \cap \Sigma^*R)\setminus\Sigma^*F\Sigma^*
\]
where $Q,R \subseteq \Sigma$ and $F \subseteq \Sigma \times \Sigma$.
This requires for an Expression, $w$, that is a member of a Local
Language to have its first Symbol in $Q$, and its second Symbol in
$R$, and no factor of $w$ of length 2 is in $F$. A Local Language is
Recognized by a \emph{Local Automaton} (\S\ref{sec:dfa}).



\subsubsection{Star-free}\label{sec:starfree_grammar}

A \emph{Star-free Language} is one having a Generalized Star Height
(\S\ref{sec:star_height}) equal to zero, that is, the minimal Star
Height of all Expressions in the Language with the Star Height of an
Expression's \emph{Complement} being equal.
% FIXME complement: ref or explain

Star-free Languages are characterized as those with Aperiodic
Syntactic Monoids
(\S\ref{sec:syntactic_monoid})\cite{schutzenberger65} and also as the
\emph{Counter-free Languages}\cite{mcnaughton-papert71} by the
Aperiodic Finite-state Automaton (\S\ref{sec:aperiodic_automaton}) and
Linear Temporal Logic (\S\ref{sec:linear_temporal}).



% --------------------------------------------------------------------
\subsection{Affix Grammar}\label{sec:affix_grammar}
% --------------------------------------------------------------------

\emph{Affix Grammars} are those of a Context-free Grammar
(\S\ref{sec:context_free}) with a Subset of the Non-terminals used as
\emph{Affix Arguments}. If the same Affix appears multiple places in a
Production, the value must be the same.



\subsubsection{Two-level Grammars}\label{sec:two_level_grammar}

Allowing the values for Affixes to be described by a Context-free
Grammar (\S\ref{sec:context_free}) results in a Two-level Grammar.
Two-level Grammars are \emph{Grammar Generators} that may
\emph{Generate} Grammars with infinite rules.



\begin{description}
\item[W-grammar] \emph{Van Wijngaarden Grammar} consists of a finite
  Set of \emph{Meta-rules} used to derive a possibly infinite set of
  Production Rules from a finite Set of \emph{Hyper-rules}.
\item[Extended Affix Grammar] is a restricted W-grammar.
\end{description}



\subsubsection{Attribute Grammar}\label{sec:attribute_grammar}
\cite{slonneger-kurtz95}

\emph{Attribute Grammars} allow for Affixes (called \emph{Attributes})
to take Values from arbitrary Domains and for Functions to calculate
Values of those Attributes. Attribute Grammars can be seen as
Context-free Grammars (\S\ref{sec:context_free}) extended to provide
Context-sensitivity using a Set of \emph{Attributes}, \emph{Attribute
  Values}, \emph{Evaluation Rules} and \emph{Conditions}.

A finite, possibly empty Set of Attributes is associated with each
distinct Symbol in the Grammar. Each Attribute has an associated
Domain of Values they may take on. Attributes are either
\emph{Synthesized Attributes} (\S\ref{sec:synthesized_attribute}) or
\emph{Inherited Attributes} (\S\ref{sec:inherited_attribute}).

Production Rules have additional Conditions that must be met by the
Attribute Values at that Node in the Parse Tree
(\S\ref{sec:concrete_syntax}).

In the context of Computer Science, Attribute Grammars can formally
express the rules for Type Checking and Declaration Order, as well as
the Operational Semantics of Programming Languages
(\S\ref{sec:operational_semantics}).

\emph{L-attributed Grammar}

\emph{LR-attributed Grammar}

\emph{ECLR-attributed Grammar}

\emph{S-attributed Grammar}



\paragraph{Synthesized Attribute}\label{sec:synthesized_attribute}
\hfill \\
A \emph{Synthesized Attribute} is passed from Child Node to Parent
Node.



\paragraph{Inherited Attribute}\label{sec:inherited_attribute}
\hfill \\
An \emph{Inherited Attribute} is passed from Parent Node to Child
Node.



% --------------------------------------------------------------------
\subsection{Analytic Grammar}\label{sec:analytic_grammar}
% --------------------------------------------------------------------

\emph{Analytic Grammars} are used in Parsing (\S\ref{sec:parser}). A
few examples:

\begin{description}
\item[Top-Down Parsing Language] \hfill \\
  Formal representation of Recursive Descent Parser
  (\S\ref{sec:recursive_descent}). Production rules of the form
  \[
    A \leftarrow \varepsilon
  \]\[
    A \leftarrow f
  \]\[
    A \leftarrow a
  \]\[
    A \leftarrow BC/D
  \]
\item[Parsing Expression Generator] \hfill \\
  A more generalized Top-Down Parsing Language
  (\S\ref{sec:topdown_parser}).
\item[Link Grammar] \hfill \\
  Dependency Grammar (\S\ref{sec:dependency_grammar}) with
  directionality between Symbols.
\end{description}



% --------------------------------------------------------------------
\subsection{Adaptive Grammar}\label{sec:adaptive_grammar}
% --------------------------------------------------------------------

\emph{Adaptive Grammars} allow for Production Rules to be manipulated
within the Grammar, including addition, deletion, and modification of
Rules.



\subsubsection{Imperative Adaptive Grammar}
\label{sec:imperative_adaptive}

Global

Rule changes are based on global State changing over time.

\begin{itemize}
  \item Extensible Context-Free Grammars
  \item Top-down Modifiable Grammars
  \item Bottom-up Modifiable Grammars
\end{itemize}



\subsubsection{Declarative Adaptive Grammar}
\label{sec:declarative_adaptive}

Local

Rule changes only affect the position in the Syntax Tree of the
generation of a String.

\begin{itemize}
  \item Christiansen Grammars
  \item Recursive Adaptive Grammars
\end{itemize}



\subsubsection{Time-space (Hybrid) Adaptive Grammar}
\label{sec:timespace_adaptive}

\begin{itemize}
  \item \S-Calculus
\end{itemize}



\subsubsection{Dynamic Grammars}\label{sec:dynamic_grammar}

Boullier\cite{boullier94}



% ====================================================================
\section{Abstract Reduction System}\label{sec:abstract_rewrite}
% ====================================================================

Formal Grammars may be abstracted as \emph{Abstract Reduction Systems}
(or \emph{Abstract Rewrite Systems}), abbreviated $ARS$. An ARS is
simply:
\[
  (A,\rightarrow)
\]
where $A$ is a Set of Objects (Expressions) and $\rightarrow \subseteq
A \times A$ is a Binary Relation on those Objects called the
\emph{Reduction Relation} (\S\ref{sec:reduction_relation}). The Object
appearing in the right-hand side of a Reduction Relation is called a
\emph{Reduct} (or \emph{Expansion}) of the left-hand side. This is
equivalent to an Unlabelled State Transition System
(\S\ref{sec:state_transition_system}).

An \emph{Indexed Abstract Reduction System} differentiates Reductions
into Classes so that $\rightarrow$ is the Indexed Union of these
relations:
\[
  (A, \rightarrow_1, \rightarrow_2, \cdots)
\]
This is identical to a Labeled Transition System
(\S\ref{sec:state_transition_system}).

A \emph{Reduction Sequence}, $x \rightarrow y \rightarrow z$, is
denoted $x \stackrel{*}\rightarrow z$, where $\stackrel{*}\rightarrow$
is the Reflexive Transitive Closure of $\rightarrow$ (see below).

Given the Reduction Relation, $\rightarrow$, for an ARS, the following
Relations may be defined:

\begin{description}

\item [$\stackrel{*}\rightarrow$]: Reflexive Transitive Closure of
  $\rightarrow$, the smallest Preorder (Reflexive and Transitive
  Relation) containing $\rightarrow$, that is, the Transitive Closure
  of $\rightarrow \cup =$:
  \[ (\rightarrow \cup =)^+ =
  \bigcup_{i \in \{1,2,3,...\}} (\rightarrow \cup =)^i \]

\item [$\leftrightarrow$]: Symmetric Closure of $\rightarrow$:
  \[ \rightarrow \cup \rightarrow^{-1} \]

\item [$\stackrel{*}\leftrightarrow$]: Reflexive Transitive
  Symmetric Closure of $\rightarrow$ (smallest Equivalence Relation
  containing $\rightarrow$), that is, the Transitive Closure of
  $\leftrightarrow \cup =$:
  \[ (\leftrightarrow \cup =)^+ =
  \bigcup_{i \in \{1,2,3,...\}} (\leftrightarrow \cup =)^i \]

\end{description}



% --------------------------------------------------------------------
\subsection{Reduction Relation}\label{sec:reduction_relation}
% --------------------------------------------------------------------

Operational Semantics (\S\ref{sec:operational_semantics})



\subsubsection{Rewrite Relation}\label{sec:rewrite_relation}

\emph{Rewrite Relation}

\emph{Rewrite Preorder}

\emph{Reduction Preorder}

\emph{Rewrite Closure}



% --------------------------------------------------------------------
\subsection{Normal Form}\label{sec:normal_form}
% --------------------------------------------------------------------

An Object of an Abstract Rewriting System $(A,\rightarrow)$ is in
\emph{Normal Form} (\emph{Irreducible}) if it cannot be Rewritten
further. That is, $x \in A$ is in Normal Form if $\nexists y \in A : x
\rightarrow y$. Otherwise $x$ is \emph{Reducible}.

For two Objects $x,y \in A$, $y$ is a Normal Form of $x$ if $x
\stackrel{*}{\rightarrow} y$ and $y$ is Irreducible. A unique Normal
Form of $x$ is denoted $x \downarrow$.



% --------------------------------------------------------------------
\subsection{Normalization}\label{sec:normalization}
% --------------------------------------------------------------------

An Object of an ARS is \emph{Weakly Normalizing} if some Rewrite
sequence leads to a Normal Form and \emph{Strongly Normalizing} if
every Rewrite sequence leads to a Normal Form.

An ARS is \emph{Normalizing} if every Object has at least one Normal
Form, and likewise is Weakly or Strongly Normalizing if every Object
is Weakly or Strongly Normalizing. A Strongly Normalizing ARS always
Terminates (\S\ref{sec:rewrite_termination}) and is therefore not Turing
Complete and cannot be used as a Self-interpreter (in Programming
Languages).

Untyped $\lambda$-calculus (\S\ref{sec:untyped_lambda}) lacks the
Normalization Property.

Systems of Typed $\lambda$-calculus such as Simply-typed
$\lambda$-calculus (\S\ref{sec:simply_typed}) and Calculus of
Constructions (\S\ref{sec:coq}) are Strongly Normalizing.



\subsubsection{Termination}\label{sec:rewrite_termination}

If all Objects of an ARS are Strongly Normalizing, then the system
itself is called \emph{Strongly Normalizing} (or \emph{Terminating} or
\emph{Noetherian}, cf. Noetherian Relation
\S\ref{sec:noetherian_relation}) and has no infinite Reduction
Sequences.



% --------------------------------------------------------------------
\subsection{Joinability}\label{sec:rewrite_join}
% --------------------------------------------------------------------

For an ARS $(A, \rightarrow)$, two Objects $x,y\in A$ are
\emph{Joinable} when:
\[
  \exists z \in A :
  x \stackrel{*}\rightarrow z \stackrel{*}\leftarrow y
\]
The \emph{Joinability Relation} can be defined as:
\[
  \stackrel{*}\rightarrow \circ \stackrel{*}\leftarrow
\]
where $\circ$ is Relation Composition
(\S\ref{sec:relation_composition}). As a Binary Relation, Joinability
is denoted $x \downarrow y$.

An ARS has the \emph{Church-Rosser Property} if and only if:
\[
  \forall x,y \in A, x \stackrel{*}\leftrightarrow y
  \Rightarrow x \downarrow y
\]
Equivalently, the Church-Rosser Property can be expressed as:
\[
  \stackrel{*}\leftrightarrow \subseteq \downarrow
\]
The Church-Rosser Property is equivalent to Confluence
(\S\ref{sec:rewrite_confluence}).

In a Church-Rosser system, an Object can have zero or one Normal
Forms. By the \emph{Church-Rosser Theorem}, $\lambda$-calculus
(\S\ref{sec:untyped_lambda}) has this property, which means that
Evaluation ($\beta$-reduction) can be carried out in any order. See
also Rosser's Trick (\S\ref{sec:rossers_trick}).



% --------------------------------------------------------------------
\subsection{Confluence}\label{sec:rewrite_confluence}
% --------------------------------------------------------------------

An ARS $(A, \rightarrow)$ is \emph{Confluent} (or \emph{Globally
  Confluent}) if and only if:
\[
  \forall w,x,y \in A,
  x \stackrel{*}\leftarrow w \stackrel{*}\rightarrow y
  \Rightarrow x \downarrow y
\]
That is, for any two diverging paths, they eventually meet at a common
successor. An individual Object can be said to be Confluent if it has
this property.

\emph{Semi-confluent}:
\[
  \forall w,x,y \in A,
  x \leftarrow w \stackrel{*}\rightarrow y
  \Rightarrow x \downarrow y
\]

\emph{Weakly Confluent} (or \emph{Locally Confluent}):
\[
  \forall w,x,y \in A,
  x \leftarrow w \rightarrow y \Rightarrow x \downarrow y
\]

\emph{Strongly Confluent}
\[
  \forall w,x,y \in A,
  x \leftarrow w \rightarrow y \Rightarrow
  \exists z \in A : x \stackrel{*}\rightarrow z \wedge
  (y \rightarrow z \vee y = z)
\]

A Semi-confluent system is necessarily Confluent. The Church-Rosser
Property, Confluence, and Semi-confluence are all equivalent
properties for an ARS. If Confluence is restricted to single
Reductions (rather than Reduction Sequences), the stronger property is
called the \emph{Diamond Property}.

A Reduction Relation $\rightarrow$ is Confluent if and only if
$\stackrel{*}\rightarrow$ is Locally Confluent.

By \emph{Newman's Lemma}, if an ARS is Locally Confluent and
Terminating, then it is Confluent.

Reduction of Polynomials Modulo an Ideal is a Confluent Rewrite System
when working with a Grobner Basis (\S\ref{sec:ring_ideal}).



% --------------------------------------------------------------------
\subsection{Convergence}\label{sec:rewrite_convergence}
% --------------------------------------------------------------------

An ARS that is both Confluent (\S\ref{sec:rewrite_confluence}) and
Terminating (\S\ref{sec:rewrite_termination}) is called \emph{Convergent} (or
\emph{Canonical}) and every Object has a unique Normal Form
(\S\ref{sec:normal_form}).



% --------------------------------------------------------------------
\subsection{String Rewriting System}\label{sec:string_rewriting}
% --------------------------------------------------------------------

A \emph{String Rewriting System} (\emph{SRS}, also \emph{Semi-Thue
  System}) is defined by a Tuple $(\Sigma, R)$ with Alphabet $\Sigma$
and Binary Relation $R$ corresponding to the Rewrite Rules between
some fixed Strings of Alphabet Symbols.

Monoid Presentation (\S\ref{sec:presentation})

Markov Algorithm (\S\ref{sec:markov_algorithm})



% --------------------------------------------------------------------
\subsection{Graph Rewriting System}\label{sec:graph_rewriting}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Term Rewriting System}\label{sec:term_rewriting}
% --------------------------------------------------------------------

A \emph{Term Rewriting System} is an Abstract Rewriting System with
\emph{Terms} (Formal Logic\S\ref{sec:term}) as Objects and a Rewrite
Relation, $\rightarrow_R$, given by the Set of \emph{Term Rewriting
  Rules}, $R$, of the form:
\[
  l \rightarrow r
\]
where $l$ and $r$ are a pair of Terms. Such a Rule is \emph{Applied}
to a Term $s$ if $l$ matches a Subterm of $s$.

Termination (\S\ref{sec:rewrite_termination}) is Decidable for Finite Ground
Systems, but Undecidable for Systems with a single Rule consisting of
a Linear (\S\ref{sec:linear_term}) left-hand side, or for Systems with
only Unary Function Symbols. % FIXME clarify finite ground system



% ====================================================================
\section{Parser} \label{sec:parser}
% ====================================================================

\emph{Parsing} is the process of \emph{Syntactic Analysis}. A
\emph{Parser} analyzes Expressions according to the rules of a Formal
Grammar, generating a Data Structure (\S\ref{sec:f_algebra})
describing the Syntax of the input.

The Data Structure produced by the Parser may be a \emph{Concrete
  Syntax Tree} (Parse Tree \S\ref{sec:concrete_syntax}), an
\emph{Abstract Syntax Tree} (Syntax Tree \S\ref{sec:abstract_syntax}),
or other hierarchical structure.



% --------------------------------------------------------------------
\subsection{Concrete Syntax Tree}\label{sec:concrete_syntax}
% --------------------------------------------------------------------

A \emph{Concrete Syntax Tree} (or \emph{Parse Tree}) is an Ordered,
Rooted Tree (\S\ref{sec:tree_graph}) representing the \emph{Syntactic
  Structure} of a String according to a Context-free Grammar
(\S\ref{sec:context_free}) such as the Constituency Relation of a
Constituency Grammar (\S\ref{sec:constituency_grammar}) or the
Depenency Relation of a Dependency Grammar
(\S\ref{sec:dependency_grammar})



% --------------------------------------------------------------------
\subsection{Abstract Syntax Tree}\label{sec:abstract_syntax}
% --------------------------------------------------------------------

\emph{Abstract Syntax} is a representation of the Source (Concrete)
Syntax being Parsed.

\emph{First-order Abstract Syntax} (FOAS), \emph{Higher-order Abstract
  Syntax} (HOAS)



\subsubsection{First-order Abstract Syntax}\label{sec:foas}

\emph{FOAS}

Abstract Structure

Concrete Names (Identifiers): requiring Name Resolution
(\S\ref{sec:name_resolution})

relation between Binding Site and Use indicated by using the same
Identifier



\subsubsection{Higher-order Abstract Syntax}\label{sec:hoas}

\emph{HOAS}

Abstract Structure

Abstract Names

Languages with Variable Binders %FIXME

each Use refers directly to Binding Site

$\alpha$-equivalent Programs have identical representations in HOAS

de Bruijn Indices (\S\ref{sec:debruijn_index})



\subsubsection{Name Binding}\label{sec:name_binding}

%FIXME
Free Variable (\S\ref{sec:free_variable})



\paragraph{Name Resolution}\label{sec:name_resolution}

%FIXME merge with name binding? wikipedia



\subsubsection{Homoiconicity}\label{sec:homoiconicity}

A Language for which the Abstract Syntax Tree and the Syntax are
Isomorphic is called \emph{Homoiconic}.

Circular Defenition (\S\ref{sec:circular_definition})

Reflection (\S\ref{sec:reflective_subcategory})



% --------------------------------------------------------------------
\subsection{Syntactic Analysis}\label{sec:syntactic_analysis}
% --------------------------------------------------------------------

\emph{Syntactic Analysis} is the process of Parsing input. The Parser
determines if and how the input can be derived from the Start Symbol
of a Context-free Grammar. Parsing can proceed in two directions:
\begin{description}
\item [Top-down Parsing] (\S\ref{sec:topdown_parser}) starts with
  the highest level of the Parse Tree (\S\ref{sec:concrete_syntax}).
  Proceeds greedily and may be \emph{Exponential} with
  \emph{Backtracking}.
\item [Bottom-up Parsing] (\S\ref{sec:bottomup_parser}) starts with
  the lowest level of the Parse Tree.
\end{description}
The output of Syntactic Analysis is a Data Structure
(\S\ref{sec:f_algebra}) such as a Concrete or Abstract Syntax Tree.
Further Context-sensitive Parsing may follow
(\S\ref{sec:semantic_analysis}).



\subsubsection{Lexical Analysis}\label{sec:lexical_analysis}

A Parser may be preceded by a \emph{Lexical Analyzer} (also
\emph{Lexer} or \emph{Tokenizer}) which creates \emph{Tokens} from
input Expressions. A Token is a structure representing a \emph{Lexeme}
(that is, a String forming a Syntactic unit) and associated
\emph{Type} and \emph{Value} information.

A Lexical Analyzer is itself a Parser and usually the \emph{Lexical
  Syntax} is defined by a Regular Language
(\S\ref{sec:regular_language}). The Tokens are then analyzed by the
Parser according to the rules of the \emph{Phrase Syntax}, which is
usualy a Context-free Language (\S\ref{sec:context_free}). A Parser
without a separate Lexer is called a \emph{Scannerless Parser}.

Lexing may be divided into two stages: \emph{Scanning} and
\emph{Evaluation}. Prior to Tokenization, a \emph{Scanner} (usually a
Finite-state Machine \S\ref{sec:finite_automaton}) may perform its own
Lexical Analysis, producing Lexemes categorized by Token class. An
\emph{Evaluator} then produces the final Tokens by either adding Value
information to the Type to produce a Type-Value pair, or returning
just the Type, or possibly also suppressing the Lexeme so the Parser
doesn't see it (e.g. whitespace or comments).

Token identification methods include Regular Expressions
(\S\ref{sec:regular_expression}), \emph{Flags} (specific sequences),
\emph{Delimiters} (specific separators), or explicit
\emph{Dictionaries}.



\subsubsection{Semantic Analysis}\label{sec:semantic_analysis}

\emph{Semantic Analysis} (or \emph{Symantic Parsing} or
\emph{Contextual Analysis}) may be performed after Syntactic Analysis.
Parsing for Context-sensitive Semantics, such as Type Checking or
declaration order, can be formally expressed with Attribute Grammars
(\S\ref{sec:attribute_grammar}).

A further example would be the in the resulting actions of Evaluation
in an Interpreter or Code Generation of a Compiler for a Programming
Language, for which purpose an Attribute Grammar may also be used.



% --------------------------------------------------------------------
\subsection{Top-down Parser}\label{sec:topdown_parser}
% --------------------------------------------------------------------

\subsubsection{Recursive Descent Parser}\label{sec:recursive_descent}

Mutual (Indirect) Recursion (\S\ref{sec:indirect_recursion})



\subsubsection{LL Parser}\label{sec:ll_parser}

\subsubsection{Early Parser}\label{sec:early_parser}



% --------------------------------------------------------------------
\subsection{Bottom-up Parsers}\label{sec:bottomup_parser}
% --------------------------------------------------------------------

\subsubsection{Precedence Parser}\label{sec:precedence_parser}

\subsubsection{LR Parser}\label{sec:lr_parser}

\emph{Canonical LR} LR(1)

\paragraph{SLR Parser}\label{sec:slr_parser}

\paragraph{LALR Parser}\label{sec:lalr_parser}

\paragraph{GLR Parser}\label{sec:glr_parser}



\subsubsection{CYK Parser}\label{sec:cyk_parser}

\subsubsection{Recursive Ascent Parser}\label{sec:recursive_ascent}



% --------------------------------------------------------------------
\subsection{Parser Generator}\label{sec:parser_generator}
% --------------------------------------------------------------------

A \emph{Parser Generator} takes as a Grammar (for example a BNF
Grammar) and outputs the source code of a Parser for the Language
specified by the Grammar.



% --------------------------------------------------------------------
\subsection{Template Processing}\label{sec:template_processing}
% --------------------------------------------------------------------



% ====================================================================
\section{Automata Theory}\label{sec:automata_theory}
% ====================================================================

%FIXME section needs a rewrite or cleanup

% --------------------------------------------------------------------
\subsection{Abstract Machine} \label{sec:abstract_machine}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{State Transition Systems} \label{sec:state_transition_system}
% --------------------------------------------------------------------

A \emph{State Transition System} can have an infinite number of
\emph{States} and \emph{Transitions}, represented as the pair:
\[
  (S,\rightarrow)
\]
where $S$ is a set of States and $\rightarrow \subseteq S \times S$.
This is identical to an Un-indexed Abstract Rewriting System
(\S\ref{sec:abstract_rewrite}).

\emph{Labelled State Transition Systems} have an additional set of
\emph{Labels}, $\Lambda$:
\[
  (S,\Lambda,\rightarrow)
\]
and $\rightarrow \subseteq S \times \Lambda \times S$.

\emph{Action Programming Languages} add a set of \emph{Fluents}, $F$, and
\emph{Values}, $V$, and a function mapping $F \times S$ to $V$.

Finite Automata (\S\ref{sec:finite_automaton}) may be seen as State
Transition Systems with an initial State and a number of final
\emph{Accept} states indicating \emph{Word} (Expression) membership
for a Language.



\subsubsection{Bisimulation} \label{sec:bisimulation}

Universal Coalgebra (\S\ref{sec:universal_coalgebra})



% --------------------------------------------------------------------
\subsection{Semiautomaton}\label{sec:semiautomaton}
% --------------------------------------------------------------------

A State Transition System may be formulated as a \emph{Semiautomaton}
\[
  (Q,\Sigma,T)
\]
where $\Sigma$ is a Non-empty Set of \emph{Input Symbols}, $Q$ is the
Set of States, and $T$ is a \emph{Transition Function}:
\[
  T:Q \times \Sigma \rightarrow Q
\]
A Semiautomaton induces a Monoid called the \emph{Transition Monoid}
(\S\ref{sec:transition_monoid}):
\[
  M(Q,\Sigma,T) = \{T_w | w \in \Sigma^*\}
\]

A Semiautomaton induces a Monoid Action (\S\ref{sec:monoid_action})

Transformation Semigroup (\S\ref{sec:transformation_semigroup})




% --------------------------------------------------------------------
\subsection{Automaton} \label{sec:automaton}
% --------------------------------------------------------------------

An \emph{Automaton} reads \emph{Input Strings} (\emph{Words}) and
either \emph{Accepts} or \emph{Rejects} the Words depending on whether
they are a member of the Language \emph{Recognized} by that Automaton.
By convention the Set of Expressions from Formal Languages will be
re-cast as Alphabets of Words, $\Sigma$. The Set of all Words Accepted
by an Automaton are those belonging to a Formal Language.

Automata may be arranged in a hierarchy according to increasing
power:\\
DFA = NFA $\subset$ DPDA-I $\subset$ NPDA-I $\subset$ LBA
$\subset$ DPDA-II = NPDA-II = DTM = NTM = PTM = MDTM

where
\begin{itemize}
\item DFA = Deterministic Finite Automata (\S\ref{sec:dfa})
\item NFA = Non-deterministic Finite Automata (\S\ref{sec:ndfa})
\item DPDA = Deterministic Pushdown Automata
  (\S\ref{sec:deterministic_pda}) with 1 or 2 push-down stores
\item NPDA = Non-deterministic Pushdown Automata
  (\S\ref{sec:pushdown_automaton}) with 1 or 2 push-down stores
\item LBA = Linear Bounded Automata
  (\S\ref{sec:linear_bounded_automaton})
\item DTM = Deterministic Turing Machine
  (\S\ref{sec:deterministic_turing_machine})
\item NTM = Non-deterministic Turing Machine
  (\S\ref{sec:nondeterministic_turing_machine})
\item PTM = Probabilistic Turing Machine
  (\S\ref{sec:probabilistic_turing_machine})
\item MDTM = Multidimensional Turing Machine
  (\S\ref{sec:multidimensional_turing_machine})
\end{itemize}



% --------------------------------------------------------------------
\subsection{Finite Automaton}\label{sec:finite_automaton}
% --------------------------------------------------------------------

\emph{Finite Automata} are \emph{Finite-state Machines} and take a
finite Input String of Symbols and either Accept or Reject the Input
depending on the final State of the computation. Finite Automata are
able to recognize Regular Languages(\S\ref{sec:regular_language}).



\subsubsection{Deterministic Finite Automaton}\label{sec:dfa}

\emph{Deterministic Finite Automata} have the restriction that an
Input Symbol has a Transition Function to a single State.
Deterministic Finite Automata recognize Regular
Languages (\S\ref{sec:regular_language}).

Representation of a Deterministic Finite Automaton as a 5-tuple:
\[
  (Q,\Sigma,\delta,q_0,F)
\]
where:
\begin{itemize}
  \item $Q$ is a Finite Set of States
  \item $\Sigma$ is the Alphabet
  \item $\delta$ is the transition function $\delta: Q \times
    \Sigma \rightarrow Q$
  \item $q_0 \in Q$ is the Initial State
  \item $F \subseteq Q$ is the Set of final Accept States.
\end{itemize}

Running for a given input $w = a_1,a_2, \cdots , a_n \in \Sigma^*$
produces a sequence of States $q_0,q_1,q_2,\cdots , q_n$ where $q_i
\in Q$ such that $q_i = \delta (q_{i-1},a_i)$ and $w$ is accepted if
$q_n \in F$.

A recursive definition using Composition of Transition Functions:
\[
  \widehat{\delta}(q,\varepsilon) = q
\]\[
  \widehat{\delta}(q,wa) = \delta_a(\widehat{\delta}(q,w))
\]
where $w \in \Sigma^*$, $a \in \Sigma$ and $q \in Q$. Repeated
application describes the Transition Monoid
(\S\ref{sec:semiautomaton}).



\paragraph{Minimum DFA}\label{sec:minimum_dfa}\hfill \\

For each Regular Language accepted by a DFA, there is a \emph{Minimum
  DFA} which is unique up to Isomorphism with a minimum number of
States. \emph{Minimization} from the original DFA is achieved by
removing \emph{Unreachable States} and merging
\emph{Nondistinguishable States}.



\paragraph{Local Automaton}\label{sec:local_automaton}\hfill \\

A \emph{Local Automaton} is a DFA for which all Edges with the same
Label lead to a single Vertex.

Local Automata Recognize Local Languages (\S\ref{sec:k_testable}).



\subsubsection{Non-deterministic Finite Automaton}\label{sec:ndfa}

\emph{Nondeterministic Finite Automata} are Finite-state Machines that
may transition from one State to a number of different states, given
as an Element of the Powerset of $Q$, $\pow(Q)$.

Representation of a Nondeterministic Finite Automaton as a
5-tuple:
\[
  (Q,\Sigma,\Delta,q_0,F)
\]
where:
\begin{itemize}
  \item $Q$ is a Finite Set of States
  \item $\Sigma$ is the Alphabet
  \item $\Delta$ is a \emph{Transition Relation} $\Delta: Q \times
    \Sigma \rightarrow \pow(Q)$
  \item $q_0 \in Q$ is the Initial State
  \item $F \subseteq Q$ is the Set of final Accept States.
\end{itemize}

A Word, $w=a_1,a_2,\cdots,a_n$, is Accepted when there exists a
Sequence of States, $r_0,r_1,\cdots,r_n$ such that:
\begin{enumerate}
  \item $r_0 = q_0$
  \item $r_{i+1} \in \Delta(r_i, a_{i+1})$, for $i = 0, \cdots, n-1$
  \item $r_n \in F$
\end{enumerate}

A DFA (\S\ref{sec:dfa}) may be seen as a NFA which restricts
transitions to allow only one State, and can be constructed from a NFA
with $n$ States using \emph{Powerset Construction}, requiring up to
$2^n$ States. Both types recognize the same Regular Languages
(\S\ref{sec:regular_language}).



\paragraph{NFA-$\varepsilon$}\label{sec:nfa_e}\hfill \\
An \emph{NFA-$\varepsilon$} is a NFA that allows transitions without
consuming Input Symbols. A Transition that changes state without
consuming Input is an \emph{$\varepsilon$-move}. Each State $q$
defines an $\varepsilon$-\emph{closure}, $E(q)$, which is the set of
States that are reachable by $\varepsilon$-moves.

The Languages recognized by NFA-$\varepsilon$ are the same as NFA/DFA.



\subsubsection{Aperiodic Finite-state Automaton}
\label{sec:aperiodic_automaton}



% --------------------------------------------------------------------
\subsection{Pushdown Automaton}\label{sec:pushdown_automaton}
% --------------------------------------------------------------------

\emph{Pushdown Automata} add to Finite Automata a \emph{Stack} as a
parameter for choice of States and can recognize Context-free
Languages(\S\ref{sec:context_free}).

Adding a second Stack makes a Pushdown Automaton equal in power to a
Turing Machine.

Unlike Finite Automata, Deterministic PDA are not equivalent to
Nondeterministic PDA. The general representation for a PDA is:
\[
  M = (Q, \Sigma, \Gamma, q_0, Z_0, F, \delta)
\]
where:
\begin{itemize}
  \item $Q$ is a Finite Set of States
  \item $\Sigma$ is a Finite Set of input Symbols
  \item $\Gamma$ is a Finite Set of Stack Symbols
  \item $q_0 \in Q$ is the Initial State
  \item $Z_0 \in \Gamma$ is the Initial Stack Symbol
  \item $F \subseteq Q$ is the Set of final Accept States
  \item $\delta$ is the Transition Function $\delta: (Q \times (\Sigma
    \cup \{\varepsilon\}) \times \Gamma) \rightarrow \pow(Q \times
    \Gamma^*)$
\end{itemize}

An Element $(p,a,Z,q,\alpha)\in\delta$, with $M$ in State $p \in Q$,
Input $a \in \Sigma \cup \{\varepsilon\}$, and top Stack Symbol $Z \in
\Gamma$ results in the following:
\begin{enumerate}
  \item read $a$
  \item change state to $q$
  \item pop $Z$
  \item push $\alpha \in \Gamma^*$
\end{enumerate}



\subsubsection{Deterministic Pushdown Automaton}
\label{sec:deterministic_pda}

\emph{Deterministic Pushdown Automata} have the restriction of only
one Derivation per Accepted input Word. This allows recognition of a
Subset of Context-free Languages termed Deterministic
(\S\ref{sec:deterministic_cfg}). Such Languages can be parsed in
Linear Time and Parsers for such Languages can be generated
automatically (\S\ref{sec:parser_generator}).

A Pushdown Automata is Deterministic iff both:
\begin{enumerate}
\item $\forall q \in Q, a \in \Sigma \cup {\varepsilon}, x \in
  \Gamma \vdash |\delta(q,a,x)| \leq 1$
\item $\forall q \in Q, x \in \Gamma \vdash |\delta(q,\varepsilon,x)|
  \neq 0 \Rightarrow \forall a \in \Sigma \vdash |\delta(q,a,x)|=0$
\end{enumerate}



\subsubsection{Embedded Pushdown Automaton}\label{sec:embedded_pushdown}

\subsubsection{Nested Stack Automaton}\label{sec:nested_stack_automaton}

\subsubsection{Thread Automaton}\label{sec:thread_automaton}



% --------------------------------------------------------------------
\subsection{Linear Bounded Automaton} \label{sec:linear_bounded_automaton}
% --------------------------------------------------------------------

\emph{Linear Bounded Automata} are Turing Machines restricted to an
Input of finite length and are Acceptors for Context-sensitive
Languages (\S\ref{sec:context_sensitive}) which require that
Production Rules do not increase the size of the Expression as a
result; therefore the size of the Input is sufficient for calculation.



% --------------------------------------------------------------------
\subsection{Turing Machine}\label{sec:turing_machine}
% --------------------------------------------------------------------

A \emph{Turing Machine} operates on an infinite \emph{Storage Tape},
which acts as the Read Input as well as Write Storage. Pushdown
Automata (\S\ref{sec:pushdown_automaton}) with 2 Stacks are equivalent
to Turing Machines.

A Turing Machine that halts for every Input is a \emph{Total Turing
  Machine} or \emph{Decider}.



\subsubsection{Non-deterministic Turing Machine}
\label{sec:nondeterministic_turing_machine}

\emph{Non-deterministic Turing Machines} (\emph{NTM}s) can be defined
as:
\[
  M = (Q, \Sigma, q_0, \sqcup, A, \delta)
\]
where:
\begin{itemize}
  \item $Q$ is a finite set of States
  \item $\Sigma$ is the finite Alphabet
  \item $q_0 \in Q$ is the initial State
  \item $\sqcup \in \Sigma$ is the blank Symbol
  \item $F \subseteq Q$ is the set of final Accept States
  \item $\delta \subseteq (Q \setminus F \times \Sigma) \times (Q
    \times \Sigma \times \{L,R\})$ and $L$ and $R$ are left and right
    shift.
\end{itemize}

The operation of $M$ in State $q_i$ and current Read Input $a_j$ is a
Transition Function, $q_i a_j \rightarrow q_{i1} a_{j1} d_k$. Note
that for an NTM, $\delta$ is a Relation and more than one Function can
exist for each possible Input/State combination. The result is to
Write the new Symbol $a_{j1}$ in the current position and shift the
Storage left or right as specified by $d_k$, afterwards assuming State
$q_{i1}$.



\subsubsection{Deterministic Turing Machine}
\label{sec:deterministic_turing_machine}

\emph{Deterministic Turing Machines} (\emph{DTM}s) have one possible
Output Transition per unique Input/State combination, thus $\delta$ is
a Partial Function rather than a Relation:
\[
  \delta : Q \setminus F \times \Sigma \rightarrow Q \times
  \Sigma \times {L,R}
\]
The computational power of DTMs and NTMs is equivalent (they can solve
the same problems) as NTMs include DTMs as a special case. An
equivalent Accepting Computation in a DTM is generally Exponential to
the length of the shortest Accepting Computation of an NTM.



\subsubsection{Probabilistic Turing Machine}
\label{sec:probabilistic_turing_machine}

A \emph{Probabilistic Turing Machine} adds to Transitions a
Probability Distribution (or a Tape with random Symbols). It is an
open question whether this is more powerful than a DTM
($\mathsf{BPP}=\mathsf{P}$ ?)  but it is useful in the definition of
Interactive Proof Systems. %FIXME refs



\subsubsection{Multidimensional Turing Machine}
\label{sec:multidimensional_turing_machine}

\emph{Multidimensional Turing Machines} allow for Tapes of varying
Topologies (Part \ref{sec:topology}). This requires additional Shift
directions (i.e. $\{L, R, U, D\}$ for a 2-dimensional tape) but does
not increase the computing power; even an $\infty$-dimensional Turing
Machine can be simulated by a DTM.



% ====================================================================
\section{Metalanguage}\label{sec:metalanguage}
% ====================================================================

A \emph{Metalanguage} is a Language used to describe another Language,
the \emph{Object Language}.

Metalanguage Expressions find use in:
\begin{itemize}
  \item Deductive Systems (\S\ref{sec:deductive_apparatus})
  \item Metavariables (\S\ref{sec:metavariable})
  \item Metatheory (\S\ref{sec:metatheory})
  \item Interpretations (\S\ref{sec:interpretation})
\end{itemize}

Note it is common to use Greek letters ($\Phi$, $\Psi$) for
Metavariables and Roman characters otherwise for Object Variables.



% --------------------------------------------------------------------
\subsection{Metasyntax}\label{sec:metasyntax}
% --------------------------------------------------------------------

The \emph{Metasyntax} describes the structure of valid Expressions in
a Metalanguage. There are three types of Symbols distinguished in
Metasyntax:
\begin{description}
  \item [Terminal Symbol] stand-alone Syntactic structure in the
    Object Language
  \item [Non-terminal Symbol] Syntactic category defining a Set of
    valid Expression structures in an $n$-element Subset
  \item [Metasymbol] Metalinguistic denotations
\end{description}
Depending on the Object Language, not all of these Categories may be
used. For example, Metasyntax for Regular Languages
(\S\ref{sec:regular_language}) does not have Non-terminals.



% --------------------------------------------------------------------
\subsection{Embedded Metalanguage}\label{sec:embedded_metalanguage}
% --------------------------------------------------------------------

The use of an Object Language to describe itself is an \emph{Embedded
  Metalanguage} (e.g. the English words \emph{noun} and \emph{verb}
are used to describe English itself).



% --------------------------------------------------------------------
\subsection{Ordered Metalanguage}\label{sec:ordered_metalanguage}
% --------------------------------------------------------------------

An \emph{Ordered Metalanguage} is constructed to discuss an
\emph{Object Language}, and a further Ordered Metalanguage may be
constructed to discuss the first.

analogous to \emph{Ordered Logic}
(\S\ref{sec:noncommutative_logic})



% --------------------------------------------------------------------
\subsection{Hierarchical Metalanguage}\label{sec:hierarchical_metalanguage}
% --------------------------------------------------------------------

an Ordered Hierarchy of Embedded Metalanguages
