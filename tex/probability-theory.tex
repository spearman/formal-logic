%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Probability Theory}\label{part:probability_theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ====================================================================
\section{Measure Theory}\label{sec:measure_theory}
% ====================================================================

% --------------------------------------------------------------------
\subsection{$\sigma$-algebra}\label{sec:sigma_algebra}
% --------------------------------------------------------------------

A \emph{$\sigma$-algebra} on a Set $X$ is a Non-empty Set $\mathcal{A}
\subseteq 2^X$ that is Closed under Set Operations of Complement
(\S\ref{sec:absolute_complement}) and Countable Union (\S\ref{sec:set_union}).

\begin{enumerate}
  \item $\varnothing \in \mathcal{A}$
  \item $X \in \mathcal{A}$
  \item $(E_i)_{i \geq 1} \in \mathcal{A} \Rightarrow
    \bigcap_{i \geq 1} E_i \in \mathcal{A}$
\end{enumerate}

$\Sigma$ Subsets of $X$

$\{ \varnothing, X \}$

Collection of $\sigma$-algebras $\{ \Sigma_\alpha : \alpha \in \class{A} \}$

$\mathcal{F}$ Family of Subsets of $X$, $\sigma(\mathcal{F})$ is the
$\sigma$-algebra Generated by $\mathcal{F}$

$\sigma(\varnothing) = \{ \varnothing, X \}$



\subsubsection{Borel Algebra}\label{sec:borel_algebra}

Collection of all Borel Sets (\S\ref{sec:borel_set})

Smallest $\sigma$-algebra containing all Open Sets of $\reals$:
$\struct{B}_\reals$ is equal to the Minimal $\sigma$-algebra Generated by the
Closed Sets $E_2 = \{ [a,b] \;|\; a < b \}$.



\paragraph{Borel Hierarchy}\label{sec:borel_hierarchy}\hfill



% --------------------------------------------------------------------
\subsection{Measure}\label{sec:measure}
% --------------------------------------------------------------------

Set $X$

$(X,\Sigma)$ -- $\sigma$-algebra (\S\ref{sec:sigma_algebra}) on $X$

A \emph{Measure} on $(X,\Sigma)$:
\[
  \mu : \Sigma \rightarrow [0,\infty]
\]

\begin{enumerate}
  \item $\mu(\varnothing) = 0$

  \item For $(E_i)_{i \geq 1} \in \Sigma$ with $E_i \cap E_j =
    \varnothing$ when $i \neq j$, then:
    \[
      \mu (\bigcup_i E_i) = \sum_i \mu(E_i)
    \]
\end{enumerate}

$(X,\Sigma,\mu)$ is called a \emph{Measure Space}
(\S\ref{sec:measure_space})

Measurable



\subsubsection{Measure Space}\label{sec:measure_space}

$(X,\Sigma,\mu)$

A Measure Space with a Probability Measure
(\S\ref{sec:probability_measure}) is a Probability Space
(\S\ref{sec:probability_space})

For $E,F \in \Sigma$:
\[
  \mu(E) = \mu(E \cap F) + \mu(E \cap F^c)
\]

For $E,F \in \Sigma$ such that $F \subset E$:
\[
  \mu(F) \leq \mu(E)
\]

Countable Additivity (or $\sigma$-additivity): %FIXME

For $E_i \in \Sigma$ (Countable Collections):
\[
  \mu(\bigcup_i E_i) \leq \sum_i \mu(E_i)
\]



\subsubsection{$\sigma$-finite Measure}\label{sec:sigma_finite}

$(X,\Sigma,\mu)$ is \emph{$\sigma$-finite} if there exists a Sequence
$(E_i)_{i \geq 1}$ in $M$ such that $\bigcup_{i} E_i = X$ and
$\mu(E_i) \leq \infty$.



\paragraph{Counting Measure}\label{sec:counting_measure}\hfill

$\mu(A) = |A|$ for $A \subset \nats$



\paragraph{Lebesgue Measure}\label{sec:lebesgue_measure}\hfill

Measure of Subsets of $\reals$

$m((a,b)) = b - a$



\subsubsection{Radon Measure}\label{sec:radon_measure}



\subsubsection{Probability Measure}\label{sec:probability_measure}

A Measure Space with a Probability Measure is a Probability Space
(\S\ref{sec:probability_space})

Probability Measure Function
(\S\ref{sec:probability_measure_function})



% --------------------------------------------------------------------
\subsection{Measurable Function}\label{sec:measurable_function}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Non-measurable Set}\label{sec:nonmeasurable_set}
% --------------------------------------------------------------------



% ====================================================================
\section{Population}\label{sec:population}
% ====================================================================

Totality of Observations

Observation: Value of a Random Variable $X$ having some Probability
Distribution $f(x)$

Paired Observation (Dependent) (???)



% --------------------------------------------------------------------
\subsection{Population Parameter}\label{sec:population_parameter}
% --------------------------------------------------------------------

$\mu$, $\sigma$



% --------------------------------------------------------------------
\subsection{Sample}\label{sec:sample}
% --------------------------------------------------------------------

Subset of a Population



\subsubsection{Random Sample}\label{sec:random_sample}

\paragraph{Simple Random Sample}\label{sec:simple_random_sample}\hfill

\paragraph{Stratified Random Sample}\label{sec:stratified_random_sample}\hfill



% --------------------------------------------------------------------
\subsection{Statistic}\label{sec:statistic}
% --------------------------------------------------------------------

Function of the Random Variable (\S\ref{sec:random_variable})
constituting a Random Sample (\S\ref{sec:random_sample})

$\overline{x}$, $\sigma^2$

The Probability Distribution (\S\ref{sec:probability_distribution}) of
a Statistic is a Sampling Distribution
(\S\ref{sec:sampling_distribution})

Estimator (\S\ref{sec:estimator})



% ====================================================================
\section{Probability Space}\label{sec:probability_space}
% ====================================================================

$(\Omega, \Sigma, P)$

A \emph{Probability Space} is a Measure Space
(\S\ref{sec:measure_space}) with a Probability Measure
(\S\ref{sec:probability_measure}).

\fist Measure-preserving Dynamical Systems
(\S\ref{sec:measure_preserving_system})



% --------------------------------------------------------------------
\subsection{Sample Space}\label{sec:sample_space}
% --------------------------------------------------------------------

$S$

Set of all possible outcomes of Statistical Experiment

Random Variable (\S\ref{sec:random_variable}): Function on a Sample
Space



\subsubsection{Experiment}\label{sec:experiment}

\paragraph{Bernoulli Trial}\label{sec:bernoulli_trial}\hfill

Binomial Distribution (\S\ref{sec:binomial_distribution})

Bernoulli Distribution (\S\ref{sec:bernoulli_distribution})

\begin{enumerate}
  \item repeated Trials
  \item each Trial results in an Outcome
  \item Probability of Success is Constant
  \item each Trial is Independent
\end{enumerate}

Number of Successes in $n$ Bernoulli Trials is a \emph{Binomial Random
  Variable} (\S\ref{sec:binomial_random_variable})



% --------------------------------------------------------------------
\subsection{Event}\label{sec:probability_event}
% --------------------------------------------------------------------

Subset of a Sample Space (\S\ref{sec:sample_space})



\subsubsection{Independent Event}\label{sec:independent_event}

$P(B|A) = P(B)$ or $P(A|B) = P(A)$

Independent if and only if $P(A \cap B) = P(A) P(B)$



% --------------------------------------------------------------------
\subsection{Random Variable}\label{sec:random_variable}
% --------------------------------------------------------------------

Function on a Sample Space (\S\ref{sec:sample_space})

Variable (\S\ref{sec:variable})

$X : \Omega \rightarrow E$

Discrete Random Variable (\S\ref{sec:discrete_random_variable})

Continuous Random Variable (\S\ref{sec:continuous_random_variable})



\subsubsection{Characteristic}\label{sec:characteristic}

Function associating a Real Number to each Element in the Sample
Space.



\subsubsection{Discrete Random Variable}
\label{sec:discrete_random_variable}

\subsubsection{Continuous Random Variable}
\label{sec:continuous_random_variable}

has Probability $0$ of assuming a particular Value



\subsubsection{Expected Value}\label{sec:expected_value}

For Random Variable $X$ with Probability Distribution $f(x)$, the
\emph{Expected Value} (or \emph{Mean}) of $X$ is (Discrete):
\[
  \mu = E[X] = \sum_{i=1}^\infty x_i f(x_i)
\]
(Continuous):
\[
  \mu = E[X] = \int\limits_{-\infty}^{\infty} x_i f(x_i) dx
\]

Law of Large Numbers (\S\ref{sec:large_numbers})

For Random Variable $X$ with Probability Distribution $f(x)$, the
Expected Value of a Measurable Function
(\S\ref{sec:measurable_function}) of $X$, $g(X)$, is:
\[
  \mu_{g(X)} = E[g(X)] = \int\limits_{-\infty}^{\infty} g(x) f(x) dx
\]

For Joint Probability Density Function:
\[
  E[X Y] = \int\int x y j(x,y) dx dy
\]
\fist Note that $E[X Y]$ is not necessarily equal to $E[X]
E[Y]$, see Covariance (\S\ref{sec:covariance}).

For $X$ and $Y$ Independent (\S\ref{sec:independence}), then $E[X,Y] =
E[X] E[Y]$

If $a$ and $b$ are Constants, then $E[aX +b] = a E[X] + b$

$E [g(X) \pm h(X)] = E[g(X)] \pm E[h(X)]$

$E [g(X,Y) \pm h(X,Y)] = E[g(X,Y)] \pm E[h(X,Y)]$



\subsubsection{Variance}\label{sec:variance}

Discrete Random Variable $X$:
\[
  Var(X) = \sigma_X^2 = \sum_{i=1}^n f(x_i) (x_i - \mu)^2 = \sum_{i=1}^n
  f(x_i) x_i^2 - \mu^2
\]
where $\mu = \sum_{i=1}^n f(x_i) x_i$

Continuous Random Variable $X$:
\[
  Var(X) = \sigma_X^2 = \int (x - \mu)^2 f(x) dx = \int x^2 f(x) dx -
  \mu^2
\]
where $\mu = \int x f(x) dx$

For Random Variables $X$, $Y$ with Joint Probability Distribution
$f(x,y)$ and $a$, $b$, $c$ are Constants, then:
\[
  \sigma^2_{a X + b Y + c} = a^2 \sigma^2_X + b^2 \sigma^2_Y + 2ab
  \sigma_{X Y}
\]



\subsubsection{Covariance}\label{sec:covariance}

$X$, $Y$, Joint Probability Distribution
(\S\ref{sec:joint_probability}) $f(x,y)$

Discrete:
\[
  \sigma_{xy} = E [(x - \mu_X)(y - \mu_Y)] = \sum_x \sum_y (x - \mu_X)
  (y - \mu_Y) f(x,y)
\]

Continuous:
\[
  \sigma_{xy} = E [(x - \mu_X)(y - \mu_Y)] =
  \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}
  (x - \mu_X) (y - \mu_Y) f(x,y) dx dy
\]



\subsubsection{Correlation Coefficient}
\label{sec:correlation_coefficient}

\emph{Pearson Product-moment Correlation Coefficient}

$\rho_{xy} = \frac{\sigma_{xy}}{\sigma_x \sigma_y}$



\subsubsection{Skewness}\label{sec:skewness}

Asymmetry %FiXME



\subsubsection{Kurtosis}\label{sec:kurtosis}

\subsubsection{Binomial Random Variable}
\label{sec:binomial_random_variable}

Bernoulli Trial (\S\ref{sec:bernoulli_trial})

Binomial Distribution (\S\ref{sec:binomial_distribution})

$X \sim B(n,p)$

Probability Mass Function:
\[
  f(k,n,p) = P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
\]
for $k = 0,1,2, \ldots, n$



% --------------------------------------------------------------------
\subsection{Probability}\label{sec:probability}
% --------------------------------------------------------------------

Probability of an Event $A$, $P(A)$ is the Sum of Weights of all
Sample Points in $A$

$\frac{|A|}{|S|}$

$P(A \cup B) = P(A) + P(B) - P(A \cap B)$

$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) -
P(B \cap C) + P(A \cap B \cap C)$

Corollary: for Disjoint $A_1, A_2, \ldots$:
\[
  P(A_1 \cup A_2 \cup \ldots) = P(A_1) + P(A_2) + \ldots
\]

$P(A_1 \cap A_2 \cap \ldots \cap A_k) = P(A_1) P(A_2 | A_1) P(A_3 |
A_1 \cap A_2) \ldots P(A_k | A_1 \cap A_2 \cap \ldots \cap A_{k-1})$



\subsubsection{Conditional Probability}
\label{sec:conditional_probability}

$P(B|A) = \frac{P(A \cap B)}{P(A)}$ when $P(A) > 0$

$P(A \cap B) = P(A) P(B|A)$ when $P(A) > 0$



\subsubsection{Law of Total Probability}\label{sec:total_probability}

For a Countably Inifinite Partition of a Sample Space
(\S\ref{sec:sample_space}), $\{ B_n : n = 1,2,3,\ldots \}$, with each
Event $B_n$ being Measurable (\S\ref{sec:measure}), then for any Event
$A$ in the same Probability Space (\S\ref{sec:probability_space}):
\[
  P(A) = \sum_n P(A \cap B_n)
\]
or equivalently:
\[
  P(A) = \sum_n P(A|B_n) P(B_n)
\]
where terms such that $P(B_n) = 0$ are omitted from the summation.



\subsubsection{Baye's Theorem}\label{sec:bayes_theorem}

\[
  P(A|B) = \frac{P(A) P(B|A)}{P(B)}
\]


\subsubsection{Law of Large Numbers}\label{sec:large_numbers}

\subsubsection{Central Limit Theorem}\label{sec:central_limit}

$z = \frac{\overline{x} - \mu}{\sfrac{\sigma}{\sqrt{n}}}$ as $n
\rightarrow \infty$ is $n(z; 0,1)$ %FIXME



\subsubsection{Probability Measure Function}
\label{sec:probability_measure_function}



% --------------------------------------------------------------------
\subsection{Probability Distribution}
\label{sec:probability_distribution}
% --------------------------------------------------------------------

Probability Distribution Functions:
\begin{itemize}
  \item Probability Mass Function (\S\ref{sec:probability_mass})
  \item Probability Density Function (\S\ref{sec:probability_density})
  \item Cumulative Distribution Function
    (\S\ref{sec:cumulative_distribution})
\end{itemize}
See also Probability Measure Function
(\S\ref{sec:probability_measure_function}), Distribution (Analysis
\S\ref{sec:distribution})

the Entropy (\S\ref{sec:entropy}) of a Distribution is the Mean number
of Bits-per-symbols in an Optimal Encoding (\S\ref{sec:encoding}) --
\url{https://golem.ph.utexas.edu/category/2017/02/functional_equations_iii_expla.html}



\subsubsection{Discrete Probability Distribution}
\label{sec:discrete_probability}

\paragraph{Geometric Distribution}\label{sec:geometric_distribution}
\hfill

\paragraph{Hypergeometric Distribution}\hfill
\label{sec:hypergeometric_distribution}

$h(x; N, n, k) = \frac{\binom{k}{x} \binom{N-k}{n-x}}{\binom{N}{n}}$

Mean $\mu = \frac{nk}{N}$

Variance $\sigma^2 = \frac{N-n}{N-1} n \frac{k}{N}(1 - \frac{k}{N})$



\subparagraph{Multivariate Hypergeometric Distribution}\hfill
\label{sec:multivariate_hypergeometric}



\paragraph{Parabolic Fractal Distribution}
\label{sec:parabolic_fractal_distribution}\hfill



\paragraph{Discrete Power Law Distribution}
\label{sec:discrete_power_law_distribution}\hfill

\fist Continuous Power Law Distributions
(\S\ref{sec:continuous_power_law_distribution})



\subparagraph{Zipf Distribution}\label{sec:zipf_distribution}\hfill

\subparagraph{Zeta Distribution}\label{sec:zeta_distribution}\hfill

Normalization of the Zipf Distribution

\subparagraph{Yule-Simon Distribution}
\label{sec:yule_simon_distribution}\hfill



\paragraph{Binomial Distribution}\label{sec:binomial_distribution}\hfill

Random Variable $X$ with Binomial Distribution where $n \in \nats$ and
$p \in [0,1]$:
\[
  X \sim B(n,p)
\]

Binomial Random Variable (\S\ref{sec:binomial_random_variable})

Bernoulli Trial (\S\ref{sec:bernoulli_trial})

Mean $\mu = n p$

Variance $\sigma^2 = n p q$

Sample Proportion %FIXME



\subparagraph{Negative Binomial Distribution}\hfill
\label{sec:negative_binomial}

$b^*(x; k,p) = \binom{x-1}{k-1} p^k 2^{k-k}$



\subparagraph{Bernoulli Distribution}\label{sec:bernoulli_distribution}\hfill

$n = 1$



\subparagraph{Normal Approximation}\label{sec:normal_approximation}\hfill

For Binomial Random Variable $X$ with Mean $\mu = np$ and Variance
$\sigma^2 = npq$, then:
\[
  Z = \frac{X - np}{\sqrt{npq}}
\]
as $n \rightarrow \infty$ is the Standard Normal Distribution
(\S\ref{sec:normal_distribution}) $n(Z;0,1)$



\paragraph{Multinomial Distribution}\label{sec:multinomial_distribution}\hfill

$k$ Outcomes $E_1, E_2, \ldots, E_k$

Probabilities $p_1, p_2, \ldots, p_k$

Probability Distribution of $x_1, x_2, \ldots, x_k$ number of
occurences for $E_1, E_2, \ldots, E_k$ in $n$ Independent Trials:
\[
  f(x_1, x_2, \ldots, x_k) = \binom{n}{x_1, x_2, \ldots, x_k} =
  p_1^{x_1} p_2^{x_2} \cdots p_k^{x_k}
\]
and $\sum_{i=1}^k x_i = n$ and $\sum_{i=1}^k {p_i} = 1$



\paragraph{Poisson Distribution}\label{sec:poisson_distribution}\hfill

Poisson Process (\S\ref{sec:poisson_process})

$P(x; \lambda t) = \frac{e^{-\lambda t} (\lambda t)^x}{x!}$
where $\lambda$ is the average number of outcomes per unit time



\subsubsection{Continuous Probability Distribution}
\label{sec:continuous_probability}

\fist Discrete Power Law Distributions
(\S\ref{sec:discrete_power_law_distribution})



\paragraph{Normal Distribution}\label{sec:normal_distribution}\hfill

(or \emph{Gaussian Distribution})

\[
  n (x; \mu, \sigma) =
  \frac{1}{\sqrt{2\pi \sigma}} e^{-\frac{1}{2 \sigma^2}(x - \mu)^2}
\]

2018 - Eric Jang
- \emph{Normalizing Flows Tutorial}
- \url{https://blog.evjang.com/2018/01/nf1.html}



\subparagraph{Standard Normal Distribution}\label{sec:standard_normal}\hfill

Mean $\mu = 0$

Variance $\sigma^2 = 1$



\paragraph{Log-normal Distribution}\label{sec:lognormal_distribution}\hfill

\paragraph{Gamma Distribution}\label{sec:gamma_distribution}\hfill

Gamma Function (\S\ref{sec:gamma_function})

Continuous Random Variable $X$ with parameters $\alpha > 0$ and $\beta
> 0$:
\[
  f(x; \alpha, \beta) =
  \begin{cases}
  \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{\sfrac{-x}{\beta}}     & \quad x > 0 \\
  0     & \quad\text{else} \\
  \end{cases}
\]

Mean $\mu = \alpha \beta$

Variance $\sigma^2 = \alpha \beta^2$



\paragraph{Continuous Power Law Distribution}
\label{sec:continuous_power_law_distribution}\hfill

\subparagraph{Pareto Distribution}\label{sec:pareto_distribution}\hfill

prototypical Power Law Distribution



\subparagraph{Exponential Distribution}\label{sec:exponential_squared}\hfill

Continuous Random Variable $X$ with parameter $\beta > 0$:
\[
  f(x; \beta) =
  \begin{cases}
  \frac{1}{\beta} e^{\sfrac{-x}{\beta}}     & \quad x > 0 \\
  0     & \quad\text{else} \\
  \end{cases}
\]



\subparagraph{$\chi^2$ Distribution}\label{sec:chi_squared}\hfill

Non-symmetric

\[
  f(x; v) =
  \begin{cases}
  \frac{1}{2^{\sfrac{v}{2}}\Gamma(\sfrac{v}{2})} x^{\sfrac{v}{2-1}} e^{\sfrac{-x}{2}}     & \quad x > 0 \\
  0     & \quad\text{else} \\
  \end{cases}
\]



\paragraph{$t$-distribution}\label{sec:t_distribution}\hfill

(or \emph{Student's $t$-distribution})



\subsubsection{Symmetric Probability Distribution}
\label{sec:symmetric_probability}

\paragraph{Uniform Distribution}\label{sec:uniform_distribution}\hfill



\subsubsection{Joint Probability Distribution}
\label{sec:joint_probability}

$f(x,y,\ldots)$ for two or more Random Variables $X,Y,\ldots$

Discrete Random Variables:
\begin{enumerate}
  \item $f(x,y) \geq 0$
  \item $\sum_x \sum_y f(x,y) = 1$
  \item $P(X = x, Y = y) = f(x,y)$
\end{enumerate}

Continuous Random Variables:
\begin{enumerate}
  \item $\forall (x,y) \in X \times Y, f(x,y) \geq 0$
  \item $\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}
    f(x,y) dx dy = 1$
  \item $P[(X,Y) \in B] = \iint\limits_B f(x,y) dA$
\end{enumerate}

Conditional Independence (\S\ref{sec:conditional_independence}) happens when
the Joint Probability Distribution is the Product of the individual Probability
Distributions
--\url{http://lesswrong.com/lw/pi/classical_configuration_spaces/}



\paragraph{Kalman Filter}\label{sec:kalman_filter}\hfill

a series of papers on ``Kalman Folding'':
\url{http://vixra.org/author/brian_beckman}



\subsubsection{Marginal Distribution}\label{sec:marginal_distribution}

\subsubsection{Conditional Distribution}
\label{sec:conditional_distribution}

\subsubsection{Asymptotic Distribution}
\label{sec:asymptotic_distribution}

\subsubsection{Sampling Distribution}\label{sec:sampling_distribution}

Probability Distribution of a Statistic (\S\ref{sec:statistic})

Statistical Inference (\S\ref{sec:statistical_inference})

Standard Error (\S\ref{sec:standard_error})



\subsubsection{Probability Mass Function}\label{sec:probability_mass}

(or \emph{Probability Function})

for a Discrete Random Variable (\S\ref{sec:discrete_random_variable})
$X$:
\begin{enumerate}
  \item $f(x) \geq 0$
  \item $\sum_x f(x) = 1$
  \item $P(X = x) = f(x)$
\end{enumerate}



\subsubsection{Probability Density Function}
\label{sec:probability_density}

\emph{Probability Density Function} $f(x)$

for a Continuous Random Variable
(\S\ref{sec:continuous_random_variable}) $X$:
\begin{enumerate}
  \item $\forall x \in \reals, f(x) \geq 0$
  \item $\int\limits_{-\infty}^{\infty} f(x) dx = 1$
  \item $P (a < X < b) = \int\limits_a^b f(x) dx$
\end{enumerate}



\subsubsection{Cumulative Distribution Function}
\label{sec:cumulative_distribution}

Area under Probability Density Function

$\forall x \in \reals, F_X(x) = P(X \leq x) = \int\limits_{-\infty}^x
f(t) dt$



% --------------------------------------------------------------------
\subsection{Normalizing Constant}\label{sec:normalizing_constant}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Independence}\label{sec:independence}
% --------------------------------------------------------------------

\subsubsection{Conditional Independence}\label{sec:conditional_independence}

\url{http://lesswrong.com/lw/pi/classical_configuration_spaces/}
-- ``Conditional Independence happens when the Joint Probability Distribution
is the Product of the individual Probability Distributions''



% ====================================================================
\section{Inferential Statistics}\label{sec:inferential_statistics}
% ====================================================================

Probabilistic Inference
(\S\ref{sec:probabilistic_inference}),Inductive Inference
(\S\ref{sec:inductive_inference})

Random Variation: Sampling Variation, Observational Error



% --------------------------------------------------------------------
\subsection{Statistical Inference}\label{sec:statistical_inference}
% --------------------------------------------------------------------

\subsubsection{Predictive Inference}\label{sec:predictive_inference}

\paragraph{Prediction Interval}\label{sec:prediction_interval}\hfill

Frequentist: Confidence Interval (\S\ref{sec:confidence_interval})

Bayesian: Credible Interval (\S\ref{sec:credible_interval})



\subsubsection{Proportion}\label{sec:statistical_proportion}

\paragraph{Lexis Ratio}\label{sec:lexis_ratio}\hfill



\subsubsection{Hypothesis Testing}\label{sec:hypothesis_testing}

\emph{Statistical Hypothesis}: Assertion or Conjecture concerning one
or more Populations (\S\ref{sec:population})

Methodology

Confidence Intervals (\S\ref{sec:confidence_interval})

Null Hypothesis $H_0$ represents any Hypothesis

If $H_0$ is Rejected then Alternate Hypothesis $H_1$ is Accepted

$H_1$ usually represents the question to be answered

\begin{enumerate}
  \item Sufficient Evidence: Reject $H_0$ in favor of $H_1$
  \item Insufficient Evidence: fail to Reject $H_0$
\end{enumerate}

\emph{Test Statistic}

\emph{Critical Region}, \emph{Critical Value}

Type I Error: Rejection of $H_0$ when it is True

Type II Error: Non-Rejection of $H_0$ when it is False

\emph{Level of Significance} $\alpha$: Probability of committing a
Type I Error

$\beta$: Probability of committing a Type II Error

\emph{Power} $1 - \beta$: Probability of Rejecting $H_0$ given that a
specific alternative is True

Confidence Intervals (\S\ref{sec:confidence_interval})

One-tailed Test

Two-tailed Test

Test on a single Mean

Test on a single Sample

$P$-value: lowest Level of Significance at which the observed Value of
the Statistic is Significant



% --------------------------------------------------------------------
\subsection{Statistical Model}\label{sec:statistical_model}
% --------------------------------------------------------------------

Parameters

Data

Estimate: Data $\rightarrow$ Parameters



\subsubsection{Variational Bayesian Method}
\label{sec:variational_bayesian_method}

Free Energy Principle -- implicit Minimization of Variational Free Energy;
Active Inference (Friston)



% --------------------------------------------------------------------
\subsection{Estimation Theory}\label{sec:estimation_theory}
% --------------------------------------------------------------------

\subsubsection{Estimator}\label{sec:estimator}

\paragraph{Point Estimator}\label{sec:point_estimator}\hfill

\paragraph{Interval Estimator}\label{sec:interval_estimator}\hfill



\subsubsection{Bias}\label{sec:bias}

\paragraph{Unbiased Estimate}\label{sec:unbiased_estimate}\hfill

\paragraph{Efficient Estimate}\label{sec:efficient_estimate}\hfill

Unbiased Estimator with Smallest Variance



\subsubsection{Standard Error}\label{sec:standard_error}

Sampling Distribution (\S\ref{sec:sampling_distribution})



\subsubsection{Pooled Estimate}\label{sec:pooled_estimate}

%FIXME



% --------------------------------------------------------------------
\subsection{Regression Analysis}\label{sec:regression_analysis}
% --------------------------------------------------------------------

Regression Variable

Response Variable


Linear Regression, Ordinary Least Squares, Logistic Regression



% --------------------------------------------------------------------
\subsection{Frequentist Inference}\label{sec:frequentist_inference}
% --------------------------------------------------------------------

\subsubsection{Confidence Interval}\label{sec:confidence_interval}

Confidence Coefficient

Confidence Limit

Hypothesis Testing (\S\ref{sec:hypothesis_testing})

One-tail

Two-tail



% --------------------------------------------------------------------
\subsection{Bayesian Inference}\label{sec:bayesian_inference}
% --------------------------------------------------------------------

\subsubsection{Credible Interval}\label{sec:credible_interval}

\subsubsection{Linear Quadratic Estimation (LQE)}\label{sec:lqe}

dual of Linear Quadratic Regulation (LQR \S\ref{sec:lqr})



% --------------------------------------------------------------------
\subsection{Fiducial Inference}\label{sec:fiducial_inference}
% --------------------------------------------------------------------



% ====================================================================
\section{Descriptive Statistics}\label{sec:descriptive_statistics}
% ====================================================================

Summary of Data: Mean, Median, Mode, Standard Deviation



% --------------------------------------------------------------------
\subsection{Statistical Sample}\label{sec:statistical_sample}
% --------------------------------------------------------------------

Random Sample (\S\ref{sec:random_sample})



\subsubsection{Order Statistic}\label{sec:order_statistic}



% --------------------------------------------------------------------
\subsection{Summary Statistics}\label{sec:summary_statistics}
% --------------------------------------------------------------------

\subsubsection{Measure of Location}\label{sec:location_measure}

\paragraph{Arithmetic Mean}\label{sec:arithmetic_mean}\hfill

\emph{Arithmetic Mean} $\overline{x} = \frac{1}{n}\sum_{i=1}^n x_i$



\paragraph{Trimmed Mean}\label{sec:trimmed_mean}\hfill

\paragraph{Sample Median}\label{sec:median}\hfill



\subsubsection{Statistical Dispersion}\label{sec:statistical_dispersion}

\paragraph{Sample Variance}\label{sec:variability}\hfill

$s^2$

Degrees of Freedom, Linear Independence, Biased/Unbiased Estimator
(\S\ref{sec:unbiased_estimate})

Unbiased Sample Variance:
\[
  s^2 = \frac{n}{n-1}\sigma^2_y =
  \frac{1}{n-1} \sum_{i=1}^n (y_i - \overline{y})^2
\]



\paragraph{Standard Deviation}\label{sec:standard_deviation}\hfill

Variance (\S\ref{sec:variance})

\emph{Uncorrected Standard Deviation}:
\[
  s_n = \sqrt{\frac{1}{n}\sum_{i=1}^n (x_i - \overline{x})^2}
\]

\emph{Corrected Standard Deviation}

\emph{Unbiased Standard Deviation}



\subparagraph{Chebyshev's Inequality}\label{sec:chebyshevs_inequality}
\hfill

Probability that a Random Variable $X$ will assume Value within $k$
Standdard Deviations

Random Variable $X$ with Finite Expected Value
(\S\ref{sec:expected_value}) $\mu$ and Finite Non-zero Variance
$\sigma^2$, for any $k \in \reals : k > 0$:
\[
  P(k\sigma \leq |X - \mu|) \leq \frac{1}{k^2}
\]



% --------------------------------------------------------------------
\subsection{Earthmover Distance}\label{sec:earthmover_distance}
% --------------------------------------------------------------------

%FIXME: does this section belong here?

a measure of ``nearness'' for Probability Distributions

\url{https://jeremykun.com/2018/03/05/earthmover-distance/}



% ====================================================================
\section{Large Deviations Theory}\label{sec:large_deviations_theory}
% ====================================================================

% ====================================================================
\section{Stochastic Process}\label{sec:stochastic_process}
% ====================================================================

Harmonic Functions (\S\ref{sec:harmonic_function})

\fist cf. Stochastic Optimization (\S\ref{sec:stochastic_optimization})



% --------------------------------------------------------------------
\subsection{Discrete-time Stochastic Process}
\label{sec:discretetime_stochastic}
% --------------------------------------------------------------------

\subsubsection{Bernoulli Process}\label{sec:bernoulli_process}

Bernoulli Trial (\S\ref{sec:bernoulli_trial})

Stochastic Computing



% --------------------------------------------------------------------
\subsection{Point Process}\label{sec:point_process}
% --------------------------------------------------------------------

\subsubsection{Poisson Process}\label{sec:poisson_process}

Memory-less

Poisson Distribution (\S\ref{sec:poisson_distribution})



% --------------------------------------------------------------------
\subsection{Markov Process}\label{sec:markov_process}
% --------------------------------------------------------------------

(or \emph{Markov Chain})

Markov Property (``Memorylessness'')

can be seen as a special case of Petri Nets (\S\ref{sec:petri_net})
where every Transition has a single Input and a single Output



% ====================================================================
\section{Multivariate Statistics}\label{sec:multivariate_statistics}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Multivariate Analysis}\label{sec:multivariate_analysis}
% --------------------------------------------------------------------

\subsubsection{Ordination}\label{sec:ordination}

\paragraph{Principal Components Analysis}
\label{sec:principal_components_analysis}\hfill

\paragraph{Multidimensional Scaling}\label{sec:multidimensional_scaling}\hfill

\paragraph{Correspondence Analysis}\label{sec:correspondence_analysis}\hfill

\subparagraph{Detrended Correspondence Analysis}
\label{sec:detrended_correspondence}\hfill

\subparagraph{Canonical Correspondence Analysis}
\label{sec:canonical_correspondence}\hfill



\paragraph{Bray-Curtis Ordination}\label{sec:bray_curtis_ordination}\hfill

\paragraph{Redundancy Analysis}\label{sec:redundancy_analysis}\hfill



% ====================================================================
\section{Statistical Mechanics}\label{sec:statistical_mechanics}
% ====================================================================

Thermodynamics, ``Irreversibility''

Jarzynski Equality

Crooks' Fluctuation Theorem



% --------------------------------------------------------------------
\subsection{Non-equilibrium Statistical Mechanics}
\label{sec:nonequilibrium_statistical_mechanics}
% --------------------------------------------------------------------

Jarzynski



% ====================================================================
\section{Information Geometry}\label{sec:information_geometry}
% ====================================================================

Harper09 - \emph{The Replicator Equation as an Inference Dynamic}

Harper09 - \emph{Information Geometry and Evolutionary Game Theory}

\url{http://math.ucr.edu/home/baez/information/}

\url{https://johncarlosbaez.wordpress.com/2017/01/31/biology-as-information-dynamics/}

application of Differential Geometry
(\S\ref{sec:differential_geometry}) techniques to Probability Theory



% --------------------------------------------------------------------
\subsection{Statistical Manifold}\label{sec:statistical_manifold}
% --------------------------------------------------------------------

Riemannian Manifold (\S\ref{sec:riemannian_manifold}) with the
\emph{Fisher Information Metric} as the Riemannian Metric
(\S\ref{sec:riemannian_metric})



\subsubsection{Fisher Information Metric}
\label{sec:fisher_information}

Riemannian Metric (\S\ref{sec:riemannian_metric}) for a Statistical
Manifold

\fist \textbf{Fisher's Fundamental Theorem of Natural Selection},
Quasi-linkage Equilibrium: approximation in the case of Weak Selection
and Weak Epistasis -- Evolutionary Optimization
(\S\ref{sec:evolutionary_optimization}) %FIXME



% ====================================================================
\section{Statistical Learning Theory}\label{sec:statistical_learning_theory}
% ====================================================================

%FIXME start new document ???



% --------------------------------------------------------------------
\subsection{Statistical Classification}\label{sec:statistical_classification}
% --------------------------------------------------------------------



% ====================================================================
\section{Computational Learning Theory}\label{sec:computational_learning_theory}
% ====================================================================

%FIXME start new document ???



% --------------------------------------------------------------------
\subsection{Vapnik-Chervonenkis Theory}\label{sec:vc_theory}
% --------------------------------------------------------------------

\emph{VC Theory}
