%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Optimization}\label{part:optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ====================================================================
\section{Root-finding Algorithm}\label{sec:root_finding}
% ====================================================================

%FIXME: move this section to analysis ?

% --------------------------------------------------------------------
\subsection{Bracketing Method}\label{sec:bracketing_method}
% --------------------------------------------------------------------

\subsubsection{Bisection Method}\label{sec:bisection_method}

\subsubsection{False Position Method}\label{sec:false_position}



% --------------------------------------------------------------------
\subsection{Iterative Method}\label{sec:iterative_rootfinding}
% --------------------------------------------------------------------

cf. \emph{Direct Methods}

\fist Iterative Methods (Numerical Linear Algebra \S\ref{sec:iterative_method})

\fist Systems of Nonlinear Equations
(\S\ref{sec:system_of_nonlinear_equations})

\fist cf. Nonlinear Programming (\S\ref{sec:nonlinear_programming}), Nonlinear
Optimization (\S\ref{sec:nonlinear_optimization})



\subsubsection{Newton's Method}\label{sec:newtons_method}

\subsubsection{Secant Method}\label{sec:secant_method}

\subsubsection{Inverse Interpolation Method}\label{sec:inverse_interpolation}



% ====================================================================
\section{Optimization Problem}\label{sec:optimization_problem}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Continuous Optimization Problem}
\label{sec:continuous_optimization_problem}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Combinatorial Optimization Problem}
\label{sec:combinatorial_optimization_problem}
% --------------------------------------------------------------------

\fist Combinatorial Optimization (\S\ref{sec:combinatorial_optimization})



% ====================================================================
\section{Unconstrained Optimization}\label{sec:unconstrained_optimization}
% ====================================================================

(wiki):

for a Constrained Optimization Problem (\S\ref{sec:constrained_optimization})
with only Equality Constraints, the method of Lagrange Multipliers
(\S\ref{sec:lagrange_multiplier}) can be used to convert it into an
Unconstrained Problem with number of Variables equal to the original number
plus the original number of Equality Constraints

if the Constraints are all Equality Constraints and are all \emph{Linear}, they
can be ``solved'' for some of the Variables in terms of the others and have
those substituted \emph{out} of the Objective Function, leaving an
Unconstrained Problem in a smaller number of Variables (TODO: example)



% --------------------------------------------------------------------
\subsection{Unconstrained Nonlinear Optimization}
\label{sec:unconstrained_nonlinear}
% --------------------------------------------------------------------

\fist cf. Nonlinear Optimization (\S\ref{sec:nonlinear_optimization})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})



% ====================================================================
\section{Constrained Optimization}\label{sec:constrained_optimization}
% ====================================================================

(wiki)

general form:

\begin{align*}
  & f(\vec{x})
  & g_i(\vec{x}) = c_i
  & h_j(\vec{x}) \geq d_j
\end{align*}
where $f$ is the Objective Function to be Maximized/Minimized, $g_i$ for $i \in
\{ 1, \ldots, n \}$ are Equality Constraints and $h_j$ for $j \in \{ 1, \ldots,
m \}$ are Inequality Constraints


\textbf{Equality Constraints}

a Constrained Problem with only Equality Constraints may be converted using the
method of Lagrange Multipliers (\S\ref{sec:lagrange_multiplier}) to an
Unconstrained Optimization Problem (\S\ref{sec:unconstrained_optimization})
with a number of Variables equal to the original number of Variables plus the
original number of Equality Constraints

if the Constraints are all Equality Constraints and are all \emph{Linear}, they
can be ``solved'' for some of the Variables in terms of the others and have
those substituted \emph{out} of the Objective Function, leaving an
Unconstrained Problem in a smaller number of Variables (TODO: example)


\textbf{Inequality Constraints}

\emph{Geometric Optimality Conditions} (FIXME: Geometric Programming ???
\S\ref{sec:geometric_programming})

\emph{Fritz John Conditions} -- necessary condition for a solution in Nonlinear
Programming; lemma for Karush-Kuhn-Tucker Conditions

\emph{Karush-Kuhn-Tucker Conditions} (\S\ref{sec:karush_kuhn_tucker}) --
First-order necessary condition for a solution in Nonlinear Programming,
provided some Regularity Conditions; allowing Inequality Constraints
generalizes the method of Lagrange Multipliers



% --------------------------------------------------------------------
\subsection{Objective Function}\label{sec:objective_function}
% --------------------------------------------------------------------

\emph{Cost Function} (or \emph{Loss Function})

\emph{Reward Function}



% --------------------------------------------------------------------
\subsection{Lagrange Multiplier}\label{sec:lagrange_multiplier}
% --------------------------------------------------------------------

(wiki): for a Constrained Optimization Problem with only Equality Constraints,
the method of Lagrange Multipliers can be used to convert into Unconstrained
Problem (\S\ref{sec:unconstrained_optimization}) with number of Variables equal
to the original number of Variables plus the original number of Equality
Constraints (FIXME: details)

\fist Lagrangian (\S\ref{sec:lagrangian})

\fist cf. \emph{Karush-Kuhn-Tucker Conditions} (\S\ref{sec:karush_kuhn_tucker})
-- First-order necessary condition for a solution in Nonlinear Programming,
provided some Regularity Conditions; allowing Inequality Constraints
generalizes the method of Lagrange Multipliers

\fist Convex Optimization (\S\ref{sec:convex_optimization})

$\lambda$

$\nabla f(x_m, y_m) = \lambda\nabla g(x_m, y_m)$

\url{https://www.youtube.com/watch?v=dPMEUi77qZk}:

the Lagrange Multiplier represents the proportion that an increase in the
Constraint causes an increase in the Function being Optimized

(FIXME: clarify)

\[
  \mathcal{L}(x^*,s^*,\lambda^*) = f(x^*,y^*) - \lambda^*(C(x^*,y^*)-c)
\]

\[
  \mathcal{L}^*(x^*(c),s^*(c),\lambda^*(c), c)
    = f(x^*(c),y^*(c)) - \lambda^*(C(x^*(c),y^*(c))-c)
\]

\[
  \frac{d\mathcal{L}^*}{dc} = \frac{\partial{\mathcal{L}}}{\partial{c}} =
  \lambda^*(c)
\]



% --------------------------------------------------------------------
\subsection{Nonlinear Programming}\label{sec:nonlinear_programming}
% --------------------------------------------------------------------

Constrained Nonlinear Optimization: an Optimization Problem defined by a System
of Equalities and Inequalities (Constraints) over a Set of unknown Real
Variables along with an Objective Function, in which some of the Constraints
\emph{or} the Objective Function are \emph{Non-linear}
(\S\ref{sec:system_of_nonlinear_equations})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})

Barrier Methods

Penalty Methods

Sequential Quadratic programming

Successive Linear Programming



\subsubsection{Karush-Kuhn-Tucker Conditions}\label{sec:karush_kuhn_tucker}

(KKT Conditions)

\emph{Karush-Kuhn-Tucker Conditions} -- First-order necessary condition for a
solution in Nonlinear Programming, provided some Regularity Conditions;
allowing Inequality Constraints generalizes the method of Lagrange Multipliers
(\S\ref{sec:lagrange_multiplier})

Fritz John Conditions -- lemma for KKT Conditions



\subsubsection{Sequential Quadratic Programming}
\label{sec:sequential_quadratic_programming}

SQP

\fist cf. Quadratic Programming (\S\ref{sec:quadratic_programming})



% --------------------------------------------------------------------
\subsection{Linear Programming}\label{sec:linear_programming}
% --------------------------------------------------------------------

\fist Convex Optimization (\S\ref{sec:convex_optimization})

Constrained Linear Optimization

Objective Function and all the Hard Constraints are Linear; can be solved by
Simplex Method (\S\ref{sec:simplex_algorithm}) usually in Polynomial Time, or
by Interior Point Method (\S\ref{sec:interior_point}) in Polynomial Time

all Linear Programs can be expressed as Semidefinite Programs
(\S\ref{sec:semidefinite_programming})

Ellipsoid Method (\S\ref{sec:ellipsoid_method}): when specialized for solving
feasible Linear Optimization Problems with Rational Data, the Ellipsoid Method
finds an Optimal Solution in a Finite number of steps


\subsubsection{Linear Inequality}\label{sec:linear_inequality}

\subsubsection{Integer Linear Programming}\label{sec:integer_linear_programming}

ILP

NP-Complete



% --------------------------------------------------------------------
\subsection{Quadratic Programming}\label{sec:quadratic_programming}
% --------------------------------------------------------------------

all Hard Constraints are Linear but Objective Function is Quadratic

for Positive Definite (\S\ref{sec:positive_definite}) $Q$ (i.e. Convex
Objective Function) the problem is solvable by Ellipsoid Method
(\S\ref{sec:ellipsoid_method}) in Polynomial Time; otherwise the problem is NP
Hard

with Convex Objective Function and Linear Constraints \fist Convex Optimization
(\S\ref{sec:convex_optimzation})

\fist cf. Sequential Quadratic Programming (Iterative Method
\S\ref{sec:sequential_quadratic_programming})

2001 - Milenkovic, Schmidl - Optimization-Based Animation



\subsubsection{Quadratically Constrained Quadratic Programming}
\label{sec:quadratically_constrained}

QCQP

both Objective Function and Constraints are Quadratic

with Convex Quadratic Constraints (FIXME: and convex objective function ???)
\fist Convex Optimization (\S\ref{sec:convex_optimization})

if the $P_0,\ldots,P_m$ Matrices are all Positive-definite Matrices
(\S\ref{sec:positive_definite}) then the problem is Convex and can be solved
using Interior Point Methods (\S\ref{sec:interior_point_method}), as in
Semidefinite Programming (\S\ref{sec:semidefinite_programming})



% --------------------------------------------------------------------
\subsection{Geometric Programming}\label{sec:geometric_programming}
% --------------------------------------------------------------------

not Convex in general but may be transformed into Convex Problems by change of
Variables and a transformation of the Objective and Constraint Functions

Inequality Constraints are Posynomials (\S\ref{sec:posynomials})
$f_i(x) \leq 1$

Equality Constraints are Monomials (\S\ref{sec:monomials}) $h_i(x) = 1$



% --------------------------------------------------------------------
\subsection{Relaxation}\label{sec:relaxation}
% --------------------------------------------------------------------

\fist not to be confused with Iterative Methods of Relaxation (Stationary
Iterative Methods \S\ref{sec:stationary_iterative})

Lagrangian Relaxation

Linear Programming Relaxation



% ====================================================================
\section{Non-linear Optimization}\label{sec:nonlinear_optimization}
% ====================================================================

%FIXME: merge this section ???

\emph{Nonlinear Optimization}

Nonlinear Constraint Solving in Programming Languages: ATS (Xi16) uses
explicit Proof Construction to handle Nonlinear Constraints

\fist cf. Nonlinear Programming (\S\ref{sec:nonlinear_programming})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})



% ====================================================================
\section{Convex Optimization}\label{sec:convex_optimization}
% ====================================================================

Convex Analysis (\S\ref{sec:convex_analysis})

Maximum Principle (Harmonic Functions \S\ref{sec:harmonic_function})

may be formulated as Convex Optimization Problems:
\begin{itemize}
  \item Lagrange Multipliers (\S\ref{sec:lagrange_multiplier})
  \item Linear Programming (\S\ref{sec:linear_programming})
  \item Quadratic Programming (\S\ref{sec:quadratic_programming}) with Convex
    Objective Function and Linear Constraints
  \item Quadratic Programming with Convex Quadratic Constraints
    (QCQP \S\ref{sec:quadratically_constrained})
\end{itemize}



% --------------------------------------------------------------------
\subsection{Interior Point Method}\label{sec:interior_point}
% --------------------------------------------------------------------

Linear Programming Problems (\S\ref{sec:linear_programming}) can be solved by
the Interior Point Method (\S\ref{sec:interior_point}) in Polynomial Time

Semidefinite Programming Problems (\S\ref{sec:semidefinite_programming})
can be solved efficiently by Interior Point Methods

Quadratically Constrained Quadratic Programming
(\S\ref{sec:quadratically_constrained}): if the $P_0,\ldots,P_m$ Matrices are
all Positive-definite Matrices (\S\ref{sec:positive_definite}) then the problem
is Convex and can be solved using Interior Point Methods, as in Semidefinite
Programming

any Convex Optimization Problem can be transformed into Minimizing a Linear
Function over a Convex Set by converting to the \emph{Epigraph Form}

\emph{Primal-dual Path-following Interior Point Methods}; Mehrotra's
Predictor-Corrector Algorithm



% --------------------------------------------------------------------
\subsection{Basis-exchange Algorithm}\label{sec:basis_exchange}
% --------------------------------------------------------------------

\subsubsection{Simplex Algorithm}\label{sec:simplex_algorithm}

\emph{Simplex Method}

Dantzig

Linear Programming (\S\ref{sec:linear_programming}) problems can be solved by
Simplex Method (\S\ref{sec:simplex_algorithm}), usually (but not guaranteed) in
Polynomial Time



% --------------------------------------------------------------------
\subsection{Ellipsoid Method}\label{sec:ellipsoid_method}
% --------------------------------------------------------------------

Iterative Method (\S\ref{sec:iterative_method})

when specialized for solving feasible Linear Optimization
(\S\ref{sec:linear_programming}) Problems with Rational Data, the Ellipsoid
Method finds an Optimal Solution in a Finite number of steps

a Quadratic Programming (\S\ref{sec:quadratic_programming}) problem can be
solved in Polynomial Time using the Ellipsoid method if the Objective Function
is Convex



% --------------------------------------------------------------------
\subsection{Cone Optimization}\label{sec:cone_optimization}
% --------------------------------------------------------------------

includes Linear Programming and Semidefinite Programming problems



\subsubsection{Semidefinite Programming}\label{sec:semidefinite_programming}

SDP

can be efficiently solved by Interior Point Methods

all Linear Programs can be expressed as SDPs

hierarchies of SDPs can approximate the solutions of Polynomial Optimization
problems



% ====================================================================
\section{Stochastic Optimization}\label{sec:stochastic_optimization}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Multi-armed Bandit}\label{sec:multiarmed_bandit}
% --------------------------------------------------------------------

\url{http://datagenetics.com/blog/october12017/index.html}

Epsilon-first Strategy

Epsilon-greedy Strategy

Epsilon-decreasing Strategy

Bayesian Bandits



% ====================================================================
\section{Combinatorial Optimization}
\label{sec:combinatorial_optimization}
% ====================================================================

\fist Combinatorial Optimization Problem
(\S\ref{sec:combinatorial_optimization_problem})



% --------------------------------------------------------------------
\subsection{Dynamic Programming}\label{sec:dynamic_programming}
% --------------------------------------------------------------------

or \emph{Dynamic Optimization}



% ====================================================================
\section{Evolutionary Optimization}\label{sec:evolutionary_optimization}
% ====================================================================

\fist Information Geometry (\S\ref{sec:information_geometry}), Fisher
Information Metric (\S\ref{sec:fisher_information})

\fist \textbf{Fisher's Fundamental Theorem of Natural Selection},
Quasi-linkage Equilibrium: approximation in the case of Weak Selection
and Weak Epistasis %FIXME



% --------------------------------------------------------------------
\subsection{Fitness Function}\label{sec:fitness_function}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Fitness Landscape}\label{sec:fitness_landscape}
% --------------------------------------------------------------------

A \emph{Fitness Landscape} (or \emph{Adaptive Landscape}) is an
evaluation of a Fitness Function for all candidate Solutions %FIXME



% ====================================================================
\section{Constraint Satisfaction Problem (CSP)}\label{sec:csp}
% ====================================================================

%FIXME: merge with model theory constraint satisfaction

% --------------------------------------------------------------------
\subsection{Constraint Resolution}\label{sec:constraint_resolution}
% --------------------------------------------------------------------

Constraint Resolution on Finite Domains typically uses a form of \emph{Search}



% --------------------------------------------------------------------
\subsection{Constraint Satisfaction Problem (DCSP)}\label{sec:dcsp}
% --------------------------------------------------------------------



% ====================================================================
\section{Linear Complementarity Problem (LCP)}
\label{sec:linear_complementarity}
% ====================================================================

%FIXME an instance of linear optimization ???

Inequality Constraints and Limits %FIXME

Real Matrix -- $M$

Vector -- $q$

\emph{Linear Complementarity Problem} -- $LCP(M,q)$

find Vectors $z$, $w$ with constraints:

\begin{itemize}
  \item $0 \leq w_i,z_i$ -- each component of $w$ and $z$ is
    non-negative
  \item $z^T w = 0$ (or equivalently) $\sum_i w_i z_i = 0$ --
    \emph{Complementarity Condition}: Imples at most one of each pair
    $\{w_i,z_i\}$ can be Positive
  \item $w = M z + q$
\end{itemize}

Sufficient Condition for Existence and Uniqueness of a solution to LCP
is that $M$ is \emph{Symmetric} (\S\ref{sec:symmetric_matrix})
\emph{Positive-definite} (\S\ref{sec:positive_definite})

\fist Projected Gauss-Seidel (PGS \S\ref{sec:projected_gauss_seidel})


% --------------------------------------------------------------------
\subsection{Mixed Linear Complementarity Problem}\label{sec:mlcp}
% --------------------------------------------------------------------

Generalized LCP to include Free Variables



% ====================================================================
\section{Global Analysis}\label{sec:global_analysis}
% ====================================================================
