%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Optimization}\label{part:optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ====================================================================
\section{Root-finding Algorithm}\label{sec:root_finding}
% ====================================================================

%FIXME: move this section to analysis ?

% --------------------------------------------------------------------
\subsection{Bracketing Method}\label{sec:bracketing_method}
% --------------------------------------------------------------------

\subsubsection{Bisection Method}\label{sec:bisection_method}

\subsubsection{False Position Method}\label{sec:false_position}



% --------------------------------------------------------------------
\subsection{Iterative Method}\label{sec:iterative_rootfinding}
% --------------------------------------------------------------------

cf. \emph{Direct Methods}

\fist Iterative Methods (Numerical Linear Algebra \S\ref{sec:iterative_method})

\fist Systems of Nonlinear Equations
(\S\ref{sec:system_of_nonlinear_equations})

\fist cf. Nonlinear Programming (\S\ref{sec:nonlinear_programming}), Nonlinear
Optimization (\S\ref{sec:nonlinear_optimization})



\subsubsection{Newton's Method}\label{sec:newtons_method}

\subsubsection{Secant Method}\label{sec:secant_method}

\subsubsection{Inverse Interpolation Method}\label{sec:inverse_interpolation}



% ====================================================================
\section{Unconstrained Optimization}\label{sec:unconstrained_optimization}
% ====================================================================

(wiki):

for a Constrained Optimization Problem (\S\ref{sec:constrained_optimization})
with only Equality Constraints, the method of Lagrange Multipliers
(\S\ref{sec:lagrange_multiplier}) can be used to convert it into an
Unconstrained Problem with number of Variables equal to the original number
plus the original number of Equality Constraints

if the Constraints are all Equality Constraints and are all \emph{Linear}, they
can be ``solved'' for some of the Variables in terms of the others and have
those substituted \emph{out} of the Objective Function, leaving an
Unconstrained Problem in a smaller number of Variables (TODO: example)



% --------------------------------------------------------------------
\subsection{Unconstrained Nonlinear Optimization}
\label{sec:unconstrained_nonlinear}
% --------------------------------------------------------------------

\fist cf. Nonlinear Optimization (\S\ref{sec:nonlinear_optimization})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})



% ====================================================================
\section{Constrained Optimization}\label{sec:constrained_optimization}
% ====================================================================

(wiki)

general form:

\begin{align*}
  & f(\vec{x})
  & g_i(\vec{x}) = c_i
  & h_j(\vec{x}) \geq d_j
\end{align*}
where $f$ is the Objective Function to be Maximized/Minimized, $g_i$ for $i \in
\{ 1, \ldots, n \}$ are Equality Constraints and $h_j$ for $j \in \{ 1, \ldots,
m \}$ are Inequality Constraints


\textbf{Equality Constraints}

a Constrained Problem with only Equality Constraints may be converted using the
method of Lagrange Multipliers (\S\ref{sec:lagrange_multiplier}) to an
Unconstrained Optimization Problem (\S\ref{sec:unconstrained_optimization})
with a number of Variables equal to the original number of Variables plus the
original number of Equality Constraints

if the Constraints are all Equality Constraints and are all \emph{Linear}, they
can be ``solved'' for some of the Variables in terms of the others and have
those substituted \emph{out} of the Objective Function, leaving an
Unconstrained Problem in a smaller number of Variables (TODO: example)


\textbf{Inequality Constraints}

\emph{Geometric Optimality Conditions} (FIXME: Geometric Programming ???
\S\ref{sec:geometric_programming})

\emph{Fritz John Conditions} -- necessary condition for a solution in Nonlinear
Programming; lemma for Karush-Kuhn-Tucker Conditions

\emph{Karush-Kuhn-Tucker Conditions} (\S\ref{sec:karush_kuhn_tucker}) --
First-order necessary condition for a solution in Nonlinear Programming,
provided some Regularity Conditions; allowing Inequality Constraints
generalizes the method of Lagrange Multipliers



% --------------------------------------------------------------------
\subsection{Objective Function}\label{sec:objective_function}
% --------------------------------------------------------------------

\emph{Cost Function} (or \emph{Loss Function})

\emph{Reward Function}



% --------------------------------------------------------------------
\subsection{Lagrange Multiplier}\label{sec:lagrange_multiplier}
% --------------------------------------------------------------------

(wiki): for a Constrained Optimization Problem with only Equality Constraints,
the method of Lagrange Multipliers can be used to convert into Unconstrained
Problem (\S\ref{sec:unconstrained_optimization}) with number of Variables equal
to the original number of Variables plus the original number of Equality
Constraints (FIXME: details)

\fist Lagrangian (\S\ref{sec:lagrangian})

\fist cf. \emph{Karush-Kuhn-Tucker Conditions} (\S\ref{sec:karush_kuhn_tucker})
-- First-order necessary condition for a solution in Nonlinear Programming,
provided some Regularity Conditions; allowing Inequality Constraints
generalizes the method of Lagrange Multipliers

\fist Convex Optimization (\S\ref{sec:convex_optimization})

$\lambda$

$\nabla f(x_m, y_m) = \lambda\nabla g(x_m, y_m)$

\url{https://www.youtube.com/watch?v=dPMEUi77qZk}:

the Lagrange Multiplier represents the proportion that an increase in the
Constraint causes an increase in the Function being Optimized

(FIXME: clarify)

\[
  \mathcal{L}(x^*,s^*,\lambda^*) = f(x^*,y^*) - \lambda^*(C(x^*,y^*)-c)
\]

\[
  \mathcal{L}^*(x^*(c),s^*(c),\lambda^*(c), c)
    = f(x^*(c),y^*(c)) - \lambda^*(C(x^*(c),y^*(c))-c)
\]

\[
  \frac{d\mathcal{L}^*}{dc} = \frac{\partial{\mathcal{L}}}{\partial{c}} =
  \lambda^*(c)
\]



% --------------------------------------------------------------------
\subsection{Nonlinear Programming}\label{sec:nonlinear_programming}
% --------------------------------------------------------------------

Constrained Nonlinear Optimization: an Optimization Problem defined by a System
of Equalities and Inequalities (Constraints) over a Set of unknown Real
Variables along with an Objective Function, in which some of the Constraints
\emph{or} the Objective Function are \emph{Non-linear}
(\S\ref{sec:system_of_nonlinear_equations})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})

Barrier Methods

Penalty Methods

Sequential Quadratic programming

Successive Linear Programming



\subsubsection{Karush-Kuhn-Tucker Conditions}\label{sec:karush_kuhn_tucker}

(KKT Conditions)

\emph{Karush-Kuhn-Tucker Conditions} -- First-order necessary condition for a
solution in Nonlinear Programming, provided some Regularity Conditions;
allowing Inequality Constraints generalizes the method of Lagrange Multipliers
(\S\ref{sec:lagrange_multiplier})

Fritz John Conditions -- lemma for KKT Conditions



% --------------------------------------------------------------------
\subsection{Linear Programming}\label{sec:linear_programming}
% --------------------------------------------------------------------

\fist Convex Optimization (\S\ref{sec:convex_optimization})

Constrained Linear Optimization

Objective Function and all the Hard Constraints are Linear; can be solved by
Simplex Method (\S\ref{sec:simplex_algorithm}) usually in Polynomial Time, or
by Interior Point Method (\S\ref{sec:interior_point}) in Polynomial Time



\subsubsection{Linear Inequality}\label{sec:linear_inequality}

Linear Integer Programming: NP-Complete



% --------------------------------------------------------------------
\subsection{Quadratic Programming}\label{sec:quadratic_programming}
% --------------------------------------------------------------------

all Hard Constraints are Linear but Objective Function is Quadratic

solvable by Ellipsoid Method (\S\ref{sec:ellipsoid_method}) in Polynomial Time
if the Objective Function is Convex; otherwise the problem is NP Hard

with Convex Objective Function and Linear Constraints \fist Convex Optimization
(\S\ref{sec:convex_optimzation})



\subsubsection{Quadratically Constrained Quadratic Programming}
\label{sec:quadratically_constrained}

QCQP

both Objective Function and Constraints are Quadratic

with Convex Quadratic Constraints (FIXME: and convex objective function ???)
\fist Convex Optimization (\S\ref{sec:convex_optimization})



% --------------------------------------------------------------------
\subsection{Geometric Programming}\label{sec:geometric_programming}
% --------------------------------------------------------------------

not Convex in general but may be transformed into Convex Problems by change of
Variables and a transformation of the Objective and Constraint Functions

Inequality Constraints are Posynomials (\S\ref{sec:posynomials})
$f_i(x) \leq 1$

Equality Constraints are Monomials (\S\ref{sec:monomials}) $h_i(x) = 1$



% --------------------------------------------------------------------
\subsection{Relaxation}\label{sec:relaxation}
% --------------------------------------------------------------------

\fist not to be confused with Iterative Methods of Relaxation (Stationary
Iterative Methods \S\ref{sec:stationary_iterative})

Lagrangian Relaxation

Linear Programming Relaxation



% ====================================================================
\section{Non-linear Optimization}\label{sec:nonlinear_optimization}
% ====================================================================

%FIXME: merge this section ???

\emph{Nonlinear Optimization}

Nonlinear Constraint Solving in Programming Languages: ATS (Xi16) uses
explicit Proof Construction to handle Nonlinear Constraints

\fist cf. Nonlinear Programming (\S\ref{sec:nonlinear_programming})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})



% ====================================================================
\section{Convex Optimization}\label{sec:convex_optimization}
% ====================================================================

Convex Analysis (\S\ref{sec:convex_analysis})

Maximum Principle (Harmonic Functions \S\ref{sec:harmonic_function})

may be formulated as Convex Optimization Problems:
\begin{itemize}
  \item Lagrange Multipliers (\S\ref{sec:lagrange_multiplier})
  \item Linear Programming (\S\ref{sec:linear_programming})
  \item Quadratic Programming (\S\ref{sec:quadratic_programming}) with Convex
    Objective Function and Linear Constraints
  \item Quadratic Programming with Convex Quadratic Constraints
    (QCQP \S\ref{sec:quadratically_constrained})
\end{itemize}



% --------------------------------------------------------------------
\subsection{Interior Point Method}\label{sec:interior_point}
% --------------------------------------------------------------------

Linear Programming Problems (\S\ref{sec:linear_programming}) can be solved by
the Interior Point Method (\S\ref{sec:interior_point}) in Polynomial Time



% --------------------------------------------------------------------
\subsection{Basis-exchange Algorithm}\label{sec:basis_exchange}
% --------------------------------------------------------------------

\subsubsection{Simplex Algorithm}\label{sec:simplex_algorithm}

\emph{Simplex Method}

Dantzig

Linear Programming Problems (\S\ref{sec:linear_programming}) can be solved by
Simplex Method (\S\ref{sec:simplex_algorithm}) usually, but not guaranteed, in
Polynomial Time



% --------------------------------------------------------------------
\subsection{Ellipsoid Method}\label{sec:ellipsoid_method}
% --------------------------------------------------------------------

a Quadratic Programming Problem (\S\ref{sec:quadratic_programming}) can be
solved in Polynomial Time using the Ellipsoid method if the Objective Function
is Convex \fist Convex Programming (\S\ref{sec:convex_programming})



% ====================================================================
\section{Stochastic Optimization}\label{sec:stochastic_optimization}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Multi-armed Bandit}\label{sec:multiarmed_bandit}
% --------------------------------------------------------------------

\url{http://datagenetics.com/blog/october12017/index.html}

Epsilon-first Strategy

Epsilon-greedy Strategy

Epsilon-decreasing Strategy

Bayesian Bandits



% ====================================================================
\section{Combinatorial Optimization}
\label{sec:combinatorial_optimization}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Dynamic Programming}\label{sec:dynamic_programming}
% --------------------------------------------------------------------

or \emph{Dynamic Optimization}



% ====================================================================
\section{Evolutionary Optimization}\label{sec:evolutionary_optimization}
% ====================================================================

\fist Information Geometry (\S\ref{sec:information_geometry}), Fisher
Information Metric (\S\ref{sec:fisher_information})

\fist \textbf{Fisher's Fundamental Theorem of Natural Selection},
Quasi-linkage Equilibrium: approximation in the case of Weak Selection
and Weak Epistasis %FIXME



% --------------------------------------------------------------------
\subsection{Fitness Function}\label{sec:fitness_function}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Fitness Landscape}\label{sec:fitness_landscape}
% --------------------------------------------------------------------

A \emph{Fitness Landscape} (or \emph{Adaptive Landscape}) is an
evaluation of a Fitness Function for all candidate Solutions %FIXME



% ====================================================================
\section{Constraint Satisfaction Problem (CSP)}\label{sec:csp}
% ====================================================================

%FIXME: merge with model theory constraint satisfaction

% --------------------------------------------------------------------
\subsection{Constraint Resolution}\label{sec:constraint_resolution}
% --------------------------------------------------------------------

Constraint Resolution on Finite Domains typically uses a form of \emph{Search}



% --------------------------------------------------------------------
\subsection{Constraint Satisfaction Problem (DCSP)}\label{sec:dcsp}
% --------------------------------------------------------------------



% ====================================================================
\section{Linear Complementarity Problem (LCP)}
\label{sec:linear_complementarity}
% ====================================================================

%FIXME an instance of linear optimization ???

Inequality Constraints and Limits %FIXME

Real Matrix -- $M$

Vector -- $q$

\emph{Linear Complementarity Problem} -- $LCP(M,q)$

find Vectors $z$, $w$ with constraints:

\begin{itemize}
  \item $0 \leq w_i,z_i$ -- each component of $w$ and $z$ is
    non-negative
  \item $z^T w = 0$ (or equivalently) $\sum_i w_i z_i = 0$ --
    \emph{Complementarity Condition}: Imples at most one of each pair
    $\{w_i,z_i\}$ can be Positive
  \item $w = M z + q$
\end{itemize}

Sufficient Condition for Existence and Uniqueness of a solution to LCP
is that $M$ is \emph{Symmetric} (\S\ref{sec:symmetric_matrix})
\emph{Positive-definite} (\S\ref{sec:positive_definite})

\fist Projected Gauss-Seidel (PGS \S\ref{sec:projected_gauss_seidel})


% --------------------------------------------------------------------
\subsection{Mixed Linear Complementarity Problem}\label{sec:mlcp}
% --------------------------------------------------------------------

Generalized LCP to include Free Variables



% ====================================================================
\section{Global Analysis}\label{sec:global_analysis}
% ====================================================================
