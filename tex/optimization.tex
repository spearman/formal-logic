%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Optimization}\label{part:optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ====================================================================
\section{Optimization Problem}\label{sec:optimization_problem}
% ====================================================================

A Problem involving only Constraints but not an Objective Function
(\S\ref{sec:objective_function}) is called a \emph{Feasibility Problem} \fist
Constraint Satisfaction Problem (CSP \S\ref{sec:constraint_satisfaction})



% --------------------------------------------------------------------
\subsection{Continuous Optimization Problem}
\label{sec:continuous_optimization_problem}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Combinatorial Optimization Problem}
\label{sec:combinatorial_optimization_problem}
% --------------------------------------------------------------------

\fist Combinatorial Optimization (\S\ref{sec:combinatorial_optimization})



% --------------------------------------------------------------------
\subsection{Duality}\label{sec:optimization_duality}
% --------------------------------------------------------------------

(wiki):

Optimization Problems may be viewed as either the \emph{Primal Problem} or the
\emph{Dual Problem}

the Solution to the Dual Problem gives a Lower Bound to the Solution of the
Primal (Minimization) Problem

\emph{Duality Gap} -- difference between the Optimal Values of the Primal and
Dual Problems

\emph{Primal Constraints} -- Constraints of the original Problem

for Convex Optimization Problems (\S\ref{sec:convex_optimization}), the Duality
Gap is Zero under a ``Regularity Condition'' (Constraint Qualification
\S\ref{sec:constraint_qualification})

\begin{itemize}
  \item Lagrangian Dual Problem (\S\ref{sec:lagrangian_dual})
  \item Wolfe Dual Problem
  \item Fenchel Dual Problem
\end{itemize}



% ====================================================================
\section{Unconstrained Optimization}\label{sec:unconstrained_optimization}
% ====================================================================

(wiki):

for a Constrained Optimization Problem (\S\ref{sec:constrained_optimization})
with only Equality Constraints, the method of Lagrange Multipliers
(\S\ref{sec:lagrange_multiplier}) can be used to convert it into an
Unconstrained Problem with number of Variables equal to the original number
plus the original number of Equality Constraints

if the Constraints are all Equality Constraints and are all \emph{Linear}, they
can be ``solved'' for some of the Variables in terms of the others and have
those substituted \emph{out} of the Objective Function, leaving an
Unconstrained Problem in a smaller number of Variables (TODO: example)



% --------------------------------------------------------------------
\subsection{Unconstrained Nonlinear Optimization}
\label{sec:unconstrained_nonlinear}
% --------------------------------------------------------------------

\fist cf. Nonlinear Optimization (\S\ref{sec:nonlinear_optimization})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})



% ====================================================================
\section{Constrained Optimization}\label{sec:constrained_optimization}
% ====================================================================

(wiki)

general form:

\begin{align*}
  & f(\vec{x})
  & g_i(\vec{x}) = c_i
  & h_j(\vec{x}) \geq d_j
\end{align*}
where $f$ is the Objective Function to be Minimized (or Maximized), $g_i$ for
$i \in \{ 1, \ldots, n \}$ are Equality Constraints and $h_j$ for $j \in \{ 1,
\ldots, m \}$ are Inequality Constraints

\emph{note}: a Maximization Problem can always be converted to a Minimization
Problem by multiplying by $-1$; usually Problems are presented as Minimization
Problems

Points $\vec{x}$ Satisfying all Constriants are said to be \emph{Feasible}
and the Set of all Feasible Points is called the \emph{Feasible Region}

a Point $\vec{x}^*$ is \emph{Optimal} if there are no other Feasible Points
with $f(\vec{x}) < f(\vec{x}^*)$


\textbf{Equality Constraints}

a Constrained Problem with only Equality Constraints may be converted using the
method of Lagrange Multipliers (\S\ref{sec:lagrange_multiplier}) to an
Unconstrained Optimization Problem (\S\ref{sec:unconstrained_optimization})
with a number of Variables equal to the original number of Variables plus the
original number of Equality Constraints

if the Constraints are all Equality Constraints and are all \emph{Linear}, they
can be ``solved'' for some of the Variables in terms of the others and have
those substituted \emph{out} of the Objective Function, leaving an
Unconstrained Problem in a smaller number of Variables (TODO: example)


\textbf{Inequality Constraints}

\emph{Geometric Optimality Conditions} (FIXME: Geometric Programming ???
\S\ref{sec:geometric_programming})

\emph{Fritz John Conditions} -- necessary condition for a solution in Nonlinear
Programming; lemma for Karush-Kuhn-Tucker Conditions

\emph{Karush-Kuhn-Tucker Conditions} (\S\ref{sec:karush_kuhn_tucker}) --
First-order necessary condition for a solution in Nonlinear Programming,
provided some Regularity Conditions (\S\ref{sec:constraint_qualification});
allowing Inequality Constraints generalizes the method of Lagrange Multipliers



% --------------------------------------------------------------------
\subsection{Objective Function}\label{sec:objective_function}
% --------------------------------------------------------------------

\emph{Cost Function} (or \emph{Loss Function})

\emph{Reward Function} (or \emph{Utility Function})

\fist cf. Decision Theory (\S\ref{sec:decision_theory}) -- choice of Estimator
(\S\ref{sec:estimator}) or Decision Rule (\S\ref{sec:decision_rule}) based on
minimizing Loss; Risk (Average Loss \S\ref{sec:risk})

\fist Optimality Criterion (\S\ref{sec:optimality_criterion}), Model Selection
(\S\ref{sec:model_selection})

\fist \emph{Bayes Estimator} (\S\ref{sec:bayes_estimator}) -- Estimator or
Decision Rule that minimizes (maximizes) Posterior Expected Value of a Loss
(Utility) Function (\emph{Posterior Expected Loss/Utility})

2018 - \url{https://blog.evjang.com/2018/04/aesthetic-lr.html} - \emph{Aesthetically
  Pleasing Learning Rates}:

``The optimal learning rate is commensurate with the scale of the smoothness of
the gradient of the loss function (or in fancy words, the 'Lipschitz constant'
of a function). The smoother the function, the larger the learning rate we are
allowed to take without the optimization 'blowing up'.''

\fist Lipschitz Continuity (\S\ref{sec:real_continuous})

Auto-correlated Noise seems to be beneficial for State-dependent Exploration

``waving'' Learning Rates up and down are good for Deep Learning

Pixel Values from natural images have both of the above properties



\subsubsection{Local Optimality Condition}\label{sec:local_optimality}

%FIXME: move this section ???

\fist multivariable generalization of Second Derivative Test
(\S\ref{sec:second_derivative_test})

for a Function $f$, Local Minima are identified by \emph{Local Optimality
  Conditions} at a Point $\vec{x}^*$:
\begin{align*}
  \nabla f(\vec{x}^*) & =    \vec{0} \\
  H(f(\vec{x}^*))     & \geq \vec{0} \\
\end{align*}
where $\nabla f(\vec{x}^*)$ is the Gradient (\S\ref{sec:gradient}) of $f$ at
$\vec{x}^*$ and $H(f(\vec{x}^*)) \geq \vec{0}$ the Hessian Matrix
(\S\ref{sec:hessian_matrix}) of Second-order Partial Derivatives is Positive
Definite or Positive Semi-definite

if $f$ is a Convex Function (\S\ref{sec:convex_function}), then $x^*$ is
guaranteed ot be a Global Minimum



% --------------------------------------------------------------------
\subsection{Constraint}\label{sec:constraint}
% --------------------------------------------------------------------

\subsubsection{Hard Constraint}\label{sec:hard_constraint}

\subsubsection{Soft Constraint}\label{sec:soft_constraint}



% --------------------------------------------------------------------
\subsection{Feasible Region}\label{sec:feasible_region}
% --------------------------------------------------------------------

The \emph{Feasible Region} of a Constrained Optimization Problem is the
Solution Set (\S\ref{sec:solution_set}) of the Constraints
(\S\ref{sec:constraint}).



% --------------------------------------------------------------------
\subsection{Penalty Method}\label{sec:penalty_method}
% --------------------------------------------------------------------

add a Term called the \emph{Penalty Function} to the Objective Function
consisting of a \emph{Penalty Parameter} multiplied by a measure of the
Constraint violation

cf. Barrier Methods (Interior Point Methods \S\ref{sec:barrier_method})

(Witkin-Baraff97):

\emph{Penalty Method} -- use of extra ``Energy Terms'' to impose Constraints

a Penalty Method for a Behavior Function is given by an \emph{Energy Function}
that simulates a Spring Force

applied Forces (e.g. Gravity, other Springs, etc.) and restoring (Constraint)
Forces ``communicate'' only indirectly through \emph{displacements}

note that this is a ``sloppy'', approximate Constraint mechanism as the
Constraint Force must compete with all other Forces acting on the object, and
can only ``win'' with a large Spring Constant which results in a Stiff Equation
(\S\ref{sec:stiff_equation}) which is numerically intractible

\emph{Constrained Dynamics} -- make Particles obey Netwon's (force) laws and
at the same time obey \emph{Geometric Constraints}: directly calculate
\emph{Constraint Forces} required to maintain Constraints to cancel out just
those parts of applied Forces that act against the Constraints, that is the
Constraint Forces must convert object Accelerations into ``legal'' Accelerations
that are consistent with the Constraints



% --------------------------------------------------------------------
\subsection{Constraint Qualification}\label{sec:constraint_qualification}
% --------------------------------------------------------------------

or \emph{Regularity Condition}

\fist Karush-Kuhn-Tucker Conditions (\S\ref{sec:karush_kuhn_tucker})

for Convex Optimization Problems (\S\ref{sec:convex_optimization}), the Duality
Gap (\S\ref{sec:optimization_duality}) is Zero under a Constraint Qualification
Condition



% --------------------------------------------------------------------
\subsection{Lagrange Multiplier}\label{sec:lagrange_multiplier}
% --------------------------------------------------------------------

(wiki): for a Constrained Optimization Problem with only Equality Constraints,
the method of Lagrange Multipliers can be used to convert into Unconstrained
Problem (\S\ref{sec:unconstrained_optimization}) with number of Variables equal
to the original number of Variables plus the original number of Equality
Constraints (FIXME: details)

\fist Lagrangian (\S\ref{sec:lagrangian})

\fist cf. \emph{Karush-Kuhn-Tucker Conditions} (\S\ref{sec:karush_kuhn_tucker})
-- First-order necessary condition for a solution in Nonlinear Programming,
provided some Regularity Conditions (\S\ref{sec:constraint_qualification});
allowing Inequality Constraints generalizes the method of Lagrange Multipliers

\fist Convex Optimization (\S\ref{sec:convex_optimization})

cf. Shadow Prices (\S\ref{sec:shadow_price})

$\lambda$

$\nabla f(x_m, y_m) = \lambda\nabla g(x_m, y_m)$

\url{https://www.youtube.com/watch?v=dPMEUi77qZk}:

the Lagrange Multiplier represents the proportion that an increase in the
Constraint causes an increase in the Function being Optimized

(FIXME: clarify)

\[
  \mathcal{L}(x^*,s^*,\lambda^*) = f(x^*,y^*) - \lambda^*(C(x^*,y^*)-c)
\]

\[
  \mathcal{L}^*(x^*(c),s^*(c),\lambda^*(c), c)
    = f(x^*(c),y^*(c)) - \lambda^*(C(x^*(c),y^*(c))-c)
\]

\[
  \frac{d\mathcal{L}^*}{dc} = \frac{\partial{\mathcal{L}}}{\partial{c}} =
  \lambda^*(c)
\]



\subsubsection{Lagrangian Dual Problem}\label{sec:lagrangian_dual}

Dual Problem (\S\ref{sec:optimization_duality})

general Optimization Problem:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $f(\vec{x})$      \\
  subject to               & $g_i(\vec{x}) = 0, i \in \{1,\ldots,m\}$ \\
                           & $h_j(\vec{x}) \leq 0, j \in \{1,\ldots,n\}$
\end{tabular}

with $\vec{x} \in \reals^n$

Lagrangian:
\[
  \mathcal{L}(\vec{x},\vec{\lambda},\vec{\nu}) =
    f(\vec{x}) + \sum_{i=1}^m\lambda_i g_i(x) + \sum_{j=1}^n \nu_j h_j(x)
\]
the Vectors $\lambda, \nu$ are \emph{Dual Variables}

Non-empty Domain $D$

\emph{Lagrange Dual Function}:
\[
  \Lambda(\vec{\lambda}, \vec{nu}) =
    \mathrm{inf}_{x \in D} \mathcal{L}(\vec{x},\vec{\lambda},\vec{\nu})
\]

if the Lagrangian is Unbounded below, then the Dual Function is said to take
the value Negative Infinity $\Lambda(\vec{\lambda}, \vec{\nu}) = -\infty$

the Lagrange Dual gives a Lower Bound on the Optimum of the original
Optimization Problem: if $\vec{\lambda} \geq \vec{0}$ then
$\Lambda(\vec{\lambda}, \vec{\nu}) \leq \vec{p}^*$ where $\vec{p}^*$ is the
Optimal Value of the original Problem

... TODO

the Problem of finding the Greatest Lower Bound on $\vec{p}^*$ from
$\Lambda(\vec{\lambda},\vec{\nu})$ is the \emph{Lagrange Dual Problem}:
\begin{tabular}{r l}
  $\mathrm{max}_{\vec{\lambda},\vec{\nu}}$ &
    $\Lambda(\vec{\lambda},\vec{\nu})$ \\
  subject to & $\vec{\lambda} \geq \vec{0}$ \\
\end{tabular}

$\vec{\lambda} \geq \vec{0}$ and $\Lambda(\vec{\lambda},\vec{\nu}) > -\infty$
  -- $\vec{\lambda}$, $\vec{\nu}$ are called ``Dual Feasible''

$(\vec{\lambda}^*, \vec{\nu}^*)$ -- called ``Dual Optimal'' if Optimizes the
Lagrange Dual Problem


Solution scenarios:
\begin{enumerate}
  \item the Primal is Unbounded: the Dual is Infeasible
  \item the Dual is Unbounded: the Primal is Infeasible
  \item the Primal and Dual may both be Infeasible
  \item both the Primal and the Dual have Solutions
\end{enumerate}
normally both the Primal and Dual have Solutions (4.)


\emph{Strong Duality}: both Primal and Dual have the same Optimal Objective


\emph{Complementary Slackness}: for a Problem with \emph{Strong Duality},
either a Dual Constraint is Binding (``Sharp'') or else the corresponding
Shadow Price (\S\ref{sec:shadow_price}) $x_j^* = 0$ (is Non-basic)


\emph{Examples}

\begin{itemize}
  \item Linear Programming (\S\ref{sec:linear_programming}): Standard Form LP
    and Inequality Form LP are Dual with \emph{Strong Duality} (have the same
    Optimum Objective) -- Standard Form LP:

    \begin{tabular}{r l}
      $\mathrm{max}_{\vec{\lambda},\vec{\nu}}$ &
        $-\vec{b}^T\vec{\nu}$ \\
      subject to & $A^T\vec{\nu} - \vec{\lambda} + \vec{c} = 0$ \\
                 & $\vec{\lambda} \geq \vec{0}$ \\
    \end{tabular}

    or equivalently:

    \begin{tabular}{r l}
      $\mathrm{max}_{\vec{\nu}}$ &
        $-\vec{b}^T\vec{\nu}$ \\
      subject to & $-A^T\vec{\nu} \leq \vec{c}$ \\
    \end{tabular}

    note this is the form of an \emph{Inequality Form LP}

    for Inequality Form LP:

    \begin{tabular}{r l}
      $\mathrm{max}_{\vec{\lambda}}$ & $-\vec{b}^T\vec{\lambda}$ \\
      subject to & $A^T\vec{\lambda} = -\vec{c}$ \\
                & $\vec{\lambda} \geq \vec{0}$  \\
    \end{tabular}

    note this is the form of a \emph{Standard Form LP}

\end{itemize}



% --------------------------------------------------------------------
\subsection{Nonlinear Programming}\label{sec:nonlinear_programming}
% --------------------------------------------------------------------

Constrained Nonlinear Optimization: an Optimization Problem defined by a System
of Equalities and Inequalities (Constraints) over a Set of unknown Real
Variables along with an Objective Function, in which some of the Constraints
\emph{or} the Objective Function are \emph{Non-linear}
(\S\ref{sec:nonlinear_equation_system})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})

Barrier Methods

Penalty Methods

Sequential Quadratic programming

Successive Linear Programming



\subsubsection{Karush-Kuhn-Tucker Conditions}\label{sec:karush_kuhn_tucker}

(KKT Conditions)

\emph{Karush-Kuhn-Tucker Conditions} -- First-order necessary condition for a
solution in Nonlinear Programming, provided some Regularity Conditions
(Constraint Qualification \S\ref{sec:constraint_qualification});
allowing Inequality Constraints generalizes the method of Lagrange Multipliers
(\S\ref{sec:lagrange_multiplier})

Fritz John Conditions -- lemma for KKT Conditions

\fist KKT Method for Equality-constrianed Quadratic Programs
(\S\ref{sec:quadratic_programming})



\subsubsection{Sequential Quadratic Programming}
\label{sec:sequential_quadratic_programming}

Active Set Methods (TODO: xref)

SQP

\fist cf. Quadratic Programming (\S\ref{sec:quadratic_programming})



% --------------------------------------------------------------------
\subsection{Linear Programming}\label{sec:linear_programming}
% --------------------------------------------------------------------

\fist Convex Optimization (\S\ref{sec:convex_optimization})

\fist Linear Regression (\S\ref{sec:linear_regression}) -- article: \emph{Linear
  Programming for Linear Regression};
\url{https://lazyprogrammer.me/linear-programming-for-linear-regression/}

Constrained Linear Optimization

Objective Function and all the Hard Constraints are Linear; can be solved by
Simplex Method (\S\ref{sec:simplex_algorithm}) usually in Polynomial Time

Interior Point (\S\ref{sec:interior_point}) Linear Programming
(Polynomial Time) often performs at least as well or better than the Simplex
Method

all Linear Programs can be expressed as Semidefinite Programs
(\S\ref{sec:semidefinite_programming}), a special case of Cone Optimization
(\S\ref{sec:cone_optimization})

Ellipsoid Method (\S\ref{sec:ellipsoid_method}): when specialized for solving
feasible Linear Optimization Problems with Rational Data, the Ellipsoid Method
finds an Optimal Solution in a Finite number of steps

applications: Chebyshev Approximation ($\infty$-norm
\S\ref{sec:chebyshev_approximation}), $1$-norm Best-fit


UC Math 352 Lec. 9 - \url{https://www.youtube.com/watch?v=g3E38tYcU4E&t=243s}:

all of the Functions are \emph{Affine} (FIXME: ???)

\emph{General Form LP}

Objective Function:
\[
  \mathrm{min}_{\vec{x}} \vec{c}^T\vec{x} + d
\]
since $d$ is a Constant, the Minimization Problem $\vec{c}^T\vec{x}$ has the
same solution without $d$

Subject to:
\begin{align*}
  G\vec{x} & \leq \vec{h} \\
  A\vec{x} & = \vec{b}
\end{align*}

\emph{Inequality Form LP} -- only Inequality Constraints:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $\vec{c}^T\vec{x}$      \\
  subject to               & $A\vec{x} \leq \vec{b}$ \\
\end{tabular}

\emph{Standard Form LP} -- Set of Equality Constraints and Non-negativity
Constraints:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $\vec{c}^T\vec{x}$     \\
  subject to               & $A\vec{x} = \vec{b}$   \\
                           & $\vec{x} \geq \vec{0}$ \\
\end{tabular}

note that Inequality Form LP and Standard Form LP are \emph{Dual}
(\S\ref{sec:lagrangian_dual}) to one another (see below)

General Form LP can be converted to Standard Form by introducing extra
\emph{Slack Variables} $\vec{s}$

General Form:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $\vec{c}^T\vec{x}$     \\
  subject to               & $G\vec{x} \leq \vec{h}$   \\
                           & $A\vec{x} = \vec{b}$ \\
\end{tabular}

an example Inequality $3x \leq 5$ can be split into:
\begin{align*}
  3x + s & = 5 \\
  s      & \geq 0
\end{align*}

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $\vec{c}^T\vec{x}$     \\
  subject to               & $G\vec{x} + \vec{s} = \vec{h}$   \\
                           & $A\vec{x} = \vec{b}$ \\
                           & $\vec{s} \geq 0$ \\
\end{tabular}

express $\vec{x}$ as the Difference of two Non-negative Variables $\vec{x}^+
\geq \vec{0}$ and $\vec{x}^- \geq \vec{0}$:
\[
  \vec{x} = \vec{x}^+ - \vec{x}^-
\]

Linear Program in Standard Form:

\begin{tabular}{r l}
  $\mathrm{min}_{[\vec{x} \ \vec{s}]}$ &
    $\vec{c}^T\vec{x}^+ - \vec{c}^T\vec{x}^-$     \\
  subject to               & $G\vec{x}^+ - G\vec{x}^- + \vec{s} = \vec{h}$   \\
                           & $A\vec{x}^+ - A\vec{x}^- = \vec{b}$ \\
                           & $\vec{x}^+ \geq 0$ \\
                           & $\vec{x}^- \geq 0$ \\
                           & $\vec{s}   \geq 0$ \\
\end{tabular}

or:

\begin{tabular}{r l}
  $\mathrm{min}_{[\vec{x} \ \vec{s}]}$ &
    $[\vec{c}^T \ -\vec{c}^T \ \vec{0}] [\vec{x} \ \vec{s}]$ \\
  subject to               &
    $\begin{bmatrix} G & -G & I \\ A & -A & 0 \\ \end{bmatrix}
      [\vec{x} \ \vec{s}] = [\vec{h} \ \vec{b}]^T$   \\
                           & $[\vec{x} \ \vec{s}] \geq 0$ \\
\end{tabular}


\emph{Properties of LP}

\begin{enumerate}
  \item if Optimum $\vec{x}^*$ exists, then \emph{an} Optimal Point exists at a
    \emph{Vertex}, i.e. the Intersection of $n$ Constraint Boundaries (in
    Configuration Space $\reals^n$); the number of such Vertices is
    $\binom{m}{n}$ in the number of Constraints $m$ and number of Variables $n$
  \item if there is more than one Optimum, then there is a Surface of Optimal
    Points
  \item the Problem may be \emph{Infeasible}, i.e. an Empty Feasible Region
  \item the Problem may be \emph{Unbounded}, i.e. an Infinite Feasible Region
    (Objective Function may be made arbitrarily large)
\end{enumerate}

the number of Vertices $\binom{m}{n}$ is Factorial:
\[
  \frac{m!}{n!(m-n)!}
\]

an efficient algorithm for solving large LP Problems is the Simplex Method
(\S\ref{sec:simplex_algorithm})


UC Math 352 Lec. 11 - \url{https://www.youtube.com/watch?v=ESzYPFkY3og}:


\emph{Fundamental Theorem of Linear Programming}

Assume $A$ is Full Rank; if a Feasible Solution exists, then a Basic Feasible
Solution Exists, and if an Optimal Feasible Solution exists, then an Optimal
Basic Feasible Solution exists.

(a Basic Solution only involves $n$ Linearly Independent Columns of $A$; see
Simplex Method for more details)

the implication is that only Basic Feasible Solutions need to be considered
when searching for an Optimum, i.e. an \emph{Optimal Basis} (Set of Columns)
$B$


\textbf{Lagrangian Dual Problems} (\S\ref{sec:lagrangian_dual})

Standard Form LP and Inequality Form LP are Lagrangian Duals for each other; in
both cases the number of Primal Constraints is equal to the number of Dual
Variables and vice versa

\emph{Standard Form LP}

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $\vec{c}^T\vec{x}$     \\
  subject to               & $A\vec{x} = \vec{b}$   \\
                           & $\vec{x} \geq \vec{0}$ \\
\end{tabular}

Lagrangian Dual:

\begin{align*}
  \mathcal{L}(\vec{x},\vec{\lambda},\vec{\nu}) & =
    \vec{c}^T\vec{x} + \vec{\nu}^T(A\vec{x} - \vec{b}) - \vec{\lambda}^T\vec{x} \\
  \mathcal{L}(\vec{x},\vec{\lambda},\vec{\nu}) & =
    -\vec{b}^T\vec{\nu} + (A^T\vec{\nu} - \vec{\lambda} + \vec{c})^T\vec{x}
\end{align*}

Dual Function:

\begin{align*}
  \Lambda(\vec{\lambda},\vec{\nu}) & = -\vec{b}^T\vec{\nu} +
    \mathrm{inf}_{\vec{x}}(A^T\vec{\nu} - \vec{\lambda} + \vec{c})^T\vec{x} \\
  \Lambda(\vec{\lambda},\vec{\nu}) & = \begin{cases}
    -\vec{b}^T\vec{\nu} & \ A^T\vec{\nu} - \vec{\lambda} + \vec{c} = \vec{0} \\
    -\infty             & otherwise \\
  \end{cases}
\end{align*}

Lagrange Dual Problem for Standard Form LP:

\begin{tabular}{r l}
  $\mathrm{max}_{\vec{\lambda},\vec{\nu}}$ &
    $-\vec{b}^T\vec{\nu}$ \\
  subject to & $A^T\vec{\nu} - \vec{\lambda} + \vec{c} = 0$ \\
             & $\vec{\lambda} \geq \vec{0}$ \\
\end{tabular}

or equivalently:

\begin{tabular}{r l}
  $\mathrm{max}_{\vec{\nu}}$ &
    $-\vec{b}^T\vec{\nu}$ \\
  subject to & $-A^T\vec{\nu} \leq \vec{c}$ \\
\end{tabular}


\emph{Inequality Form LP}

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $\vec{c}^T\vec{x}$      \\
  subject to               & $A\vec{x} \leq \vec{b}$ \\
\end{tabular}

since there are no Equality Constraints, only the $\vec{\lambda}$ Dual Variable
is needed

Lagrangian Dual:

\begin{align*}
  \mathcal{L}(\vec{x}, \vec{\lambda}) & =
    \vec{c}^T\vec{x} + \vec{\lambda}^T(A\vec{x} - \vec{b})
  \mathcal{L}(\vec{x}, \vec{\lambda}) & =
    -\vec{b}^T\vec{\lambda} + (A\vec{\lambda} + \vec{c})^T \vec{x}
\end{align*}

Dual Function:

\begin{align*}
  \Lambda(\vec{\lambda},\vec{\nu}) & = \begin{cases}
    -\vec{b}^T\vec{\lambda} & \ A^T\vec{\lambda} = -\vec{c} \\
    -\infty                 & otherwise \\
  \end{cases}
\end{align*}

Lagrange Dual Problem for Inequality Form LP:

\begin{tabular}{r l}
  $\mathrm{max}_{\vec{\lambda}}$ & $-\vec{b}^T\vec{\lambda}$ \\
  subject to & $A^T\vec{\lambda} = -\vec{c}$ \\
             & $\vec{\lambda} \geq \vec{0}$  \\
\end{tabular}

which is a \emph{Standard Form LP}


\emph{Optimality}

Standard Form LP and Inequality Form LP have the \emph{Strong Duality}
property: for a Primal in Standard Form with Optimal Basis $B$, if $\vec{\nu}$
is also Dual Feasible, then it must Solve the Dual, i.e. \emph{the Primal and
  Dual of LP have the same Optimum Objective}


\emph{Complementary Slackness}: either a Dual Constraint is Binding (``Sharp'')
or else the corresponding Shadow Price (\S\ref{sec:shadow_price}) $x_j^* = 0$
(is Non-basic)



\subsubsection{Linear Inequality}\label{sec:linear_inequality}

\subsubsection{Shadow Price}\label{sec:shadow_price}

cf. Lagrange Multipliers (\S\ref{sec:lagrange_multiplier})

UC Math 352 Lec. 15 - \url{https://www.youtube.com/watch?v=wNflFE4m17E}:

for a Standard Form LP, perturbing a RHS $b_i \rightarrow b_i + \Delta{b_i}$
may cause a change to the Optimal Objective
$\vec{p}^* \rightarrow \vec{p}^* + \Delta{\vec{p}^*}$ and the quantity:
\[
  -\frac{\Delta p_i^*}{\Delta b_i}
\]
is called the \emph{Shadow Price} of the $i$th Constraint $\vec{a}_i^T\vec{x} =
b_i$
(FIXME: how is a column of A multiplied by x ???)

\emph{assuming the Basis remains constant}, then the Dual Variable
$\vec{\nu}^*$ won't Change and:
\[
  \Delta \vec{p}^* = - \nu_i^* \Delta b_i
\]
so the Shadow Price of the $i$th Constraint is just the $i$th Optimal Dual
Variable $\nu_i^*$

\emph{Complementary Slackness}: either a Dual Constraint is Binding (``Sharp'')
or else the corresponding Shadow Price $x_j^* = 0$ (is Non-basic)

(Economics) Shadow Prices represent how much improvement can be made to the
Objective by ``loosening Constraints'', e.g. if a Constraint has a Shadow Price
of $\nu_i^* = \$10$, then loosening the Constraint by an amount $a$ results in
the Objective decreasing by $\$10 a$, i.e. if a Constraint can be loosened for
less than the Shadow Price, then the Objective can be improved by doing so; if
the Shadow Prices is \$0, then loosening a Constraint does not improve the
Objective

\fist cf. Shadow Prices for Quadratic Programs
(\S\ref{sec:quadratic_programming})



\subsubsection{Integer Linear Programming}\label{sec:integer_linear_programming}

ILP

NP-Complete



\subsubsection{Linear-fractional Programming}
\label{sec:linear_fractional_programming}



% --------------------------------------------------------------------
\subsection{Quadratic Programming}\label{sec:quadratic_programming}
% --------------------------------------------------------------------

all Hard Constraints are Linear but Objective Function is Quadratic

(wiki):

for a Quadratic Form augmented with Linear Terms, $x^TAx + 2b^Tx$, the
First-order Conditions for a Maximum or a Minimum are found by setting the
Matrix Derivative to the Zero Vector:
\[
  2Ax + 2b = 0
\]
giving:
\[
  x = -A^{-1}b
\]
assuming $A$ is Non-singular (Invertible); if the Quadratic Form (and hence
$A$) are Positive Definite, then the Second-order Conditions for a
Minimum are met at the Point
(TODO: xrefs)

for Positive Definite (\S\ref{sec:positive_definite}) $Q$ (i.e. Convex
Objective Function) the problem is solvable by Ellipsoid Method
(\S\ref{sec:ellipsoid_method}) in Polynomial Time; otherwise the problem is NP
Hard

with Convex Objective Function and Linear Constraints \fist Convex Optimization
(\S\ref{sec:convex_optimization})

\fist cf. Sequential Quadratic Programming (Iterative Method
\S\ref{sec:sequential_quadratic_programming})

2001 - Milenkovic, Schmidl - Optimization-Based Animation

UC Math 352 Lec. 18 - \url{https://www.youtube.com/watch?v=S538WFC8JLY}

Quadratic Objective Function:
\begin{align*}
  f(\vec{x})        & = \frac{1}{2}\vec{x}^T P \vec{x} + \vec{q}^T\vec{x} \\
  \nabla f(\vec{x}) & = P\vec{x} + \vec{q} \\
  H(f(\vec{x}))     & = P \\
\end{align*}
where $H(f(x))$ is the Hessian Matrix (\S\ref{sec:hessian_matrix}) of
Second-order Partial Derivatives of $f$

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x} \in \reals^m}$ &
    $\frac{1}{2}\vec{x}^T P\vec{x} + \vec{q}^T\vec{x}$ \\
\end{tabular}

if $P$ is not Positive Semidefinite (\S\ref{sec:positive_semidefinite}), choose
an $\vec{x}_0$ such that $\vec{x}_0^T P\vec{x}_0 < 0$, then
$f(t\vec{x}_0) = (\frac{1}{2}\vec{x}_0^TP\vec{x}_0)t^2 + (\vec{q}^T\vec{x}_0)t$
and $f(t\vec{x}_0)$ goes to Negative Infinity as $t$ goes to Infinity

if $P$ is Positive Definite then there is a Local Minimum at $\vec{x}^* =
P^{-1}\vec{q}$ and since the Quadratic Objective Function is Convex,
$\vec{x}^*$ is a Global Minimum-- a \emph{Unique Stationary Point} (FIXME:
clarify)

if $P$ is strictly Positive Semi-definite, then if there are any Minimizers
$\vec{x}^*$, they satisfy $P\vec{x}^* = - \vec{q}$ (there may be Zero, one or
more Minimizers)


\textbf{Equality Constrained QP}

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x} \in \reals^m}$ &
    $f(x) = \frac{1}{2}\vec{x}^T P\vec{x} + \vec{q}^T\vec{x}$ \\
  subject to & $A\vec{x} = \vec{b}$ \\
\end{tabular}

where $A \in \reals^{m \times n}, m < n$ is Full Rank (no redundant
Constraints)

\emph{Nullspace Method} (or \emph{QR Method})

take QR Factorization (\S\ref{sec:qr_decomposition}) of $A$:
\[
  A^T = QR = [\hat{Q} \ Q_N] [\hat{R} \ \vec{0}]^T
\]
writing $\vec{x}$ as:
\[
  \vec{x} = \hat{Q}\vec{u} + Q_N\vec{v}
\]
then $\vec{u} = \hat{R}^{-T}\vec{b}$ is fixed and $\vec{v}$ is free,
let $\hat{x} = \hat{Q}\hat{R}^{-T}\vec{b}$ in:
\[
  \vec{x} = \hat{x} + Q_N\vec{v}
\]
and the Objective Function can be written as the following Unconstrained
Quadratic Function in $\vec{v}$:
\[
  f(\vec{x}) = \frac{1}{2}\vec{v}^T(Q_N^T P Q_N)\vec{v} +
    (\hat{x}^T PQ_N + \vec{q}^T Q_N)\vec{v} +
    \frac{1}{2}\hat{x}^T P\hat{x} + \vec{q}^T\hat{x}
\]
if $Q_N^T P Q_N$ is Positive Definite, i.e. $P$ is Positive Definite on
$\mathrm{null}(A)$ (FIXME: clarify), then the Unique Minimizer is:
\[
  \vec{v} = -(Q_N^T P Q_N)^{-1}(\hat{x}^T P Q_N + \vec{q}^T Q_N)
\]
and the Optimal $\vec{x}^*$:
\[
  \vec{x}^* = \hat{x} + Q_N\vec{v}^*
\]


\emph{KKT Method}

for:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{\vec{x}} \in \reals^m}$ &
    $f(\vec{x}) = \frac{1}{2}\vec{x}^T P\vec{x} + \vec{q}^T\vec{x}$ \\
  subject to & $h_j(\vec{x}) = 0, j \in \{1,\ldots,\ell\}$ \\
\end{tabular}

where $f(\vec{x})$ is Convex (\S\ref{sec:convex_function}) and $h_j(\vec{x})$
are Affine (\S\ref{sec:affine_transformation}),
then the Optimality Conditions are given by the Karush-Kuhn-Tucker (KKT)
Conditions (\S\ref{sec:karush_kuhn_tucker}):
\begin{align*}
  h_j(\vec{x}^*) & = 0 & \ \ \ (Feasibility) \\
  \nabla f(\vec{x}^*) +
    \sum_{j=1}^\ell \lambda_j^* \nabla h_j(\vec{x}^*) & = 0 & \\
\end{align*}
where $\nabla f(\vec{x}^*) + \sum_{j=1}^\ell \lambda^*_j \nabla h_j(\vec{x}^*)$
with Optimal $\lambda^*_j$ KKT Multipliers, is the Gradient of the Lagrangian
(\S\ref{sec:lagrange_multiplier}) at Optimality;
these conditions are \emph{necessary and sufficient} for Global Optimality of a
Convex Problem with Affine Equality Constraints

in Matrix form:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{\vec{x}} \in \reals^m}$ &
    $\frac{1}{2}\vec{x}^T P\vec{x} + \vec{q}^T\vec{x}, P > 0$ \\
  subject to & $A\vec{x} = \vec{b}$ \\
\end{tabular}

where $P > 0$ indicates that $P$ is Positive Definite
(\S\ref{sec:positive_definite})

the KKT Conditions can be written as:

\begin{align*}
  A\vec{x}^*                                & = \vec{b} \\
  P\vec{x}^* + \vec{q} + A^T\vec{\lambda}^* & = \vec{0}
\end{align*}

or:

\[
  \begin{bmatrix}
    P & A^T \\
    A & 0   \\
  \end{bmatrix}
  \begin{bmatrix}
    \vec{x}^*       \\
    \vec{\lambda}^* \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    -\vec{q} \\
    \vec{b}  \\
  \end{bmatrix}
\]
which is a Set of $m + n$ Equations in $m + n$ Variables which can be Solved;
if the KKT Matrix is Non-singular, then the Equation completely determines the
Solution (FIXME: clarify)

compared to the QR Method above, the KKT method is less general since $P$ is
required to be Positive Definite


UC Math 352 Lec. 19 - \url{https://www.youtube.com/watch?v=bVN7gNFlSPA}:

$P\vec{x}^* + \vec{q} + A^T\vec{\lambda}^* = \vec{0}$ is equivalent to:
\[
  f(\vec{x}^*) + A^T\vec{\lambda}^* = \vec{0}
\]
(FIXME: is this only true for positive definite P ???)


\emph{Constraint Perturbation (Shadow Prices)} (\S\ref{sec:shadow_price})

if $\vec{x}^*$ Minimizes $f(\vec{x})$ over $A\vec{x} = \vec{b}$ then $\nabla
f(\vec{x}^*)$ must be Orthogonal to $\mathrm{null}(A)$, i.e. it is in the Range
of $A^T$, $\mathrm{range}(A^T)$, so $\nabla f(\vec{x}^*) = -A^T\vec{\lambda}$
for some $\vec{\lambda} \in \reals^m$

a small perturbation $\Delta \vec{x}$ to $\vec{x}^*$ gives a corresponding
First-order change to $f(\vec{x})$ is:
\begin{align*}
  \Delta f & = \Delta\vec{x}^T\nabla{f} = -\Delta\vec{x}^TA^T\vec{\lambda} \\
  \Delta f & = -\Delta\vec{b}^T\vec{\lambda} \\
\end{align*}
therefore the Lagrange Multipliers (\S\ref{sec:lagrange_multiplier}), i.e.
Shadow Prices, relate increments in the RHS of the Constraints to corresponding
changes in the Objective


\textbf{Inequality Constrained QP}

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{\vec{x}} \in \reals^m}$ &
    $f(\vec{x}) = \frac{1}{2}\vec{x}^T P\vec{x} + \vec{q}^T\vec{x}$ \\
  subject to & $A\vec{x} \leq \vec{b}$ \\
\end{tabular}

where $P$ is Positive Definite and $A \in \reals^{m \times n}$


\emph{Active Set Method} (TODO: xref)

cf. Sequential Quadratic Programming
(\S\ref{sec:sequential_quadratic_programming})

an \emph{Active Set} $S$ is the Set of Constraints that hold with Equality
(i.e. are Binding or ``Sharp'')

(FIXME: in the following how is a column of A multiplied by x ???)

$S^*$ is the \emph{Optimal Active Set} when:
\begin{enumerate}
  \item the Optimizer $\vec{x}^*$ is in the Feasible Region
  \item $\lambda^*_i \geq 0$ where $\vec{\lambda}^*$ is the Set of Lagrange
    Multipliers for the Equality Constraints $\vec{a}_i^T\vec{x} = b_i$ for $i
    \in S^*$
\end{enumerate}

given an Optimal Active Set $S^*$, then solving an Equality Constrained QP:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{\vec{x}} \in \reals^m}$ &
    $f(\vec{x}) = \frac{1}{2}\vec{x}^T P\vec{x} + \vec{q}^T\vec{x}$ \\
  subject to & $\vec{a}_i^T\vec{x} = b_i, i \in S^*$ \\
\end{tabular}

yields the \emph{Optimizer} (Minimizer) $\vec{x}^*$ of the Inequality
Constrained QP

Active Set Algorithm (basic):
\begin{enumerate}
  \item start at a Feasible Point $\vec{x}_0$ and an initial Active Set $S_0$
  \item calculate $\vec{x}^*_{EQP}$ and $\vec{\lambda}^*_{EQP}$ which Minimizes
    the EQP defined by the current Active Set
    \begin{enumerate}
      \item if $\vec{x}^*_{EQP}$ is Feasible, then ``move'' to $\vec{x}^*$ and
        check Lagrange Multipliers and if all $\lambda^*_i > 0, \lambda^*_i \in
        \vec{\lambda}^*_{EQP}$, then $\vec{x}^*_{EQP}$ is the Optimizer (FIXME:
        clarify); otherwise if one of the Lagrange Multipliers $\lambda^*_i$ is
        Negative then remove that Constraint from the Active Set (FIXME: and
        add a new constraint ???)
      \item if $\vec{x}^*_{EQP}$ is Infeasible then ``move'' as far as possible
        along the line segment from $\vec{x}_0$ to $\vec{x}^*_{EQP}$ while
        staying Feasible and add to this the first Constraint that blocs
        further progress
    \end{enumerate}
\end{enumerate}

finding an initial Feasible Point $\vec{x}_0$ is the problem of finding a
Feasible Point $\vec{x}$ that Satisfies $A\vec{x} \leq \vec{b}$ which can be
solved as a Phase I Linear Program (\S\ref{sec:linear_programming}) and will
also tell which Constraints are Active-- the Initial Active Set is the Set of
Constraints that are Binding (``Sharp''), i.e. hold with Equality at
$\vec{x}_0$

Active Set Algorithm (formal):
\begin{enumerate}
  \item let $\vec{x} = \vec{x}_0$, $S = S_0$
  \item let $\vec{x}^*_{EQP}$ and $\vec{\lambda}^*_{EQP}$ ... (FIXME ???) solve
    the Equality Constrained QP defined by $S$
  \item if $\vec{x}^*_{EQP}$ is Feasible:
    \begin{enumerate}
      \item if $\lambda^*_i \geq 0$ for all $\lambda^*_i \in
        \vec{\lambda}^*_{EQP}$, then $\vec{x}^*_{EQP}$ is Optimal and the
        algorithm is finished
      \item otherwise remove from $S$ a Constraint with $\lambda^*_i < 0$ and
        Set $\vec{x} = \vec{x}^*_{EQP}$ and go to Step (2.)
    \end{enumerate}
    otherwise let $t_{max}$ ... (FIXME ???) solve:

    \begin{tabular}{r l}
      $\mathrm{max}$ & $t$ \\
      subject to     & $A(\vec{x} + t(\vec{x}^*_{EQP} - \vec{x})) \leq \vec{b}$
    \end{tabular}

    set $\vec{x}_+ = \vec{x} + t_{max}(\vec{x}^*_{EQP} - \vec{x})$ and add to
    $S$ one Constraint that is Binding at $\vec{x}_+$ and that is violated by
    $\vec{x}^*_{EQP}$ (i.e. $\vec{a}_i^T\vec{x}^*_{EQP} > b_i$), then go to
    Step (2.)
\end{enumerate}

cf. the Simplex Method (\S\ref{sec:simplex_algorithm}) where the number of
Constraints in the ``Active Set'' is fixed (FIXME: at two corresponding to a
vertex ???)

a worked example can be found here: UC Math 352 Lec. 20 -
\url{https://www.youtube.com/watch?v=e6jDGxNZ-kk}

since $P$ is Positive-definite, along any line $\vec{x}(t) = \vec{x} +
t\vec{u}, t \in \reals, \vec{u} \neq \vec{0}$ the Quadratic $f(\vec{x}(t))$ is
Convex

at each iteration $\vec{x}^{(k)}$ and $s\vec{x}_{EQP}^{(k)}$ both lie on the
\emph{Affine Manifold} (\S\ref{sec:affine_manifold}):
\[
  \mathcal{M}^(k) = \{
    \vec{x} : \forall i \in S^{(k)}, \vec{a}_i^T\vec{x} = b_i
  \}
\]
so $f(\vec{x}^{(k)}) \geq f(\vec{x}_{EQP}^{(k)})$ with Equality if and only if
$\vec{x}^{(k)} = \vec{x}_{EQP}^{(k)}$, i.e. Minimizing subject to Constraints
produces a Minimum that is less than or equal to the current value, and because
the Quadratic Term of $f(\vec{x}(t))$ is Nonzero, the Objective strictly
decreases unless the two Points are equal, that is $f$ decreases along the line
segment joining $\vec{x}^{(k)}$ to $\vec{x}_{EQP}^{(k)}$-- \emph{every ``nonzero
  move'' from $\vec{x}^{(k)}$ to $\vec{x}_{EQP}^{(k)}$ strictly reduces $f$}:
\[
  \vec{x}^{(k+1)} \neq \vec{x}^{(k)} \Rightarrow
    f(\vec{x}^{(k+1)}) < f(\vec{x}^{(k)}
\]

when deleting a Constraint from $S^{(k)}$, the current Point is already at the
``lowest Point'' on the Manifold $\mathcal{M}^{(k)}$; then a ``nonzero move''
necessarily goes to a lower Point and it will not be possible to return to
$\mathcal{M}^{(k)}$, i.e. it is not possible to return to the same Active Set
$S^{(k)}$ again-- because the number of Active Sets is Finite, \emph{the
  algorithm must terminate with the Optimal Active Set unless making moves of
  Length $0$ (Degeneracy, i.e. Cycling)}

Degeneracy in QP has the same features as in LP: a ``coincidence'' of
Constraints allows Cycling through several Active Sets at the same Point
$\vec{x}$-- apply Bland's Anticycling Rules

some more example problems: UC Math 352 Lec. 21 -
\url{https://www.youtube.com/watch?v=VJ-04jOfu-E},
Lec. 22 - \url{https://www.youtube.com/watch?v=YGq1bRFDpHo} (Newton's Method \S\ref{sec:newtons_method})



\subsubsection{Quadratically Constrained Quadratic Programming}
\label{sec:quadratically_constrained}

QCQP

both Objective Function and Constraints are Quadratic

with Convex Quadratic Constraints (FIXME: and convex objective function ???)
\fist Convex Optimization (\S\ref{sec:convex_optimization})

if the $P_0,\ldots,P_m$ Matrices are all Positive-definite Matrices
(\S\ref{sec:positive_definite}) then the problem is Convex and can be solved
using Interior Point Methods (\S\ref{sec:interior_point}), as in
Semidefinite Programming (\S\ref{sec:semidefinite_programming})



% --------------------------------------------------------------------
\subsection{Geometric Programming}\label{sec:geometric_programming}
% --------------------------------------------------------------------

not Convex in general but may be transformed into Convex Problems by change of
Variables and a transformation of the Objective and Constraint Functions

Inequality Constraints are Posynomials (\S\ref{sec:posynomial}) $f_i(x) \leq 1$

Equality Constraints are Monomials (\S\ref{sec:monomial}) $h_i(x) = 1$



% --------------------------------------------------------------------
\subsection{Relaxation}\label{sec:relaxation}
% --------------------------------------------------------------------

\fist not to be confused with Iterative Methods of Relaxation (Stationary
Iterative Methods \S\ref{sec:stationary_iterative})

Lagrangian Relaxation

Linear Programming Relaxation



% --------------------------------------------------------------------
\subsection{Differentiable Programming}\label{sec:differentiable_programming}
% --------------------------------------------------------------------

2016 - Sajovic, Vuk - \emph{Operational Calculus for Differentiable Programming}
-- Operational Calculus (\S\ref{sec:operational_calculus}) provides a Formal
Semantics through an algebra of higher-order constructs

Differentiable Programming Space

2016 - Izzo, Biscani, Mereta - \emph{Differentiable Genetic Programming}



\subsubsection{Automatic Differentiation}\label{sec:automatic_differentiation}

repeated application of the \emph{Chain Rule} (\S\ref{sec:chain_rule}), i.e.
Forward and Reverse Accumulation

cf. \emph{classical methods}:

\begin{itemize}
  \item Symbolic Differentiation (Computer Algebra)
  \item Numerical Differentiation (\S\ref{sec:numerical_differentiation})
\end{itemize}

Automatic Differentiation can be more efficient than Symbolic Differentiation
and does not have rounding errors like Numerical Differentiation, and is faster
than both classical methods at computing the Partial Derivatives of a Function
with respect to many Inputs, needed for Gradient-based Optimization Algorithms

Supervised Learning

\astiersm

\url{http://www.columbia.edu/~ahd2125/post/2015/12/5/} (includes Rust
implementation)

\astiersm

(wiki):

Forward-mode Automatic Differentiation is accomplished by augmenting the Algebra
of Real Numbers by adding a component to every number which will represent the
\emph{Derivative} (\S\ref{sec:derivative}) of a Function at the number, and
extending all Arithmetic Operators to take this into account, resuling in the
Algebra of Dual Numbers (\S\ref{sec:dual_number}); generalized by theory of
Operational Calculus (\S\ref{sec:operational_calculus}) on Programming Spaces
through the Tensor Algebra on the Dual Space (\S\ref{sec:dual_space})

Forward Accumulation, Reverse Accumulation

generally Forward and Reverse Accumulation are specific instances of applying
the \emph{Operator of Program Composition} (FIXME)

2018 - van de Meent, Paige, Yang, Wood -
\emph{An Introduction to Probabilistic Programming} --
two approaches to AI:

Probabilistic Programming Languages are to Probabilistic (or Bayesian) Machine
Learning as Automated Differentiation tools are to Deep Learning

\asterism

2018 - Conal Elliot - \emph{The simple essence of automatic differentiation}
(video lecture): \url{https://www.youtube.com/watch?v=Shl3MtWGu18}

1965 - Spivak - \emph{Calculus on Manifolds}

the \emph{Differentiation} is a Higher-order Function $D$ with signature:
\[
  D : (a \rightarrow b) \rightarrow (a \rightarrow (a \multimap b))
\]
that is, a Function which given a Function from $a$ to $b$ returns a
(Higher-order) Function from $a$ to a \emph{Linear Map} from $a$ to $b$; the
\emph{Derivative} (\S\ref{sec:derivative}) of a Function at a Point of $a$ is
the Linear Map given by applying the resulting Function to the Point;
specifically this Linear Map is a \emph{Local Linear Approximation} $T$ of the
original Function at that Point, i.e.:
\[
  \lim_{\varepsilon \rightarrow 0}
    \frac{\|f (a + \varepsilon) - (f a + T \varepsilon)\|}{\|\varepsilon\|}
    \equiv 0
\]

the \emph{Chain Rule} (\S\ref{sec:chain_rule}) applies to \emph{Sequential
  Composition} $(g \circ f)(a) = g (f (a))$ of Functions:
\[
  D(g \circ f) (a) \equiv D g (f(a)) \circ D f (a)
\]

for \emph{Parallel Composition} (i.e. \emph{no Data Dependencies}) or the ``Fork
Operator'', $\delta$ is the fusion of two Functions on the same Domain into a
single Function that gives a Product, $(f \vartriangle g)(a) = (f(a), g(a))$,
also has a Derivative Rule:
\[
  D(f \vartriangle g)(a) \equiv (D f (a)) \vartriangle (D g (a))
\]

note that the Chain Rule is \emph{not compositional}, i.e. the Derivative of the
Composition is not \emph{just} a Function of the two Derivatives, but it
involves sampling $g$ at $f(a)$; in Automatic Differentiation, a compositional
version of Differentiation is given as:
\begin{align*}
  D_{comp}   & : (a \rightarrow b)
    \rightarrow (a \rightarrow (b \times (a \multimap b))) \\
  D_{comp} f & = f \vartriangle D f
\end{align*}
which returns a Function which computes both the Linear Map and its value at the
Point $a$

the Derivative of a Linear Function $f$ is itself everywhere:
\[
  D_{comp} f(a) = (f a, f)
\]
that is:
\[
  D_{comp} f = f \vartriangle (const f)
\]
e.g.:
\begin{itemize}
  \item $D id (a) = id$
  \item $D fst (a) = fst$
  \item $D snd (a) = snd$
\end{itemize}

AD example of the $sqr(x) = x * x$ Function, $D_{comp}$ takes a value $x$ and
returns a pair:
\begin{enumerate}
  \item $x^2 = sqr(x)$ -- the result of applying $sqr$ to $x$
  \item $2*x = D sqr(x)$ -- the Derivative of $sqr$ at $x$, but note this is
    seen as a \emph{Linear Map} $(D sqr(x)) (x') = 2 * x * x'$ that takes any
    \emph{other} $x'$ and returns the result of applying the Local Linearization
    of $sqr$ at $x$
\end{enumerate}

\emph{Product Rule} (\emph{Leibniz Rule} \S\ref{sec:product_rule}) for Products
of Functions

\emph{Cartesian Categories} (\S\ref{sec:cartesian_category}) -- the above
follows from the property of Linear Maps that they form a Cartesian Category; it
is therefore possible to \emph{generalize} Automatic Differentiation to an
arbitrary Cartesian Category with Multiplication generalized in terms of a
\emph{Scale} operation:
\begin{align*}
scale   & : a \rightarrow (a \multimap a) \\
scale u & = \lambda v \rightarrow u * v
\end{align*}
and a (Cocartesian) \emph{Join} operation, $\triangledown$:
\begin{align*}
  (\triangledown)   & : (a \multimap c) \rightarrow (b \multimap c) \rightarrow
    ((a \times b) \multimap c) \\
  f \triangledown g & = \lambda (a,b) \rightarrow f(a) + g(b)
\end{align*}
as:
\[
mul = D (mul \vartriangle
  (\lambda (a,b) \rightarrow (scale\ b \triangledown scale\ a)))
\]

an example would be Functions between ``Additive'' Types (Types with Addition
and Zero)

using (generalized) \emph{Matrices} (as the composition of the ``row
structures'' and ``column structures'') instead of Functions as a data
representation -- Identity is the Identity Matrix, Composition is Matrix
Multiplication, Scale is constructing a $1 \times 1$ Matrix, Join takes two
Matrices of the same height and horizontally juxtaposes them, and Fork takes two
Matrices of the same width and vertically juxtaposes them

All-right Association (Forward Mode AD) --

All-left Association (Reverse Mode AD) -- better for Gradient-based
Optimization, Machine Learning

Continuation Category

Dual Vectors (Linear Functionals) -- Gradient-based Optimization

Automatic Differentiation \emph{is} Symbolic Differentiation done by a Compiler



% ====================================================================
\section{Non-linear Optimization}\label{sec:nonlinear_optimization}
% ====================================================================

%FIXME: merge this section ???

\emph{Nonlinear Optimization}

Nonlinear Constraint Solving in Programming Languages: ATS (Xi16) uses
explicit Proof Construction to handle Nonlinear Constraints

\fist cf. Nonlinear Programming (\S\ref{sec:nonlinear_programming})

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})



% ====================================================================
\section{Convex Optimization}\label{sec:convex_optimization}
% ====================================================================

Convex Analysis (\S\ref{sec:convex_analysis})

Convex Functions (\S\ref{sec:convex_function}) have the property that a Local
Minimum is the unique Global Minimum

Maximum Principle (Harmonic Functions \S\ref{sec:harmonic_function})

for Convex Optimization Problems, the Duality Gap
(\S\ref{sec:optimization_duality}) is Zero under a Constraint Qualification
Condition (\S\ref{sec:constraint_qualification})

may be formulated as Convex Optimization Problems:
\begin{itemize}
  \item Lagrange Multipliers (\S\ref{sec:lagrange_multiplier})
  \item Linear Programming (\S\ref{sec:linear_programming})
  \item Quadratic Programming (\S\ref{sec:quadratic_programming}) with Convex
    Objective Function and Linear Constraints
  \item Quadratic Programming with Convex Quadratic Constraints
    (QCQP \S\ref{sec:quadratically_constrained})
\end{itemize}

cf. for Linear Regression (\S\ref{sec:linear_regression}), a Mean-Square Error
Loss Function is always \emph{Convex} (example
\url{https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a}

UC Math 352 Lec. 22 - \url{https://www.youtube.com/watch?v=YGq1bRFDpHo}:
Interior Point Method (\S\ref{sec:interior_point}) for general Convex
Optimization Problems

SU EE 364A - \url{https://www.youtube.com/playlist?list=PL3940DD956CDF0622}



% --------------------------------------------------------------------
\subsection{Linear Matrix Inequality (LMI)}\label{sec:lmi}
% --------------------------------------------------------------------

\[
  LMI(y) := A_0 + y_1A_1 + y_1A_2 + \cdots + y_mA_m \geq 0
\]

where ... TODO



% --------------------------------------------------------------------
\subsection{Interior Point Method}\label{sec:interior_point}
% --------------------------------------------------------------------

Linear Programming Problems (\S\ref{sec:linear_programming}) can be solved by
the Interior Point Method (\S\ref{sec:interior_point}) in Polynomial Time

Semidefinite Programming Problems (\S\ref{sec:semidefinite_programming})
can be solved efficiently by Interior Point Methods

Quadratically Constrained Quadratic Programming
(\S\ref{sec:quadratically_constrained}): if the $P_0,\ldots,P_m$ Matrices are
all Positive-definite Matrices (\S\ref{sec:positive_definite}) then the problem
is Convex and can be solved using Interior Point Methods, as in Semidefinite
Programming

any Convex Optimization Problem can be transformed into Minimizing a Linear
Function over a Convex Set by converting to the \emph{Epigraph Form}

\emph{Primal-dual Path-following Interior Point Methods}; Mehrotra's
Predictor-Corrector Algorithm

\asterism

UC Math 352 Lec. 22 - \url{https://www.youtube.com/watch?v=YGq1bRFDpHo}:
Interior Point Method for general Convex Optimization Problems

widely used approach for solving general Convex Problem (includes LP, QP,
etc.):

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $f(\vec{x})$     \\
  subject to               & $h_i(\vec{x}) \leq 0, i \in \{1,\ldots,m\}$ \\
                           & $A\vec{x} = \vec{b}$   \\
\end{tabular}

where $f, h_i$ are Convex and Twice-continuously Differentiable

$A \in \reals^{p \times n}, p < n$ is Full Rank

approximate Inequality Constraints by Continuous Functions in the Objective:
incorporate Inequality Constraints into the Objective by:

\begin{tabular}{r l}
  $\mathrm{min}$ & $f(\vec{x}) + \sum_{i=1}^m I_{-} (h_i(\vec{x}))$ \\
  subject to     & $A\vec{x} = \vec{b}$ \\
\end{tabular}

where $I_{-}$ is the Indicator Function for Non-positive Reals:
\[
  I_{-}(u) = \begin{cases}
    0       & \ \ \ u \leq 0 \\
    \infty  & \ \ \ u >    0 \\
  \end{cases}
\]
can be ``softened'' by a Continuous Approximation:
\[
  \hat{I}_{-}(u) = -\frac{1}{t} \log (-u)
\]
where $t > 0$ is a parameter setting the accuracy of the approximation;
substituting into the original problem:

\begin{tabular}{r l}
  $\mathrm{min}$ &
    $f(\vec{x}) + \sum_{i=1}^m -\frac{1}{t} \log(-h_i(\vec{x}))$ \\
  subject to     & $A\vec{x} = \vec{b}$ \\
\end{tabular}

has an Objective that is both Continuous and Convex, so Newton's Method
(\S\ref{sec:newtons_method}) can be used to solve-- solve Equality Constrained
Problem for a particular value of $t$ and then increase $t$ using the Optimum
as the starting point for the next iteration; as $t \rightarrow \infty$, the
approximation becomes exact

note: as $t$ becomes large the problem becomes less well Conditioned, so it is
sensitive to initial starting point

\fist Interior Point Linear Programming (\S\ref{sec:linear_programming}) often
performs at least as well or better than the Simplex Method
(\S\ref{sec:simplex_algorithm})


\asterism

2004 - Boyd, Vandenberghe - \emph{Convex Optimization}, Ch.11

``hierarchy'' of Convex Optimization algorithms (from ``lowest''=``simplest'' to
``highest''):
\begin{itemize}
  \item Linear Equality Constrained Quadratic Programming Problems --
    ``simplest''; KKT Conditions are a Set of Linear Equations which can be
    solved Analytically
  \item Newton's Method (\S\ref{sec:newtons_method}) -- technique for solving a
    Linear Equality Constrained Optimization Problem with Twice-differentiable
    Objective by reducing it to a sequence of Linear Equality Constrained
    Quadratic Problems
  \item Inerior Point Methods -- solves Optimization Problems with Linear
    Equality and Inequality Constraints by reducing it to a sequence of Linear
    Equality Constrained Problems
\end{itemize}



\subsubsection{Barrier Method}\label{sec:barrier_method}

class of algorithms for Constrained Optimization
(\S\ref{sec:constrained_optimization})

cf. Penalty Method (\S\ref{sec:penalty_method})



% --------------------------------------------------------------------
\subsection{Basis-exchange Algorithm}\label{sec:basis_exchange}
% --------------------------------------------------------------------

\subsubsection{Simplex Algorithm}\label{sec:simplex_algorithm}

\emph{Simplex Method}

Dantzig

Linear Programming (\S\ref{sec:linear_programming}) problems can be solved by
Simplex Method (\S\ref{sec:simplex_algorithm}), usually (but not guaranteed) in
Polynomial Time

\fist Interior Point (\S\ref{sec:interior_point}) Linear Programming often
performs at least as well or better than the Simplex Method

cf. Active Set Method (TODO: xref) for Quadratic Programming
(\S\ref{sec:quadratic_programming}) where the number of constriants in the
``Active Set'' changes as the algorithm progresses

UC Math 352 Lec. 11 - \url{https://www.youtube.com/watch?v=ESzYPFkY3og}:

Full Rank $A \in \reals^{m \times n}, m \leq n$ with Linearly Independent Rows

Standard Form LP:

\begin{tabular}{r l}
  $\mathrm{min}_{\vec{x}}$ & $\vec{c}^T\vec{x}$     \\
  subject to               & $A\vec{x} = \vec{b}$   \\
                           & $\vec{x} \geq \vec{0}$ \\
\end{tabular}

assume Variables written in an order such that the last $m$ Columns of $A$ are
Linearly Independent:
\begin{align*}
  A       & = [A_N \ A_B] \\
  \vec{x} & = [x_N \ x_B] \\
\end{align*}
where:
\begin{align*}
  A_N       & \in \reals^{m \times (n - m)} \\
  A_B       & \in \reals^{m \times m} \\
  \vec{x}_N & \in \reals^{n-m} \\
  \vec{x}_B & \in \reals^m \\
\end{align*}
then the Equality Constraints can be re-written as:
\[
  A_N\vec{x}_N + A_B\vec{x}_B = \vec{b}
\]
setting $\vec{x}_N = \vec{0}$, then this becomes:
\begin{align*}
  A_B\vec{x}_B & = \vec{b}
  \vec{x}_B    & = A_B^{-1}\vec{b}
\end{align*}

because $m \leq n$, this is an Underdetermined System so the resulting
$\vec{x}$ has at least $n - m$ Zero (at most $m$ Nonzero) elements and is
called a \emph{Basic Solution} where the $\vec{x}_N$ are called the
\emph{Non-basic Variables} and the $\vec{x}_B$ are called the \emph{Basic
  Variables}

more generally Basic Variables correspond to the Linearly Independent Columns
of $A$

\emph{Fundamental Theorem of Linear Programming}

Assume $A$ is Full Rank; if a Feasible Solution exists, then a Basic Feasible
Solution Exists, and if an Optimal Feasible Solution exists, then an Optimal
Basic Feasible Solution exists.

the implication is that only Basic Feasible Solutions need to be considered
when searching for an Optimum, i.e. an \emph{Optimal Basis} (Set of Columns)
$B$

Simplex Method: from a Basic Feasible Solution, try to \emph{improve} the
Solution (i.e. lower $\vec{c}^T\vec{x}$) by replacing one Basic Variable with a
Non-basic one, and repeat the process until no improvement to
$\vec{c}^T\vec{x}$ is possible

for the $k$-th Basic Feasible Solution $\vec{x}^{(k)}$, with Sets $B$, $N$ of
Basic and Non-basic Variables, computing $\vec{x}_B^{(k)} = A_B^{-1}\vec{b}$
the Cost:
\[
  \vec{c}^T\vec{x}^{(k)} =
  \vec{c}_N^T\vec{x}_N^{(k)} + \vec{c}_B^T\vec{x}_B^{(k)} =
  \vec{c}_B^T\vec{x}_B^{(k)}
\]
changing $\vec{x}_N$ makes it necessary to recompute $\vec{x}_B$ from Solving
$A_N\vec{x}_N + A_B\vec{x}_B = \vec{b}$:
\[
  \vec{x}_B = A_B^{-1}\vec{b} - A_B^{-1}A_N\vec{x}_N
\]
and the Cost can be expressed in terms of $\vec{x}_N$:
\begin{align*}
  \vec{c}^T\vec{x} & = \vec{c}_B^T\vec{x}_B + \vec{c}_N^T\vec{x}_N \\
                   & = \vec{c}^T\vec{x}^{(k)} + \vec{r}^T\vec{x}_N
\end{align*}
where $\vec{r} = (\vec{c}_N - A_N^T A_B^{-T} \vec{c}_B)$ called the \emph{Vector
  of Reduced Costs}

$\vec{r}$ quantifies how each Non-basic Variable affects the Cost; if there are
no Negative entries of $\vec{r}$, then no improvement to the Objective can be
made, and $\vec{x}^{(k)}$ is Optimal

otherwise choose the most Negative entry of $\vec{r}$ and choose to increase
the corresponding Non-basic Variable $x_e$ called the \emph{Entering Variable};
increasing $x_e$ means that one of the $\vec{x}_B$ entries will becomes
Negative (violating the Non-negativity Constraints), so the Variable that
becomes Negative (the \emph{Leaving Variable}) first will be replaced by $x_e$
in the Basis

(TODO: details)

Unbounded LP

\emph{Degeneracy} -- it is possible to return to a previously selected Basis
(\emph{Cycling}), a Basic Feasible Solution is called \emph{Degenerate} if $x_i
= 0$ for at least one $i \in B$; this occurs geometrically when two Basic
Solutions overlap, e.g. when more than two Constraints intersect at the same
Point

\fist Degeneracy in QP (\S\ref{sec:quadratic_programming}) has the same
features as in LP: a ``coincidence'' of Constraints allows Cycling through
several Active Sets at the same Point $\vec{x}$

\emph{Cycling}: the Simplex Method Strictly reduces $\vec{c}^T\vec{x}$ each
iteration for which the Entering Variable $x_e$ is \emph{increased} to a
Strictly Positive Value; Cycling can occur when $x_e$ can't be increased above
$0$, resulting in a sequence of Bases that repeats; this requires that $B_1,
\ldots, B_k$ is Degenerate (the Entering Variable is $0$) and no $x_e$ is
allowed so all Basic Feasible Solutions are the same Point
(FIXME: clarify)

\emph{Bland's Anticycling Rules} (``Smallest Subscript Rule''):
\begin{enumerate}
  \item of all Non-basic Variables for which the $\vec{r}$ entry is Negative,
    choose the smallest Index to be the Entering Variable (instead of the most
    Negative)
  \item if there is a tie for Leaving Variable, choose the one with the
    smallest Index
\end{enumerate}
these rules \emph{only} need to be invoked if no decrease in $\vec{c}^T\vec{x}$
is obtained by a particular iteration

Terminateion Guarantee for Non-degenerate LP: since LP has a Finite number of
Vertices, or Basic Feasible Solutions, and if the Objective decreases at each
step, then it is not possible to revisit a previous Basis, so the Simplex
Algorithm terminates in a Finite number of steps


\emph{Finding an initial Basic Feasible Solution}

find any $\vec{x}$ Satisfying all Constraints \fist Constraint Satisfaction
Problem (CSP \S\ref{sec:constraint_satisfaction})

\begin{itemize}
  \item ``All Slacks Basic'' -- Inequality Form LP with $\vec{b} \geq \vec{0}$;
    set the Slack Variables to the initial Basis gives an initial Basic
    Feasible Solution
  \item ``Artificial Variables'' (Phase I Problem) -- Standard Form LP; force
    $\vec{b} \geq \vec{0}$ by multiplying any Constraint with $b_i < 0$ by
    $-1$; introduce \emph{Artificial Variables} $x_{n+1}, \ldots, x_{n+m}$ and
    add to the LHS of each Constraint and extend $A$ with the $m \times m$
    Identity Matrix $I_m$ forming a \emph{Phase I Problem}:
    \begin{align*}
      [A \ I_m][x_1 \ \cdots \ x_{n+m}]^T & = \vec{b}
                                  \vec{x} & \geq \vec{0}
    \end{align*}
    Solving the Phase I Problem yields an initial Basic Feasible Solution if
    one exists; if the Phase I Solution does not have:
    \[
      x_{n+1} = x_{n+2} = \cdots = x_{n+m} = 0
    \]
    (FIXME: explain this equation)
    then the original Constraints are \emph{Infeasible}; otherwise the
    ``obvious'' initial Basis for the Phase I Problem is $B = \{ n+1, \ldots,
    n+m \}$ and the Basis that \emph{Optimizes} the Phase I Problem can be used
    as the Initial Basis for the \emph{Phase II Problem} (the original LP)
  \item Single Artificial Variable -- choose an initial Basis $B$ for which
    $A_B^{-1}$ exists; if $\vec{x}_B \geq \vec{0}$ then $B$ is an Initial Basic
    Feasible Solution, otherwise let $x_j$ be the most Negative element of
    $\vec{x}_B$, then introduce an Artificial Variable $x_{n+1}$ with the
    corresponding Column of $A$:
    \[
      \vec{a}_{n+1} = \vec{b} - (\sum_{i\in{B}}\vec{a}_i) + \vec{a}_j
    \]
    and define a new Basis $B' = B \setminus \{j\} \cup \{n+1\}$ (swap out $j$
    for the new Artificial Variable), then the Sum over the new Basis Vectors
    (Columns of $A$) is equal to $\vec{b}$, and $B'$ is an initial Basis for
    the Phase I Problem:
    \begin{align*}
      [A \ \vec{a}_{n+1}][x_1 \ \cdots \ x_{n+1}]^T & = \vec{b}
                                [\vec{x} \ x_{n+1}] & \geq \vec{0}
    \end{align*}
    if $x_{n+1}$ is ``Non-basic in the Solution to the Phase I Problem'' (Cost
    = 0) then the Optimal Basis $B$ of the Phase I Problem can be used as an
    initial Basis for the Phase II Problem (FIXME: clarify)
\end{itemize}



% --------------------------------------------------------------------
\subsection{Ellipsoid Method}\label{sec:ellipsoid_method}
% --------------------------------------------------------------------

Iterative Method (\S\ref{sec:iterative_method})

when specialized for solving feasible Linear Optimization
(\S\ref{sec:linear_programming}) Problems with Rational Data, the Ellipsoid
Method finds an Optimal Solution in a Finite number of steps

a Quadratic Programming (\S\ref{sec:quadratic_programming}) problem can be
solved in Polynomial Time using the Ellipsoid method if the Objective Function
is Convex



% --------------------------------------------------------------------
\subsection{Cone Optimization}\label{sec:cone_optimization}
% --------------------------------------------------------------------

includes Linear Programming and Semidefinite Programming problems

2018 - Selby, Sikora - \emph{How to make unforgeable money in generalised
  probabilistic theories} -- Proof using Cone Programming that under a
Quantifiable version of the No-cloning Theorem, one can create ``physical
money'' which has an Exponentially small chance of being counterfeited



\subsubsection{Semidefinite Programming}\label{sec:semidefinite_programming}

SDP

can be efficiently solved by Interior Point Methods

all Linear Programs can be expressed as SDPs

hierarchies of SDPs can approximate the solutions of Polynomial Optimization
problems



% ====================================================================
\section{Stochastic Optimization}\label{sec:stochastic_optimization}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Multi-armed Bandit}\label{sec:multiarmed_bandit}
% --------------------------------------------------------------------

\url{http://datagenetics.com/blog/october12017/index.html}

Epsilon-first Strategy

Epsilon-greedy Strategy

Epsilon-decreasing Strategy

Bayesian Bandits



% ====================================================================
\section{Combinatorial Optimization}
\label{sec:combinatorial_optimization}
% ====================================================================

\fist Combinatorial Optimization Problem
(\S\ref{sec:combinatorial_optimization_problem})



% --------------------------------------------------------------------
\subsection{Branch and Bound}\label{sec:branch_and_bound}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Dynamic Programming}\label{sec:dynamic_programming}
% --------------------------------------------------------------------

or \emph{Dynamic Optimization}

alternative to Calculus of Variations (\S\ref{sec:calculus_of_variations})

\fist Chart Parsers (\S\ref{sec:chart_parser}) -- uses Dynamic Programming which
eliminates Backtracking and prevents a Combinatorial Explosion

\url{https://medium.freecodecamp.org/follow-these-steps-to-solve-any-dynamic-programming-interview-problem-cc98e508cd0e}

Viterbi Algorithm



% ====================================================================
\section{Evolutionary Optimization}\label{sec:evolutionary_optimization}
% ====================================================================

\fist Information Geometry (\S\ref{sec:information_geometry}), Fisher
Information Metric (\S\ref{sec:fisher_metric})

\fist \textbf{Fisher's Fundamental Theorem of Natural Selection},
Quasi-linkage Equilibrium: approximation in the case of Weak Selection
and Weak Epistasis %FIXME



% --------------------------------------------------------------------
\subsection{Fitness Function}\label{sec:fitness_function}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Fitness Landscape}\label{sec:fitness_landscape}
% --------------------------------------------------------------------

A \emph{Fitness Landscape} (or \emph{Adaptive Landscape}) is an
evaluation of a Fitness Function for all candidate Solutions %FIXME



% ====================================================================
\section{Constraint Satisfaction Problem (CSP)}
\label{sec:constraint_satisfaction}
% ====================================================================

A Problem involving only Constraints but not an Objective Function
(Optimization \S\ref{sec:objective_function}) is called a \emph{Feasibility
  Problem}

\fist Decision Problem (\S\ref{sec:decision_problem})

in Finite Model Theory (\S\ref{sec:finite_model}):
\begin{itemize}
  \item Boolean Satisfiability Problem (SAT \S\ref{sec:sat})
  \item Satisfiability Modulo Theories (SMT \S\ref{sec:smt})
  \item Constraint Logic Programming (\S\ref{sec:constraint_logic_programming})
\end{itemize}


formally a triple $\langle{X,D,C}\rangle$ where $X$ is a Set of Variables, $D$
is the Set of Domains for each Variable, and $C$ is Set of Constraints

a \emph{Constraint} is a Pair $\langle{t,R}\rangle$ where $t \subset X$ is a
Subset of $k$ Variables and $R$ is a $k$-ary Relation on the corresponding
Subset of Domains in $D$

an \emph{Evaluation} of the Variables is a Function from a Subset of Variables
to a particular Set of Values in the corresponding Subset of Domains; an
Evaluation $v$ \emph{Satisfies} the Constraint $\langle{t,R}\rangle$ if the
Values assigned to the Variables $t$ Satisfies the Relation $R$

an Evaluation is \emph{Consistent} if it does not violate \emph{any} of the
Constraints

an Evaluation is \emph{Complete} if it includes \emph{all} of the Variables

an Evaluation is a \emph{Solution} if it is Consistent \emph{and} Complete--
such an Evaluation is said to \emph{Solve} the Constraint Satisfaction Problem

\fist Phase I Problem (Linear Programming \S\ref{sec:linear_programming},
Simplex Method \S\ref{sec:simplex_algorithm}): find an initial Feasible
Solution that Satisfies all Constraints \emph{without} taking into account the
Cost Function using the ``Artifical Variables'' method

\emph{Feder-Vardi Dichotomy Conjecture}: every CSP is either NP hard or in P



% --------------------------------------------------------------------
\subsection{Constraint Resolution}\label{sec:constraint_resolution}
% --------------------------------------------------------------------

Constraint Resolution on Finite Domains typically uses a form of \emph{Search}



% --------------------------------------------------------------------
\subsection{Dynamic Constraint Satisfaction Problem (DCSP)}\label{sec:dcsp}
% --------------------------------------------------------------------



% ====================================================================
\section{Complementarity Theory}\label{sec:complementarity_theory}
% ====================================================================

cf. Topological Degree Theory (\S\ref{sec:degree_theory}) applications



% --------------------------------------------------------------------
\subsection{Linear Complementarity Problem (LCP)}
\label{sec:linear_complementarity}
% --------------------------------------------------------------------

%FIXME an instance of linear optimization ???

Inequality Constraints and Limits %FIXME

Real Matrix -- $M$

Vector -- $q$

\emph{Linear Complementarity Problem} -- $LCP(M,q)$

find Vectors $z$, $w$ with constraints:

\begin{itemize}
  \item $0 \leq w_i,z_i$ -- each component of $w$ and $z$ is
    non-negative
  \item $z^T w = 0$ (or equivalently) $\sum_i w_i z_i = 0$ --
    \emph{Complementarity Condition}: Imples at most one of each pair
    $\{w_i,z_i\}$ can be Positive
  \item $w = M z + q$
\end{itemize}

Sufficient Condition for Existence and Uniqueness of a solution to LCP
is that $M$ is \emph{Symmetric} (\S\ref{sec:symmetric_matrix})
\emph{Positive-definite} (\S\ref{sec:positive_definite})

\fist Projected Gauss-Seidel (PGS \S\ref{sec:projected_gauss_seidel})



\subsubsection{Mixed Linear Complementarity Problem}\label{sec:mlcp}

Generalized LCP to include Free Variables



% ====================================================================
\section{Global Analysis}\label{sec:global_analysis}
% ====================================================================

% ====================================================================
\section{Global Optimization}\label{sec:global_optimization}
% ====================================================================

\begin{itemize}
  \item Least-squares Problems (\S\ref{sec:least_squares})
  \item Linear Programming (\S\ref{sec:linear_programming})
  \item Convex Optimization (\S\ref{sec:convex_optimization})
  \item Singular-value Decomposition (\S\ref{sec:svd})
\end{itemize}



% --------------------------------------------------------------------
\subsection{Simulated Annealing}\label{sec:simualted_annealing}
% --------------------------------------------------------------------

Metaheuristic
