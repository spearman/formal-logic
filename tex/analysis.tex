%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Mathematical Analysis}\label{part:mathematical_analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Analysis}

\begin{enumerate}
  \item \emph{Analyticity} -- Analytic Functions (\S\ref{sec:analytic_function})
  \item \emph{Holomorphy} -- Holomorphic Functions
    (\S\ref{sec:holomorphic_function})
  \item \emph{Harmonicity} -- Harmonic Functions
    (\S\ref{sec:harmonic_function})
  \item \emph{Conformality} -- Conformal Maps (\S\ref{sec:conformal_map})
\end{enumerate}

For the Complex Numbers (\S\ref{sec:complex_analysis}), the notions of
Analyticity, Holomorphy, Harmonicity, and Conformality \emph{coincide}; for the
Reals (\S\ref{sec:real_analysis}), and Quaternions
(\S\ref{sec:quaternionic_analysis}), not all notions are the same.

\emph{Differential and Integral Calculus} (\S\ref{sec:differential_calculus}),
\S\ref{sec:integral_calculus})

\begin{tabular}{|l l|}
  \hline
  Discrete & Continuous \\
  \hline
  (Finite) Difference (\S\ref{sec:finite_difference}) &
    Differential (Infinitesimal Difference \S\ref{sec:differential}) \\
  Difference Quotient (\S\ref{sec:difference_quotient}) &
    Derivative (\S\ref{sec:derivative}) \\
  Indefinite Sum (Antidifference \S\ref{sec:antidifference}) &
    Indefinite Integral (Antiderivative \S\ref{sec:antiderivative}) \\
  (Finite) Summation (\S\ref{sec:summation}) &
    Definite Integral (\S\ref{sec:integral}) \\
  \hline
\end{tabular}

\fist Analytic Geometry (Part \ref{part:analytic_geometry}), Differential
Geometry (\S\ref{sec:differential_geometry})

\fist Algebraic Analysis (\S\ref{sec:algebraic_analysis}) -- study of Systems of
Linear PDEs (\S\ref{sec:linear_pde_system}) using Sheaf Theory
(\S\ref{sec:sheaf_theory}) and Complex Analysis (\S\ref{sec:complex_analysis})

\fist Multivariate Analysis (Statistics \S\ref{sec:multivariate_analysis})



% ====================================================================
\section{Sequence}\label{sec:sequence}
% ====================================================================

A \emph{Sequence} can be defined as a Countable Totally Ordered
Multiset (\S\ref{sec:multiset}), that is, a collection of Elements (or
\emph{Terms}) in a given order where duplicate Elements are allowed.
The number of Elements in a Sequence is referred to as its
\emph{Length}.

A \emph{Finite Sequence} is called a \emph{Tuple} (\S\ref{sec:tuple}).

String (\S\ref{sec:string})

Series (\S\ref{sec:series})

Sequence Space (\S\ref{sec:sequence_space})

Sequence (Topology) (\S\ref{sec:sequence_topology})

$a_n : \nats \rightarrow \reals$



% --------------------------------------------------------------------
\subsection{Subsequence}\label{sec:subsequence}
% --------------------------------------------------------------------

A Sequence $a_n$ Converges (\S\ref{sec:convergent_sequence}) to $l$
if and only if all Subsequences of $a_n$ Converge to $l$.



% --------------------------------------------------------------------
\subsection{Tuple}\label{sec:tuple}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Periodic Sequence}\label{sec:periodic_sequence}
% --------------------------------------------------------------------

or \emph{Cycle}

\fist cf. Periodic Function (\S\ref{sec:periodic_function})



% --------------------------------------------------------------------
\subsection{Limit}\label{sec:sequence_limit}
% --------------------------------------------------------------------

Can be defined in any Metric (\S\ref{sec:metric}) or Topological Space
(\S\ref{sec:topological_space}), generalized to a Topological Net
(\S\ref{sec:net}); see also Limits (\S\ref{sec:limit}) and Colimits
(\S\ref{sec:colimit}) in Category Theory.

The Limit of a Sequence is Unique

Bounded Sequence (\S\ref{sec:bounded_sequence})

\emph{Limiting Process}

$\lim a_n = 0 \Leftrightarrow \lim |a_n| = 0$

$\lim (a_n \pm b_n) = \lim a_n \pm \lim b_n$

$\lim (a_n b_n) = (\lim a_n) (\lim b_n)$

$\lim (\frac{a_n}{b_n}) = \frac{\lim a_n}{\lim b_n}$ for $\forall
n \in \nats, b_n \neq 0$ and $\lim b_n \neq 0$

$a_n \leq b_n$ for all $n$ Implies $\lim a_n \leq \lim b_n$, but it is
not the case that $a_n < b_n$ for all $n$ Implies $\lim a_n < \lim
b_n$ since $a_n$ and $b_n$ may still be equal in the Limit.

For an Open Interval $(x,y)$, $\lim a_n \in (x,y)$ Implies $a_n \in
(x,y)$ for all $n$, but for not for a Closed Interval.

\url{https://math.stackexchange.com/questions/1490438/what-is-meant-by-limit-of-setshttps://math.stackexchange.com/questions/1490438/what-is-meant-by-limit-of-sets}
-- on what is meant by the ``Limit of Sets''



\subsubsection{Limit Inferior}\label{sec:liminf}

$\liminf$

$\underline{\lim}$



\subsubsection{Limit Superior}\label{sec:limsup}

$\limsup$

$\overline{\lim}$



\subsubsection{Convergent Sequence}\label{sec:convergent_sequence}

$\forall \varepsilon > 0, \exists N : n \geq N \Rightarrow |(a_n - l)| <
\varepsilon$

A Sequence $a_n$ Converges to $l$ if and only if all Subsequences
(\S\ref{sec:subsequence}) of $a_n$ Converge to $l$.

If $\lim a_n = l$, then for $k \in \ints$, $\lim (a_{n+k}) = l$

If $a_n$ is Convergent then it is Bounded
(\S\ref{sec:bounded_sequence}).

If $F$ is a Closed Set (\S\ref{sec:closed_set}), then for any Sequence
$x_n$ in $F$ can Converge to $x$ if and only if $x$ is in $F$.

Intervals for Convergent Real Sequences are replaced by \emph{Discs} in the
case of Convergent Sequences of Complex Numbers (\S\ref{sec:complex_analysis})

Absolute Convergence (TODO)

if a Sequence of Analytic Functions Converges Uniformly
(\S\ref{sec:uniform_convergence}) in a region $S$ of the Complex Plane then the
Limit is Analytic in $S$-- this demonstrates that the Complex Functions are
more ``well-behaved'' than the Real Functions since the Uniform Limit of
Analytic Functions on a Real Interval do not need to be Differentiable
(\S\ref{sec:nowhere_differentiable})

\fist cf. Rootfinding Algorithms (\S\ref{sec:root_finding}) -- Discrete
First-order Dynamical Systems (\S\ref{sec:discrete_dynamical_system}) can be
used to Numerically Solve Single-variable Equations-- the problem to be Solved
consisting of finding the Roots of a Real Function $f$, i.e. the values of $x$
such that:
\[
  f(x) = 0
\]
For Equations that cannot be solved Analytically, the Numerical Method involves
defining a Dynamical System with \emph{Convergent Sequences} which approach
Solutions to the Equation:
\begin{itemize}
  \item Iterative Rootfinding Methods (\S\ref{sec:iterative_rootfinding})
  \item Newton's Method (\S\ref{sec:newtons_method})
\end{itemize}



\paragraph{Squeeze Theorem}\label{sec:squeeze_theorem}\hfill

For Sequences $a_n \leq b_n \leq c_n$ and $\lim a_n = l$ and $\lim c_n
= l$, then $\lim b_n = l$



\subsubsection{Divergent Sequence}\label{sec:divergent_sequence}



% --------------------------------------------------------------------
\subsection{Bounded Sequence}\label{sec:bounded_sequence}
% --------------------------------------------------------------------

If $a_n$ is Convergent (\S\ref{sec:convergent_sequence}) then it is
Bounded.



\subsubsection{Bolzano-Weierstrass Theorem}\label{sec:bolzano_weierstrass}

\emph{Sequential Compactness Theorem}

Lemma: Every Sequence $a_n$ of Real Numbers has a Monotone
Subsequence

The \emph{Bolzano-Weierstrass Theorem} states that a Bounded Sequence
$a_n$ has at least one Subsequence that Converges.



% --------------------------------------------------------------------
\subsection{Arithmetic Sequence}\label{sec:arithmetic_sequence}
% --------------------------------------------------------------------

(or \emph{Arithmetic Progression})

Arithmetic Series (\S\ref{sec:arithmetic_series})



% --------------------------------------------------------------------
\subsection{Geometric Sequence}\label{sec:geometric_sequence}
% --------------------------------------------------------------------

(or \emph{Geometric Progression})

Sum of Terms of a Geometric Sequence form a Geometric Series
(\S\ref{sec:geometric_series})



% --------------------------------------------------------------------
\subsection{Infinite Sequence}\label{sec:infinite_sequence}
% --------------------------------------------------------------------

\emph{Infinite Sequences} may be \emph{Singly Infinite}
(\S\ref{sec:singly_infinite}), having an initial Element but no final
Element, or \emph{Doubly Infinite} (\S\ref{sec:doubly_infinite})
having neither a first nor a last Element.



\subsubsection{Singly Infinite Sequence}\label{sec:singly_infinite}

A \emph{Singly Infinite Sequence} (or \emph{One-sided Infinite
  Sequence}) can be defined as a Function, $s$, with a Countably
Infinite Totally Ordered Set of Indices, $X$, for its Domain and a Set
of Elements, $Y$, for the Codomain:

  $s : X \rightarrow Y$ \\
where:

  $X = \{1,2,\ldots,n\}$

  $Y = \{a_1, a_2,\ldots,a_n\}$

  $s = \{(1,a_1), (2,a_2),\ldots, (n,a_n)\}$ \\
for some Countable $n \geq 0$.

Singly Infinite Sequences may be interpreted as Elements of the
Semigroup Ring of the Natural Numbers, $R[\mathbb{N}]$
(\S\ref{sec:group_ring}).



\subsubsection{Doubly Infinite Sequence}\label{sec:doubly_infinite}

A \emph{Doubly Infinite Sequence} (also \emph{Two-way Infinite} or
\emph{Bi-infinite Sequence}) may be defined as a Function from the Set
of all Integers $\mathbb{Z}$ into a Set, denoted
$(2n)^{\infty}_{n=-\infty}$.

Doubly Infinite Sequences may be interpreted as Elements of the Group
Ring of the Integers, $R[\mathbb{Z}]$ (\S\ref{sec:group_ring}.



% --------------------------------------------------------------------
\subsection{Monotone Sequence}\label{sec:monotone_sequence}
% --------------------------------------------------------------------

Monotone Function (\S\ref{sec:monotonic_function})

Increasing Sequence: $\forall n \in \nats, a_n \leq a_{n+1}$

An Increasing Sequence is Bounded above if and only if it is
Convergent.

Decreasing Sequence: $\forall n \in \nats, a_n \geq a_{n+1}$

A Decreasing Sequence is Bounded below if and only if it is
Convergent.



% --------------------------------------------------------------------
\subsection{Cauchy Sequence}\label{sec:cauchy_sequence}
% --------------------------------------------------------------------

Complete Metric Space (\S\ref{sec:complete_metric_space})



% --------------------------------------------------------------------
\subsection{Oscillation}\label{sec:oscillation}
% --------------------------------------------------------------------



% ====================================================================
\section{Summation}\label{sec:summation}
% ====================================================================

\emph{Finite Summation}

$\sum$

Discrete equivalent of \emph{(Definite) Integration} (\S\ref{sec:integral})

\fist cf. \emph{Indefinite Sums} (Antidifferences \S\ref{sec:antidifference}) --
Discrete equivalent of \emph{Indefinite Integration} (Antiderivatives
\S\ref{sec:antiderivative})

Summations and Integrals can be ``put on the same foundations'' using the Theory
of Time-scale Calculus (\S\ref{sec:timescale_calculus})



% --------------------------------------------------------------------
\subsection{Series}\label{sec:series}
% --------------------------------------------------------------------

\emph{Infinite Series} -- an Infinite Summation

Sum of Terms of a Sequence (\S\ref{sec:sequence}) $a_n : \nats
\rightarrow \reals$

Transcendental Numbers (\S\ref{sec:transcendental})

some examples:
\begin{itemize}
  \item $\frac{1}{4} - \frac{1}{16} + \frac{1}{64} - \frac{1}{256} \cdots
    = \frac{1}{4}$
  \item $\frac{1}{2} - \frac{1}{4} + \frac{1}{8} - \frac{1}{16} \cdots
    = \frac{1}{3}$
  \item $1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} \cdots  = \ln{2}$
  \item $1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} \cdots  = \frac{\pi}{4}$
  \item $1 + \frac{1}{4} + \frac{1}{9} + \frac{1}{16} \cdots = \frac{\pi^2}{6}$
  \item $\sum_{n=0}^\infty \frac{x^n}{n!} = e^x$
\end{itemize}



\subsubsection{Partial Sum}\label{sec:partial_sum}

Sequence $\{ a_1, a_2, a_3, \ldots \}$

$S_n = \sum_{k=1}^n a_k$



\subsubsection{Convergent Series}\label{sec:convergent_series}

Limit (\S\ref{sec:sequence_limit}) of Partial Sums $\{ S_1, S_2, S_3,
\ldots \}$ Converges (\S\ref{sec:convergent_sequence})



\subsubsection{Divergent Series}\label{sec:divergent_series}

\paragraph{Abelian Mean}\label{sec:abelian_mean}\hfill

\paragraph{Abel Summation}\label{sec:abel_summation}\hfill

Analytic Number Theory (\S\ref{sec:analytic_number_theory})

$a_n$ Sequence of Complex Numbers

$f(t)$ Differentiable Function (\S\ref{sec:differentiable_function})

$A(x) = \sum_{n \leq x} a_n$

$\sum_{n \leq x} a_n f(n) = A(x)f(x) - \int_1^x A(t)f'(t) dt$



% --------------------------------------------------------------------
\subsection{Arithmetic Series}\label{sec:arithmetic_series}
% --------------------------------------------------------------------

Arithmetic Progression (Arithmetic Sequence
\S\ref{sec:arithmetic_sequence})



% --------------------------------------------------------------------
\subsection{Geometric Series}\label{sec:geometric_series}
% --------------------------------------------------------------------

Constant Ratio between successive Terms

Terms form a Geometric Progression (Geometric Sequence
\S\ref{sec:geometric_sequence})

Sum Converges as long as Absolute Value of the Ratio of Terms is less
than $1$



\subsubsection{Hypergeometric Series}\label{sec:hypergeometric_series}

\paragraph{Hypergeometric Function}\label{sec:hypergeometric_function}\hfill

is a Solution of a Second-order Linear ODE
(\S\ref{sec:linear_differential_equation})

Hypergeometric Series (\S\ref{sec:hypergeometric_series})

Strict Subset of the Holonomic Functions (\S\ref{sec:holonomic_function})



\subparagraph{Generalized Hypergeometric Function}
\label{sec:generalized_hypergeometric_function}\hfill



% --------------------------------------------------------------------
\subsection{Harmonic Series}\label{sec:harmonic_series}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Alternating Series}\label{sec:alternating_series}
% --------------------------------------------------------------------

\[
  \sum_{n=0}^\infty (-1)^n a_n
\]
or:
\[
  \sum_{n=0}^\infty (-1)^{n-1} a_n
\]



% --------------------------------------------------------------------
\subsection{Telescoping Series}\label{sec:telescoping_series}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Power Series}\label{sec:power_series}
% --------------------------------------------------------------------

Infinite Series of the form:
\[
  f(x) = \sum_{n=0}^\infty a_n (x - c)^n
\]

\fist Formal Power Series (\S\ref{sec:formal_power_series})

\emph{Order}



\subsubsection{Taylor Series}\label{sec:taylor_series}

Centered at $0$: \emph{Maclaurin Series}

Polynomial formed by some initial Terms of a Taylor Series: Taylor
Polynomial (\S\ref{sec:taylor_polynomial}); Taylor Series is the Limit
(\S\ref{sec:sequence_limit}) of the Taylor Polynomials with increasing
Degree.

Note that a Function may not be equal to its Taylor Series even if its
Taylor Series Converges at every Point.

A Function is \emph{Analytic} (\S\ref{sec:analytic_function}) if and only if
its Taylor Series about $x_0$ Converges to the Function in some Neighborhood
for every $x_0$ in its Domain.

A Function that is equal to its Taylor Series in an Open Interval
(\S\ref{sec:interval}, or Disc \S\ref{sec:disc}) is an Analytic Function in
that Interval.

Maclaurin Series:
\[
  \sum_{n=0}^\infty f^{(n)}(0) \frac{x^n}{n!}
\]

truncated Taylor Series are used in the following Explicit Integrators
(\S\ref{sec:explicit_integration}):
\begin{itemize}
  \item Euler Integration (First-order \S\ref{sec:euler_method})
  \item (Explicit) Midpoint Integration (Second-order
    \S\ref{sec:midpoint_method})
  \item Runge-Kutta (Higher-orders \S\ref{sec:runge_kutta})
\end{itemize}



\paragraph{Binomial Series}\label{sec:binomial_series}\hfill



% --------------------------------------------------------------------
\subsection{General Dirichlet Series}\label{sec:general_dirichlet}
% --------------------------------------------------------------------

%FIXME: move to subsection?



\subsubsection{Dirichlet Series}\label{sec:dirichlet_series}

the Riemann Zeta Function (\S\ref{sec:riemann_zeta}) is the Analytic
Continuation of the Dirichlet Series:
\[
  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}
\]
for $s$ with Real part $\leq 1$



% --------------------------------------------------------------------
\subsection{Generating Function}\label{sec:generating_function}
% --------------------------------------------------------------------

cf. Riemann Zeta Function (\S\ref{sec:riemann_zeta})

\fist Weil Conjectures (\S\ref{sec:weil_conjectures})



% ====================================================================
\section{Sequence Product}\label{sec:sequence_product}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Infinite Product}\label{sec:infinite_product}
% --------------------------------------------------------------------

Converges when the Sequence converges to $1$

$\prod_{n=1}^\infty a_n$ Converges if and only if $\sum_{n=1}^\infty
ln(a_n)$ Converges

$q_n = (1 + u_1)(1 + u_2)\cdots(1 + u_n)$ Converges if and only if
$\sum u_n$ Converges.



% ====================================================================
\section{Continued Fraction}\label{sec:continued_fraction}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Infinite Continued Fraction}\label{sec:infinite_continued_fraction}
% --------------------------------------------------------------------



% ====================================================================
\section{Analytic Expression}\label{sec:analytic_expression}
% ====================================================================

An \emph{Analytic Expression} allows the following kinds of Expressions:
\begin{itemize}
  \item Convergent Infinite Sums (\emph{Series} \S\ref{sec:series})
  \item Convergent Infinite Products (\S\ref{sec:infinite_product})
  \item Convergent Infinite Continued Fractions
    (\S\ref{sec:infinite_continued_fraction})
  \item Gamma Functions (\S\ref{sec:gamma_function}): extension of the Factorial
    Function to Real and Complex Numbers
  \item Bessel Functions (\S\ref{sec:bessel_function}): canonical Solutions
    $y(x)$ of \emph{Bessel's Differential Equation}; special cases are called
    \emph{Cylinder Functions} (\S\ref{sec:cylinder_function})
\end{itemize}
in addition to \emph{Closed-form Expressions}
(\S\ref{sec:closed_form_expression}), which are a more restricted class of
Mathematical Expressions that can be Evaluated in a Finite number of Operations.

Analytic Expressions \emph{exclude} Expressions for:
\begin{itemize}
  \item Limits (\S\ref{sec:sequence_limit})
  \item Differentials (\S\ref{sec:differential}), Integrals
    (\S\ref{sec:integral})
  \item Formal Power Series (\S\ref{sec:formal_power_series})
\end{itemize}

\fist cf. an \emph{Analytic Function} (\S\ref{sec:analytic_function}) is a
Smooth Function for which the Taylor Series (\S\ref{sec:taylor_series}) about
every $x_0$ in its Domain Converges to the Function in some Neighborhood



% ====================================================================
\section{Asymptotic Analysis}\label{sec:asymptotic_analysis}
% ====================================================================

%FIXME move sequence, limit, etc. here ?

\fist cf. Asymptotic Expansion (Formal Power Series
\S\ref{sec:asymptotic_expansion})



% --------------------------------------------------------------------
\subsection{Singularity}\label{sec:singularity}
% --------------------------------------------------------------------

\begin{itemize}
  \item Pole (Complex Analysis \S\ref{sec:complex_pole})
\end{itemize}



% ====================================================================
\section{Real Analysis}\label{sec:real_analysis}
% ====================================================================

$R^1$ -- Real Line (\S\ref{sec:real_line}): 1-dimensional Real
Coordinate Space (\S\ref{sec:real_coordinate_space})



% --------------------------------------------------------------------
\subsection{Real Interval}\label{sec:real_interval}
% --------------------------------------------------------------------

Interval (\S\ref{sec:interval})

Interval Arithmetic (\S\ref{sec:interval_arithmetic})

$[a,b] = \bigcap_n (a - \frac{1}{n}, b + \frac{1}{n})$

the Real Unit Interval $[0,1]$ is a One-dimensional Analytic Manifold
(\S\ref{sec:analytic_manifold}) and can be interpreted as a generalization of
the Boolean Domain $\{0, 1\}$ (Fuzzy Logic \S\ref{sec:fuzzy_logic})

the Real Closed Interval (Unit 0-disc) $[-1,1]$ is a Commutative Semigroup
(\S\ref{sec:commutative_semigroup}) under Multiplication

Taylor06 - \emph{Interval Analysis Without Intervals}



\subsubsection{Interval Partition}\label{sec:interval_partition}

Closed Interval $[a,b]$

Finite Sequence (\S\ref{sec:sequence}) $(x_i) = \{ x_0, x_1, \ldots,
x_n \}$

$a = x_0 < x_1 < x_2 < \ldots < x_n = b$



% --------------------------------------------------------------------
\subsection{Real-valued Function}\label{sec:real_valued}
% --------------------------------------------------------------------

$f : X \rightarrow \reals$



\subsubsection{Subadditive Set Function}\label{sec:subadditive_set_function}

cf. Subadditive Function (Arithmetic \S\ref{sec:subadditive_function})

every Non-negative Submodular Function (\S\ref{sec:submodular_function}) is
Subadditive



\subsubsection{Submodular Set Function}\label{sec:submodular_set_function}

every Non-negative Submodular Function is Subadditive
(\S\ref{sec:subadditive_set_function})

(Rosenfeld-Balkanski-Globerson-Singer18 - \emph{Learning to Optimize
  Combinatorial Functions}) Machine Learning; \emph{Adaptive Sampling} --
alternative critereon for Optimizing general Combinatorial Functions from
Sampled Data



\subsubsection{Supermodular Set Function}\label{sec:submodular_set_function}

as a Utility Function (\S\ref{sec:utility_function}) -- Complementary Goods



% --------------------------------------------------------------------
\subsection{Real Function}\label{sec:real_function}
% --------------------------------------------------------------------

A \emph{Real Function} is a \emph{Real-valued Function of a Real Variable}
$f : \reals \rightarrow \reals$

ordinary Derivative (\S\ref{sec:derivative})

\fist cf. Real Multivariate Functions (\S\ref{sec:real_multivariate_function})
$f : \reals^m \rightarrow \reals$,
Vector-valued Functions (\S\ref{sec:vector_function}) $f : \reals \rightarrow
\reals^n$

\fist Root-finding Algorithms (\S\ref{sec:root_finding})



\subsubsection{Bounded Function}\label{sec:bounded_function}

a Bounded Function on a Compact Interval $[a,b]$ is Riemann Integrable
(\S\ref{sec:integrable_function}) if and only if it is Continuous
(\S\ref{sec:continuous_function}) ``Almost Everywhere'', i.e. Set of Points of
Discontinuity has Lebesgue Measure Zero (\S\ref{sec:lebesgue_measure})



\subsubsection{Function Limit}\label{sec:function_limit}

Limit Point (\S\ref{sec:limit_point}) of $D \in \reals$ is $a \in D$
such that:
\[
  \exists a_n \in D : a_n \neq a \wedge \lim a_n = a
\]

Function $f$ has a \emph{Limit} $l$ at $a$ if for all Sequences $a_n
\in D$ with $\lim a_n = a$ and $a_n \neq a$ for all $n$, $\lim f(a_n)
= l$



\subsubsection{Convex Function}\label{sec:convex_function}

\fist Convex Geometry (\S\ref{sec:convex_geometry})

\fist Convex Optimization (\S\ref{sec:convex_optimization}) -- Convex Functions
have the property that a Local Minimum is necessarily a Global Minimum, i.e.
Local Optimality Conditions (\S\ref{sec:local_optimality}) are \emph{Global}
Optimality Conditions

for $f : X \rightarrow \reals$ where $X$ is a Convex Set
(\S\ref{sec:convex_set}) in a Real Vector Space $\reals^m$, $f$ is
\emph{Convex} when $\forall x_1, x_2 \in X, \forall t \in [0,1]$:
\[
  f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2)
\]
and \emph{Strictly Convex} when $\forall x_1 \neq x_2 \in X, \forall t \in
(0,1)$:
\[
  f(tx_1 + (1-t)x_2) < tf(x_1) + (1-t)f(x_2)
\]
where $tx_1 + (1-t)x_2$ is called a \emph{Convex Combination}
(\S\ref{sec:convex_combination}) of $x$ and $y$, and likewise $tf(x_1) +
(1-t)f(x_2)$ is a Convex Combination of $f(x_1)$ and $f(x_2)$

this means that the line segment connecting $f(x_1)$ and $f(x_2)$ is above the
curve $f(tx_1 + (1-t)x_2)$ at each point

a Function is Convex if and only if its Epigraph is a Convex Set

FIXME: relation to Second Derivative ???

Operations that conserve Convexity:
\begin{itemize}
  \item Multiplication by non-negative Scalar
  \item Addition of Convex Functions (extends to Infinite Sums, Integrals)
  \item Pre-composition with an affine Function: $f(Ax + b)$
  \item Pointwise Maximum, Pointwise Supremum of Convex Functions
  \item Composition rules: (can be derived from Vector Composition Rule and
    Affine Precomposition Rule ... TODO)
\end{itemize}

examples:
\begin{itemize}
  \item Affine (Linear) Functions $ax + b$ -- both Convex and Concave
  \item general Affine (Linear) Functions $a^T x + b$ -- both Convex and Concave
  \item Affine Functions on Matrices, i.e. Inner Product on Matrices
  \item Quadratic Function $x^2$ \fist Quadratic Forms
    (\S\ref{sec:quadratic_form})
  \item Exponential Function $e^x$
  \item Norms (\S\ref{sec:norm}), Spectral Norm (\S\ref{sec:spectral_norm})
  \item the Function for the maximum Eigenvalue of a Symmetric Matrix $X \in
    \mathsf{S}^n$:
    \[
      \lambda_{max}(X) = \mathrm{sup}_{\ |\vec{y}\|_2=1} \vec{y}^T X \vec{y}
    \]
  \item $exp (g(x)$ of a Convex Function $g$
\end{itemize}



\subsubsection{Concave Function}\label{sec:concave_function}

\fist von Neumann Entropy (\S\ref{sec:vonneumann_entropy})



\subsubsection{Level Set}\label{sec:level_set}

a special case of a Fiber (\S\ref{sec:fiber})



% --------------------------------------------------------------------
\subsection{Real-valued Continuous Function}\label{sec:real_continuous}
% --------------------------------------------------------------------

$f : D \subseteq \reals \rightarrow \reals$

Continuous at $l \in D$ if for every Sequence $a_n \in D$ such that
$\lim a_n = l$, then $\lim f(a_n) = f(l)$

$\lim f (a_n) = f (\lim a_n)$

Equivalently: Continuous at $l$ if and only if:
\[
  \forall \varepsilon > 0, \exists \delta :
  |x - l| < \delta \Rightarrow |f(x) - f(l)| < \varepsilon
\]

Equivalently: Continuous at $a$ if and only if $f$ has Limit
(\S\ref{sec:function_limit}) $f(a)$ at $a$: $\lim_{x \rightarrow
  a}f(x) = f(a)$

Continuous on an Interval if and only if the Range on that Interval is
also a single Interval

For $f,g$ Continuous on $D$ at $a \in D$, then $(f + g)$, $f g$,
$\frac{f}{g}$ (when $g(a) \neq 0$) are Continuous at $a$.

For $g$ Defined on the Range of $f$, $\{ f(x); x \in D\}$, if $f$ is
Continuous at $a \in D$ and $g$ Continuous at $f(a)$, then $g \circ f$
is Continuous at $a$: $\lim g(f(a_n)) = g(f(a))$

Differentiable (\S\ref{sec:differentiable_function}) at $a$ Implies
Continuous at $a$

Continuously Differentiable $\subseteq$ Lipschitz Continuous $\subseteq$
$\alpha$-H\"older Continuous $\subseteq$ Uniformly Continuous = Continuous



\subsubsection{Local Extrema}\label{sec:local_extrema}

Local Maximum

Local Minimum

for Differentiable (\S\ref{sec:differentiable_function}) $f$ on an
Open Interval: $f'(a) = 0$ at Local Extrema

\fist Second Derivative Test (\ref{sec:second_derivative_test})



\subsubsection{Intermediate Value Theorem}
\label{sec:intermediate_value}

For Function $f(x)$ Continuous on Closed Interval $[a,b]$, for any
$f(a) < c < f(b)$, there is a $d \in (a,b)$ such that $f(d) = c$.



\subsubsection{Extreme Value Theorem}\label{sec:extreme_value}

\subsubsection{Modulus of Continuity}\label{sec:continuity_modulus}



% --------------------------------------------------------------------
\subsection{Newton's Expansion}\label{sec:newtons_expansion}
% --------------------------------------------------------------------

% FIXME

$(1 + a)^n$ % ???



% --------------------------------------------------------------------
\subsection{Bernoulli's Inequality}\label{sec:bernoullis_inequality}
% --------------------------------------------------------------------

$n \in \nats$, $a \in \reals^+$, then:
\[
  (1 + a)^n \geq 1 + n a
\]


% --------------------------------------------------------------------
\subsection{Logistic Function}\label{sec:logistic_function}
% --------------------------------------------------------------------

%FIXME: move this section

Sigmoid Curve:
\[
  f(x) = \frac{L}{1 + e^{-k(x-x_0)}}
\]
where $L$ controls the curve's Maximum and $k$ is the steepness of the curve

\fist Continuous version of (Discrete) Logistic Map (\S\ref{sec:logistic_map})

\emph{Logistic Differential Equation} -- the Logistic Function is the Solution
to the First-order Non-linear Ordinary Differential Equation:
\[
  \frac{d}{dx}f(x) = f(x)(1 - f(x))
\]
with Boundary Condition (\S\ref{sec:boundary_value_problem}) $f(0) =
\frac{1}{2}$

\fist generalization: Softmax Function (\S\ref{sec:softmax}); the Logistic
Function is the Derivative of Softplus --TODO



\subsubsection{Sigmoid Function}\label{sec:sigmoid_function}



% ====================================================================
\section{Complex Analysis}\label{sec:complex_analysis}
% ====================================================================

For the Complex Numbers (\S\ref{sec:complex_number}), the notions of
Analyticity, Holomorphy, Harmonicity, and Conformality \emph{coincide}; for the
Reals (\S\ref{sec:real_analysis}), and Quaternions
(\S\ref{sec:quaternionic_analysis}), not all notions are the same.

Finite-dimensional Associative Division Algebra
(\S\ref{sec:associative_division_algebra})

\fist Algebraic Analysis (\S\ref{sec:algebraic_analysis}) -- study of Systems of
Linear PDEs (\S\ref{sec:linear_pde_system}) using Sheaf Theory
(\S\ref{sec:sheaf_theory}) and Complex Analysis (\S\ref{sec:complex_analysis})

Commutative

Complex Differentiable Functions are automatically Analytic %FIXME

a Laplace Transform (\S\ref{sec:laplace_transform}) transforms a Function from
the Time Domain to the Complex Frequency Domain so it can be analyzed in the
Complex Plane

Complex Solutions to the Differential Equation $y'' + y = 0$

\emph{Euler's Formula}:
\[
  e^{ix} = \cos x + i \sin x
\]
\emph{Euler's Identity}:
\[
  e^{i\pi} + 1 = 0
\]

Convergence Sequences (\S\ref{sec:convergent_sequence}): Intervals
for Convergent Real Sequences are replaced by \emph{Discs} in the case of
Convergent Sequences of Complex Numbers

all Trigonometric Functions are related to $e^z$:
\[
  e^{iz} = \cos z + i \sin z
\]
as expressed in the Polar Form:
\[
  re^{i\theta}
\]


\emph{Weierstrass Approximation Theorem}\S\ref{sec:weierstrass_approximation}

any given Continuous Complex-Valued Function defined on a Closed Interval
$[a,b]$ can be Uniformly Approximated as closely as desired by a Polynomial
Function (\S\ref{sec:polynomial_function})



% --------------------------------------------------------------------
\subsection{Principal Value}\label{sec:principal_value}
% --------------------------------------------------------------------

Argument (Angle) -- usually restricted to the Interval $(-\pi,\pi]$, or else
$[0,2\pi)$ by adding $2\pi$ if the Value is Negative



% --------------------------------------------------------------------
\subsection{Complex Function}\label{sec:complex_function}
% --------------------------------------------------------------------

$f$

usual Limit Theorems hold

for a Complex Function to have a Derivative, the Directional Derivative must
exist in every direction and be the same

Integrating Complex Functions can be done by Line Integrals
(\S\ref{sec:line_integral}):
\[
  \int_{C} f(z) dz = \int_{t_0}^{t_1} f(z(t)) z'(t) dt
\]

if the Function $f(z)$ is Analytic on the enclosed Path, then the Line Integral
is Path Independent; cf. Exact Differentials (\S\ref{sec:exact_differential})

if $f$ is Analytic on and between Curves $C_1$ and $C_2$, then:
\[
  \oint_{c_1} f(z) dz = \oint_{c_2} f(z) dz
\]



\subsubsection{Zero}\label{sec:complex_zero}

\subsubsection{Pole}\label{sec:complex_pole}

Singularity (\S\ref{sec:singularity})

a Meromorphic Function (\S\ref{sec:meromorphic_function}) Holomorphic
(\S\ref{sec:holomorphic_function}) on all of Domain except for a Set of
Isolated Points which are Poles of the Function



\subsubsection{Riemann Zeta Function}\label{sec:riemann_zeta}

Analytic Continuation of the Sum of the Dirichlet Series
(\S\ref{sec:dirichlet_series}) as a Function of a Complex Variable $s$:
\[
  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}
\]
when the Real part of $s$ is $\leq 1$

\fist cf. Generating Functions (\S\ref{sec:generating_function})

\emph{Riemann Hypothesis}: the Riemann Zeta Function has Zeros only at Negative
Even Integers and Complex Numbers with Real part $\frac{1}{2}$



% --------------------------------------------------------------------
\subsection{Euler's Formula}\label{sec:eulers_formula}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Complex Frequency}\label{sec:complex_frequency}
% --------------------------------------------------------------------

the Imaginary Component $\omega$ of a Complex Frequency $s = -\sigma + i\omega$
corresponds to the ``usual'' concept of Frequency (i.e. the rate at which the
Sinusoid Cycles), and the Real Component $\sigma$ corresponds to the degree of
\emph{Damping} (i.e. an Exponential Decrease of Amplitude)

Complex Function of Frequency (as in Laplace or Fourier Transforms): the
Component of a Signal at any given Frequency is given by a Complex Number, the
Magnitude of which is the Amplitude of that Component and the Angle of which is
the relative Phase of the Wave (FIXME: clarify)

the response of a System as a Function of Frequency can also be described by a
Complex Function (FIXME: clarify)



% --------------------------------------------------------------------
\subsection{Complex Surface}\label{sec:complex_surface}
% --------------------------------------------------------------------

\subsubsection{Enriques-Kodaira Classification}
\label{sec:enriques_kodaira}

\subsubsection{Complex Plane}\label{sec:complex_plane}

or \emph{Argand Plane}

\emph{Fundamental Theorem of Algebra link between ``Algebra'' and
  ``Geometry''}: a Monic Polynomial (Univariate with Complex Coefficients
\S\ref{sec:monic_polynomial}), an Algebraic Object, is determined by the Set of
its Roots (\S\ref{sec:function_root}), a Geometric Object, in the Complex
Plane.



\subsubsection{Disc}\label{sec:disc}

\subsubsection{Extended Complex Plane}\label{sec:extended_complex_plane}

Complex Plane with Point at Infinity

Stereographic Projection (\S\ref{sec:stereographic_projection})

Riemann Sphere (\S\ref{sec:riemann_sphere})



% --------------------------------------------------------------------
\subsection{Holomorphic Function}\label{sec:holomorphic_function}
% --------------------------------------------------------------------

or ``\emph{Complex Differentiable}''

cf. Differentiable Function (\S\ref{sec:differentiable_function})

Complex-valued Function of one or more Complex Variables that is
Complex-differentiable in a Neighborhood (\S\ref{sec:neighborhood}) of
every Point in its Domain. Implies that any Holomorphic Function is
Infinitely Differentiable (\S\ref{sec:smooth_function}) and equal to
its own Taylor Series (\S\ref{sec:taylor_series}).

a Complex Function is Analytic (\S\ref{sec:analytic_function}) if and only if
it is Holomorphic, i.e. it is ``Complex Differentiable'':
\begin{align*}
  f(z)  & = u + iv \\
  f'(z) & = u_x + iv_x
\end{align*}
with the \emph{Cauchy-Riemann Conditions}:
\[
  u_x = v_y \text{ and } u_y = - v_x
\]

Real or Imaginary part of any Holomorphic Function is a Harmonic
Function (\S\ref{sec:harmonic_function})

a Regular Map (\S\ref{sec:regular_map}) between Complex Algebraic Varieties is
a Holomorphic Map


\begin{itemize}
  \item the Laplace Transform (\S\ref{sec:laplace_transform}) is a Holomorphic
    Function of the Variable $s$ with a Power Series representation
\end{itemize}


\subsubsection{Cauchy-Riemann Conditions}\label{sec:cauchy_riemann}

necessary and sufficient condition for a Complex Function to be Holomorphic
(Complex Differentiable)

\[
  u_x = v_y \text{ and } u_y = - v_x
\]



% --------------------------------------------------------------------
\subsection{Meromorphic Function}\label{sec:meromorphic_function}
% --------------------------------------------------------------------

Holomorphic on all of Domain except for a Set of Isolated Points which are
Poles (\S\ref{sec:complex_pole}) of the Function

every Meromorphic Function on $D$ can be expressed as the ratio between two
Holomorphic Functions defined on $D$ and any Pole must correspond to a Zero in
the Denominator

since the Poles of a Meromorphic Function are \emph{Isolated}, there are at
most Countably many

if $D$ is Connected, the Meromorphic Functions form a Field Extension of the
Complex Numbers



\subsubsection{Elliptic Function}\label{sec:elliptic_function}

Meomorphic Function that is Periodic in two directions

cannot be Holomorphic

Double-periodic Functions (\S\ref{sec:double_periodic})

Pendulum (Non-linear Dynamical System \S\ref{sec:nonlinear_dynamical_system})



\subsubsection{$L$-function}\label{sec:l_function}

\subsubsection{Gamma Function}\label{sec:gamma_function}

$\Gamma(\alpha) = \int_0^{\infty} x^{\alpha -1} e^{-x} dx$

for $\alpha > 0$

\begin{enumerate}
\item $\Gamma(\alpha) = (\alpha - 1) \Gamma(\alpha -1)$
\item $\Gamma(n) = (n-1)!$
\item $\Gamma(1) = 1$
\item $\Gamma(\sfrac{1}{2}) = \sqrt{pi}$
\end{enumerate}

Gamma Distribution (\S\ref{sec:gamma_distribution})



% --------------------------------------------------------------------
\subsection{Conformal Map}\label{sec:conformal_map}
% --------------------------------------------------------------------

an Invertible Mapping (FIXME: bijective ???)

Locally Angle-preserving

if $f = u + iv$ is Analytic and $f'(z) \neq 0$, then:
\begin{align*}
  u & = u(x,y) \\
  v & = v(x,y)
\end{align*}
is \emph{Conformal}

the Real and Imaginary parts of any Analytic Function define a \emph{Conformal
  Mapping} (FIXME: clarify)

\begin{align*}
  u & = e^x \cos y \\
  v & = e^x \sin y
\end{align*}
is a \emph{Real Conformal Mapping} (FIXME: explain)

a Conformal Mapping preserves Laplace's Equation
(\S\ref{sec:laplaces_equation})-- only changes the Laplacian by a non-negative
factor



% --------------------------------------------------------------------
\subsection{Univalent Function}\label{sec:univalent_function}
% --------------------------------------------------------------------

Injective Holomorphic Function on an Open Subset of the Complex Plane



% --------------------------------------------------------------------
\subsection{Modular Form}\label{sec:modular_form}
% --------------------------------------------------------------------

Automorphic Form (\S\ref{sec:automorphic_form})



% --------------------------------------------------------------------
\subsection{Radical}\label{sec:radical}
% --------------------------------------------------------------------

\emph{Radical}

\emph{$n$th-root}

Algebraic Expressions (\S\ref{sec:algebraic_expression})



\subsubsection{Root of Unity}\label{sec:unity_root}

a Complex Number that gives $1$ when raised to some Positive Integer Power $n$

the Center (\S\ref{sec:group_center}) of $SL(n,F)$ is the Set of all Scalar
Matrices (\S\ref{sec:scalar_matrix}) with Unit Determinant and is Isomorphic to
the Group of $n$th Roots of Unity of the Field $F$



% --------------------------------------------------------------------
\subsection{Riemann Surface}\label{sec:riemann_surface}
% --------------------------------------------------------------------

One-dimensional Complex Manifold (\S\ref{sec:complex_manifold})

the Geometry of Riemann Surfaces is given by Two-dimensional Conformal
Geometry (\S\ref{sec:conformal_geometry})

there are three kinds of Simply-connected Riemann Surfaces, up to Conformal
Isomorphism:
\begin{itemize}
  \item Complex Plane
  \item Riemann Sphere
  \item Unit Disk or Hyperbolic Semiplane
\end{itemize}



\subsubsection{Riemann Mapping Theorem}
\label{sec:riemann_mapping_theorem}

\subsubsection{Riemann Sphere}\label{sec:riemann_sphere}

``simplest'' Riemann Surface

Model of the Extended Complex Plane
(\S\ref{sec:extended_complex_plane})

can be thought of as the \emph{Complex Projective Line}
$\mathbb{CP}^1$: the Projective Space (\S\ref{sec:projective_space} of
all Complex Lines in $\comps^2$

an example of an Algebraic Manifold (\S\ref{sec:algebraic_manifold})



\subsubsection{Bolza Surface}\label{sec:bolza_surface}

\subsubsection{Fundamental Polygon}\label{sec:fundamental_polygon}



% --------------------------------------------------------------------
\subsection{Countour Integration}\label{sec:contour_integration}
% --------------------------------------------------------------------

Residue Calculus (???) %FIXME



% ====================================================================
\section{Quaternionic Analysis}\label{sec:quaternionic_analysis}
% ====================================================================

the Quaternion Conjugate is Analytic everywhere in $\quats$
(note the Complex Conjugate is \emph{not} Analytic in $\comps$)

Homographies (Projective Transformation\S\ref{sec:projective_transformation}):
Screw Displacements ($SE(3)$ \S\ref{sec:special_euclidean})



% --------------------------------------------------------------------
\subsection{Quaternion Function}\label{sec:quaternion_function}
% --------------------------------------------------------------------

Maxwell's Equations

Affine Transformations of Quaternions have the form:
\[
  f(q) = aq + b, \;\;\; a,b,q \in \quats
\]

Derivation of Quaternion Functions requires a Direction-dependent Derivative
(FIXME: clarify)



% ====================================================================
\section{Harmonic Analysis}\label{sec:harmonic_analysis}
% ====================================================================

2016 - Laba, Falconer - \emph{Harmonic Analysis and Additive Combinatorics on
  Fractals}



% --------------------------------------------------------------------
\subsection{Potential Theory}\label{sec:potential_theory}
% --------------------------------------------------------------------

\subsubsection{Harmonic Function}\label{sec:harmonic_function}

Twice-continuously Differentiable Function
(\S\ref{sec:continuously_differentiable}) $f : U \rightarrow Reals$,
where $U$ is an Open Subset of $\reals^n$, satisfying \emph{Laplace's
  Equation} (\S\ref{sec:laplaces_equation})

Harmonic Functions are exactly those Functions that which lie in the Kernel of
the Laplace Operator (\S\ref{sec:laplace_operator})

Harmonic Functions on $\reals$ are exactly the Linear Functions
(\S\ref{sec:polynomial_function})

the Laplacian (\S\ref{sec:laplacian}) of a Harmonic Function is equal to Zero
everywhere; intuitively this means that the average value of Neighbors for a
given Point is equal to that Point

2016 - Samantha Davies - \emph{Voltage, Temperature, and Harmonic
  Functions} -
\url{https://jeremykun.com/2016/09/26/voltage-temperature-and-harmonic-functions/}

Physics: Simple Harmonic Motion is a type of ``Oscillation'' where the
Force that restores an object to its Equilibrium is directly
proportional to the Displacement

Stochastic Processes (\S\ref{sec:stochastic_process})

Potential Theory (Mathematical Physics) %FIXME create section?

the Real or Imaginary part of any Holomorphic Function
(\S\ref{sec:holomorphic_function}) is a Harmonic Function


\textbf{Mean Value Property}: the Value of a Harmonic Function at an
Interior Point is the Average of the Function's Values around the
Point

If $u$ is a Continuous Function satisfying the Mean Value Property on
a Region $\Omega$ then $u$ is Harmonic in $\Omega$

Any Function $\alpha$ on a Graph which satisfies the Mean Value
Property also satisfies the Discrete Laplacian
(\S\ref{sec:discrete_laplace})


\textbf{Maximum Principle}: a Non-constant Harmonic Function on a
Closed Bounded Region must attain Maximum and Minimum on its Boundary

Convex Optimization (\S\ref{sec:convex_optimization})


\textbf{Uniqueness}: if a Harmonic Function is Continuous on the
Boundary of a Closed Bounded Set and Harmonic in the Interior then the
Interior Values are Uniquely Determined by the Values of the Boundary


\textbf{Solution to a Dirichlet Problem}
(\S\ref{sec:dirichlet_problem})



% --------------------------------------------------------------------
\subsection{Automorphic Form}\label{sec:automorphic_form}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Fourier Analysis}\label{sec:fourier_analysis}
% --------------------------------------------------------------------

\fist Convolution (\S\ref{sec:convolution}), Cross-correlation
(\S\ref{sec:cross_correlation})



\subsubsection{Periodic Function}\label{sec:periodic_function}

a Function $f$ is \emph{Periodic} with Nonzero \emph{Period} $P$ if:
\[
  f(x+P) = f(x)
\]
for all $x$

the least positive $P$ is called the \emph{Fundamental Period}

\fist Periodic Sequence (\S\ref{sec:periodic_sequence})

Geometrically, a Periodic Function is a Function with a Graph exhibiting
\emph{Translational Symmetry} (\S\ref{sec:translation});
can be extended to Tessellations of the Plane (\S\ref{sec:tessellation})



\paragraph{Trigonometric Function}\label{sec:trigonometric_function}\hfill

or \emph{Circular Function}

Hyperbolic Functions (\S\ref{sec:hyperbolic_function}) arise from the study of
the Equation $x^2 - y^2 = 1$; cf. Circular (Trigonometric) Functions arise from
the study of $x^2 + y^2 = 1$

cf. Elliptic Functions (\S\ref{sec:elliptic_function}): Meromorphic Function
that is Periodic in two directions

Analytic Functions (\S\ref{sec:analytic_function})


the Sine Wave is the only Periodic Waveform that has the Property that it
retains its ``wave shape'' when added to another Sine Wave of the same
Frequency and arbitrary Phase and Magnitude (FIXME: clarify)

$\sin x = \sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(2n + 1)!}$

$\cos\theta= \sin(\frac{\pi}{2} - \theta)$

$\tan\theta = \frac{\sin\theta}{\cos\theta}$

$\sec\theta = \frac{1}{\cos\theta}$

$\csc\theta = \frac{1}{\sin\theta}$

$\cot\theta = \frac{\cos\theta}{\sin\theta}$

Integrals:

$\int_0^{2\pi} \sin(mt)dt = 0$ for any Integer $m$

$\int_0^{2\pi} \cos(mt)dt = 0$ for any Non-zero Integer $m$

$\int_0^{2\pi} \sin(mt)\cos(nt)dt = 0$ for any Integers $m,n$

$\int_0^{2\pi} \sin^2(mt)dt = \pi$ for any Non-zero Integer $m$

$\int_0^{2\pi} \sin(mt)\sin(nt)dt = 0$ for Integers $m,n$ such that $m\neq{n}$
or $m\neq{-n}$

$\int_0^{2\pi} \cos(mt)\cos(nt)dt = 0$ for Integers $m,n$ such that $m\neq{n}$
or $m\neq{-n}$

$\int_0^{2\pi} \cos(mt)dt = \pi$ for any Non-zero Integer $m$

\fist Fourier Series (\S\ref{sec:fourier_series})

(Laczkovich03) refinement of Richardson's Theorem
(\S\ref{sec:richardsons_theorem}); the use of $\pi$ can be removed and the use
of Composition reduced:

given an Expression $A(x)$ in the Ring generated by the Integers, $x$, $\sin
x^n$, and $\sin(x \sin x^n)$, the question whether $A(x) > 0$ for some $x$ and
whether $A(x) = 0$ for some $x$ are \emph{Unsolvable}



\paragraph{Hyperbolic Function}\label{sec:hyperbolic_function}\hfill

Hyperbolic Functions arise from the study of the Equation $x^2 - y^2 = 1$; cf.
Circular (Trigonometric) Functions arise from the study of $x^2 + y^2 = 1$

Hyperbolic Functions are Periodic with respect to the \emph{Imaginary}
Component

cf. Elliptic Functions (\S\ref{sec:elliptic_function}): Meromorphic Function
that is Periodic in two directions

$\sinh$

$\cosh$



\subsubsection{Double-periodic Function}\label{sec:double_periodic}

e.g. Elliptic Functions (\S\ref{sec:elliptic_function})



\subsubsection{Fourier Series}\label{sec:fourier_series}

representation of an arbitrary Periodic Function by a Series of weighted $cos$
and $sin$ (\S\ref{sec:trigonometric_function}) Terms

\fist Frequency Domain (\S\ref{sec:frequency_domain})

\fist cf. \emph{Fourier Transform} (\S\ref{sec:fourier_transform}) removes the
requirement of Fourier Series of being restricted to expressing functions on
Finite Intervals to being able to express functions on Infinite Intervals

(wiki):

for a Function $s(x)$ Integrable on an Interval $[x_0,x_0 + P]$ that is
Periodic outside the Interval with Period $P$ (Frequency $1/P$), then it can be
approximated on the entire Real Line as a Series of ``Harmonically related''
Sinusoidal Functions (FIXME: clarify)

the $N$th Partial Sum:
\[
  s_N(x) = \frac{a_0}{2} + \sum_{n=1}^N \left(
    a_n\cos(\frac{2\pi{nx}}{P}) + b_n\sin(\frac{2\pi{nx}}{P})
  \right)
\]
where $a_0,\ldots,a_N$ and $b_1,\ldots,b_N$ are \emph{Fourier Coefficients}
computed by:
\begin{align*}
  a_n & = \frac{2}{P}\int_{x_0}^{x_0+P}s(x) \cdot \cos(\frac{2\pi{nx}}{P}) dx \\
  b_n & = \frac{2}{P}\int_{x_0}^{x_0+P}s(x) \cdot \sin(\frac{2\pi{nx}}{P}) dx
\end{align*}

the Coefficient $\frac{a_0}{2} = \frac{1}{P}\int_{x_0}^{x_0+P}s(x)dt$
represents the \emph{Average} of $s(x)$ over the Interval $[x_0,P]$

equivalently the $N$th Partial Sum can be defined as:
\[
  s_N(x) = \sum_{n=-N}^N c_n \cdot e^i\frac{2\pi{nx}}{P}
\]
where:
\[
  c_n \defeq \begin{cases}
    \frac{1}{2}(a_n - ib_n) &\ \text{for n > 0} \\
    \frac{1}{2}a_0          &\ \text{for n = 0} \\
    c_{|n|}^*               &\ \text{for n < 0}
  \end{cases}
\]
(FIXME: $^*$ is the complex conjugate ???)

the Infinite Sum $s_\infty(x)$ is called the \emph{Fourier Series}
representation of $s(x)$

the Sines and Cosines in the Fourier Series are an example of an Orthonormal
Basis (\S\ref{sec:orthonormal_basis})

\begin{itemize}
  \item Weierstrass Function (\S\ref{sec:weierstrass_function}) -- Continuous,
    Nowhere Differentiable Function
\end{itemize}


\emph{Convolution Theorems}

Convolution (\S\ref{sec:convolution}), Cross-correlation
(\S\ref{sec:cross_correlation})



% --------------------------------------------------------------------
\subsection{Cylindrical Harmonics}\label{sec:cylindrical_harmonics}
% --------------------------------------------------------------------

\fist Cylinder Functions (\S\ref{sec:cylinder_function}); special case of Bessel
Function



% --------------------------------------------------------------------
\subsection{Spherical Harmonics}\label{sec:spherical_harmonics}
% --------------------------------------------------------------------

\fist cf. Spherical Bessel Functions (\S\ref{sec:spherical_bessel})



% ====================================================================
\section{Functional Analysis}\label{sec:functional_analysis}
% ====================================================================

Inner Product Spaces (Normed Vector Spaces
\S\ref{sec:innerproduct_space})

Hilbert Space (\S\ref{sec:hilbert_space})

Topological Vector Space (\S\ref{sec:topological_vectorspace})



% --------------------------------------------------------------------
\subsection{Functional}\label{sec:functional}
% --------------------------------------------------------------------

a mapping from a Space $X$ into the Real Numbers or sometimes into the Complex
Numbers for the purpose of establishing a Calculus-like structure on $X$

\fist cf. ``Functional'' used to mean a Higher-order Function
(\S\ref{sec:higherorder_function})

\fist cf. Linear Functional (Linear Form \S\ref{sec:linear_form})

the \emph{Action} (\S\ref{sec:trajectory_action}) of a Physical System with
Equations of Motion is a Functional taking the Trajectory of the System and
producing a Real Number; the Hamilton's Principle Function $S$ in the
Hamilton-Jacobi Equation (\S\ref{sec:hamilton_jacobi}) is equal to the
\emph{Classical Action})



% --------------------------------------------------------------------
\subsection{Functional Equation}\label{sec:functional_equation}
% --------------------------------------------------------------------

%FIXME: move this elsewhere ?

Functionals (\S\ref{sec:functional})

any Equation in which the Unknown (Variable \S\ref{sec:variable}) is a Function

e.g. an Additive Function (\S\ref{sec:additive_function}) $f$ is one satisfying
the Functional Equation $f(x + y) = f(x) + f(y)$


examples:
\begin{itemize}
  \item Schr\"oder's Equation (\S\ref{sec:schroders_equation}): $\Psi(h(x)) =
    s\Psi(x)$ where $h(x)$ is a given Function and $\Psi(x)$ is Unknown;
    Solutions specify \emph{Flow} (\S\ref{sec:flow}), a generalization of
    Function Iteration Count (\S\ref{sec:iterated_function}) to a Continuous
    Parameter
\end{itemize}



% --------------------------------------------------------------------
\subsection{Functional Integration}\label{sec:functional_integration}
% --------------------------------------------------------------------

(wiki):

Integral (\S\ref{sec:integral}) with a Domain of a Space of Functions
(\S\ref{sec:function_space})

Sums a Functional (Higher-order Function \S\ref{sec:higherorder_function})
$G[f]$ over a Continuous Range or Space of Functions $f$

most Functional Integrals cannot be Evaluated exactly and require Evaluation
using Perturbation Methods (\S\ref{sec:perturbation_method})

for each Function, the Integrand (the Function to be Integrated) returns a
``Value'' to add up (FIXME: clarify)

most Functional Integrals are Infinite, but the \emph{Quotient} of two
Functional Integrals may be Finite

exactly Solvable Functional Integrals usually begin with a Gaussian Integral
(\S\ref{sec:gaussian_integral})

cf. Dirac Delta Distribution (\S\ref{sec:dirac_delta})

Wiener Integral; assigned to a class of Brownian Motion Paths



% --------------------------------------------------------------------
\subsection{Adjoint Operator}\label{sec:adjoint_operator}
% --------------------------------------------------------------------

The \emph{Adjoint Operator} (or \emph{Hermitian Adjoint}) of a
Linear Operator $A : H_1 \rightarrow H_2$ between Hilbert Spaces
(\S\ref{sec:hilbert_space}) is the Linear Operator $A^\dag : H_2 \rightarrow
H_1$ satisfying:
\[
  \langle{Ah_1,Ah_2}\rangle_{H_2} = \langle{h_1,A^{\dag}h_2}\rangle_{H_1}
\]
where $\langle\cdot,\cdot\rangle_{H_i}$ is the Inner Product Space in Hilbert
Space $H_i$.

%FIXME: not required to be an endomorphism (operator) ???

Bounded Linear Operator (\S\ref{sec:bounded_linear_operator})

Complex Hilbert Space (\S\ref{sec:hilbert_space})



\subsubsection{Self-adjoint Operator}\label{sec:self_adjoint_operator}

A Linear Operator $A$ on a Hilbert Space (\S\ref{sec:hilbert_space}) is
\emph{Self-adjoint} if it is equal to its Hermitian Adjoint.

A Projection is Orthogonal (\S\ref{sec:orthogonal_projection}) if and only if
it is Self-adjoint.



% --------------------------------------------------------------------
\subsection{Closed Linear Span}\label{sec:closed_linear_span}
% --------------------------------------------------------------------

Linear Span (\S\ref{sec:linear_span})



% --------------------------------------------------------------------
\subsection{Sequence Space}\label{sec:sequence_space}
% --------------------------------------------------------------------

Vector Space (\S\ref{sec:vector_space}) whose Elements are Infinite Sequences
(\S\ref{sec:sequence}) of Real or Complex Numbers, or equivalently a Function
Space whose Elements are Functions from $\nats$ to the Field $K$ of Real or
Complex Numbers



% --------------------------------------------------------------------
\subsection{Banach Space}\label{sec:banach_space}
% --------------------------------------------------------------------

A \emph{Banach Space} is a Vector Space (\S\ref{sec:vector_space}) $X$ over the
Field of Real or Complex Numbers

a Complete (\S\ref{sec:complete_metric_space}) Normed Vector Space
(\S\ref{sec:normed_vectorspace})

Topological Vector Space (\S\ref{sec:topological_vectorspace})



\subsubsection{$L^p$ Space}\label{sec:lp_space}



% --------------------------------------------------------------------
\subsection{Banach Algebra}\label{sec:banach_algebra}
% --------------------------------------------------------------------

an Associative Algebra (\S\ref{sec:associative_algebra}) over the Real or
Complex Numbers (or over a Non-archimedean Complete Normed Field
\S\ref{sec:nonarchimedean_field}) that is also a Banach Space

\emph{Banach Rings} (FIXME: ???)



\subsubsection{C$^*$-algebra}\label{sec:cstar_algebra}

A \emph{C$^*$-algebra} a Complex Algebra $A$ of Continuous Linear
Operators (\S\ref{sec:continuous_linear}) on a Complex Hilbert Space
(\S\ref{sec:hilbert_space}) with the additional Properties that $A$ is
Topologically Closed in the Norm Topology of Operators and Closed
under the Operation of taking Adjoints of Operators

Involutive Algebra (\S\ref{sec:involutive_algebra})

Involution Semigroup (\S\ref{sec:involution_semigroup})

(wiki):

Linear Logic (\S\ref{sec:linear_logic}) can be seen as the refining
the Interpretation of Classical Logic by replacing Boolean Algebras
(\S\ref{sec:boolean_algebra}) by C$^*$-algebras

the Uniform Closure of Complex Continuous Functions over a Compact Space is a
C$^*$ Algebra



\paragraph{Dual C$^*$-algebra}\label{sec:cstar_dual}\hfill

or \emph{Dual} or \emph{Spectrum} of a C$^*$-algebra is the Set of Unitary
Equivalence Classes of Irreducible $*$-representations of $A$ (FIXME: clarify)

cf. Ring Spectrum (\S\ref{sec:ring_spectrum})



\paragraph{von Neumann Algebra}\label{sec:vonneumann_algebra}\hfill



% --------------------------------------------------------------------
\subsection{Cross-correlation}\label{sec:cross_correlation}
% --------------------------------------------------------------------

or \emph{Sliding Dot Product}

Similarity Measure (\S\ref{sec:similarity_measure}) of two Series as a Function
of the Displacement of one relative to the other

for Continuous Signals the Cross-correlation Operator is the Adjoint Operator
of the Convolution (\S\ref{sec:convolution}) Operator

\fist Fourier Analysis (\S\ref{sec:fourier_analysis})



\subsubsection{Autocorrelation}\label{sec:autocorrelation}



% --------------------------------------------------------------------
\subsection{Convolution}\label{sec:convolution}
% --------------------------------------------------------------------

$(f * g)(t) = \int_0^t f(t-\tau) g(\tau) d\tau$

Operation on two Functions giving a modified Function giving the Integral of
the Pointwise Multiplication of the two Functions as a Function of the amount
that one of the original Functions is Translated (\S\ref{sec:translation})

for Continuous Signals the Cross-correlation (\S\ref{sec:cross_correlation})
Operator is the Adjoint Operator of the Convolution Operator

Image Processing

\fist Fourier Analysis (\S\ref{sec:fourier_analysis})

Laplace Transform (\S\ref{sec:laplace_transform})



% --------------------------------------------------------------------
\subsection{Coherence Space}\label{sec:coherence_space}
% --------------------------------------------------------------------

note: not the same as Coherent Space (Spectral Space)

cf. Linear Logic (\S\ref{sec:linear_logic}) Semantics



% --------------------------------------------------------------------
\subsection{Spectral Theory}\label{sec:spectral_theory}
% --------------------------------------------------------------------

Hilbert Space (\S\ref{sec:hilbert_space})



\subsubsection{Spectrum}\label{sec:spectrum}

generalization of Matrix Spectrum (Set of Eigenvalues
(\S\ref{sec:matrix_spectrum}) of a Matrix to general \emph{Operators}

\fist Note that ``Spectrum'' is a highly overloaded term and may otherwise
refer to:
\begin{itemize}
  \item Graph Spectrum (Spectral Graph Theory \S\ref{sec:graph_spectrum}) --
    Spectrum of the Adjacency Matrix (\S\ref{sec:adjacency_matrix}) of a Graph

  \item Spectral Norm (\S\ref{sec:spectral_norm}) -- Schatten $\infty$-norm of
    a Matrix (\S\ref{sec:spectral_norm})

  \item Ring Spectrum (\S\ref{sec:ring_spectrum}) -- the Set (Space) of all
    Prime Ideals (\S\ref{sec:prime_ideal}) of a Ring
  \item C$^*$-algebra Spectrum (Dual \S\ref{sec:cstar_dual}) -- Set of Unitary
    Equivalence Classes of Irreducible $*$-representations of a C$^*$-algebra
    (similar notion to Ring Spectrum)

  \item Topological Spectrum (Stable Homotopy Theory
    \S\ref{sec:topological_spectrum}) -- represents a Generalized Cohomology
    Theory (\S\ref{sec:generalized_cohomology_theory})

  \item Polygon Spectrum (\S\ref{sec:polygon_spectrum}) -- the Set of all $n$
    for which an $n$-equidissection (\S\ref{sec:equidissection}) of a Polygon
    $P$ exists

  \item Sentence Spectrum (\S\ref{sec:sentence_spectrum}) -- the Set of Natural
    Numbers occurring as the size of a Finite Model
    (\S\ref{sec:finite_model}) in which a given Sentence is True
  \item Theory Spectrum (\S\ref{sec:theory_spectrum}) -- the number of
    Isomorphism Classes of Models of various Cardinalities
\end{itemize}



\subsubsection{Pseudospectrum}\label{sec:pseudospectrum}

Set containing the Spectrum of the Operator and the numbers that are ``almost''
Eigenvalues



% --------------------------------------------------------------------
\subsection{Calculus of Variations}\label{sec:calculus_of_variations}
% --------------------------------------------------------------------

or \emph{Variational Calculus})

\fist Optimal Control Theory (\S\ref{sec:optimal_control})

\fist cf. Dynamic Programming (\S\ref{sec:dynamic_programming})

Hamilton-Jacobi Equation (\S\ref{sec:hamilton_jacobi}

Virtual Displacements



\subsubsection{Functional Derivative}\label{sec:functional_derivative}

\fist cf. Derivative (\S\ref{sec:derivative})



% ====================================================================
\section{Numerical Analysis}\label{sec:numerical_analysis}
% ====================================================================

applications of Calculus of Finite Differences
(\S\ref{sec:finite_differences_calculus}) --TODO

\begin{itemize}
  \item \emph{Numerical Integration} (\emph{Quadrature}
    \S\ref{sec:numerical_integration}) -- Evaluating Integrals
    (\S\ref{sec:integral})
  \item Solving \emph{Differential Equations}
    (\S\ref{sec:differential_equation}) -- Finite Element Method
    (\S\ref{sec:finite_element_method}), Finite Volume Method
    (\S\ref{sec:finite_volume_method})
  \item Solving \emph{Systems of Equations} (\S\ref{sec:equation_system}):
    \begin{itemize}
      \item \emph{Direct Methods} (\S\ref{sec:direct_method}) for Solving
        Systems of Linear Equations using Matrix Decomposition
        (\S\ref{sec:matrix_decomposition}):
        \begin{itemize}
          \item Gaussian Elimination (\S\ref{sec:gaussian_elimination})
          \item LU Decomposition (\S\ref{sec:lu_decomposition})
          \item Cholesky Decomposition (\S\ref{sec:cholesky_decomposition})
          \item QR Decomposition (\S\ref{sec:qr_decomposition})
        \end{itemize}
      \item \emph{Iterative Methods} (\S\ref{sec:iterative_method}), preferred
        for larger Systems:
        \begin{itemize}
          \item Jacobi Method (\S\ref{sec:jacobi_method})
          \item Gauss-Seidel Method (\S\ref{sec:gauss_seidel})
          \item Successive Over-relaxation
            (\S\ref{sec:successive_over_relaxation})
          \item Conjugate Gradient Method
            (\S\ref{sec:conjugate_gradient_method})
        \end{itemize}
      \item \emph{Root-finding Algorithms} (\S\ref{sec:root_finding}), used to
        Solve Non-linear Equations:
        \begin{itemize}
          \item Newton's Method (\S\ref{sec:newtons_method})
          \item Linearization (\S\ref{sec:linearization})
        \end{itemize}
    \end{itemize}
  \item Approximation Theory (\S\ref{sec:approximation_theory})
  \item Singular Value Problems (TODO)
  \item ... MORE ?
\end{itemize}

see also:
\begin{itemize}
  \item Numerical Linear Algebra (\S\ref{sec:numerical_linear_algebra})
  \item Regression Analysis (\S\ref{sec:regression_analysis})
  \item Optimization (Part \ref{part:optimization})
\end{itemize}

(wiki):

the goal of Numerical Analysis is to \emph{Approximate the Continuum}
(Real Line \S\ref{sec:real_line})

Mimesis -- the quality of a Numerical Method that imitates some properties of
the ``Continuum Problem''

FIXME: explain

\asterism

\fist cf. Discrete First-order Dynamical Systems
(\S\ref{sec:discrete_dynamical_system}) can be used to Numerically Solve
Single-variable Equations-- the problem to be Solved consisting of finding the
Roots of a Real Function $f$, i.e. the values of $x$ such that:
\[
  f(x) = 0
\]
For Equations that cannot be solved Analytically, the Numerical Method involves
defining a Dynamical System with Convergent Sequences which approach Solutions
to the Equation



% --------------------------------------------------------------------
\subsection{Interval Arithmetic}\label{sec:interval_arithmetic}
% --------------------------------------------------------------------

or \emph{Interval Analysis}

approach on putting \emph{Bounds} (\S\ref{sec:bounded_set}) on \emph{Rounding
  Errors} and \emph{Measurement Errors} in Numerical Computations



\subsubsection{Unit Interval}\label{sec:unit_interval}

$[0,1]$, sometimes $I$

a Complete Metric Space (\S\ref{sec:complete_metric_space}),
Homeomorphic (\S\ref{sec:homeomorphism}) to the Extended Real Number
Line (\S\ref{sec:extended_real_line})



\subsubsection{Infinitesimal Interval}\label{sec:infinitesimal_interval}

\fist Infinitesimal Interval Object (\S\ref{sec:infinitesimal_interval_object})

(nlab):

in the ``Context'' $H$ (FIXME: clarify) of Synthetic Differential Geometry
(\S\ref{sec:synthetic_differential_geometry}), the Differentiation
(\S\ref{sec:derivative}) (Endo-)functor $\mathrm{d} : \cat{Diff} \rightarrow
\cat{Diff}$ on the Category of Smooth Manifolds and Smooth Maps is the Internal
Hom (\S\ref{sec:internal_hom}) out of the Infinitesimal Interval $D$:
\[
  \mathrm{d} = [D,-] : H \rightarrow H
\]
where $D \hookrightarrow \reals$ is the Smooth Locus (\S\ref{sec:smooth_locus})
defined by $x^2 = 0$ (in Toposes that Model Synthetic Differential Geometry this
is regarded as an Infinitesimal Neighborhood about a Point)



% --------------------------------------------------------------------
\subsection{Approximation Theory}\label{sec:approximation_theory}
% --------------------------------------------------------------------

\fist cf. Regression Analysis (\S\ref{sec:regression_analysis})



\subsubsection{Discretization}\label{sec:discretization}

Discretization Methods (\S\ref{sec:discretization_method})



\subsubsection{Interpolation}\label{sec:interpolation}

cf. Extrapolation (TODO), Regression (\S\ref{sec:regression_analysis})



\paragraph{Spline Function}\label{sec:spline}\hfill

\subparagraph{Spline Interpolation}\label{sec:spline_interpolation}\hfill

cf. Polynomial Interpolation (???) %FIXME



\subparagraph{Cubic Spline}\label{sec:cubic_spline}\hfill

\subparagraph{B-spline}\label{sec:b_spline}\hfill



\subsubsection{Curve Fitting}\label{sec:curve_fitting}

\fist if errors are \emph{normally distributed}, Least Squares ($2$-norm Best
Fit \S\ref{sec:least_squares}) should be used

may be solved with Linear Programming (\S\ref{sec:linear_programming}) methods



\paragraph{$1$-norm Best Fit}\label{sec:1norm_best_fit}\hfill

less sensitive to outliers



\subsubsection{Linear Approximation}\label{sec:linear_approximation}

\fist (Local) Linearization (\S\ref{sec:linearization}) -- finding the Linear
Approximation to a Function at a given Point

\fist Method of Finite Differences (\S\ref{sec:finite_difference})



\subsubsection{Chebyshev Approximation}\label{sec:chebyshev_approximation}

UC Math 352 Lec. 10 - \url{https://www.youtube.com/watch?v=XII0GSmEAgg}

minimizes the worst-case deviation



% --------------------------------------------------------------------
\subsection{Numerical Integration}\label{sec:numerical_integration}
% --------------------------------------------------------------------

methods for obtaining Numerical approximations to the solutions of
Time-dependent Ordinary (\S\ref{sec:differential_equation}) and Partial
(\S\ref{sec:pde}) Differential Equations

\emph{Explicit Methods} -- calculates the state of a system at a later time
from the state of the system at the current time:
\[
  Y(t+\Delta{t}) = F(Y(t))
\]
where $Y(t)$ is the current system state and $Y(t + \Delta{t})$ is the system
state after a (small) time step $\Delta{t}$

\emph{Implicit Methods} -- finds a solution by solving an equation involving
the current system state and a later system state:
\[
  G(Y(t), Y(t + \Delta{t})) = 0
\]

First-order Methods -- (Explicit) Euler Method (\S\ref{sec:euler_method}),
Symplectic (Semi-implicit) Euler

Second-order Methods -- Midpoint Method (\S\ref{sec:midpoint_method}), Verlet
Integration (\S\ref{sec:verlet_integration})

Higher-order Methods



\subsubsection{Explicit Integration Methods}\label{sec:explicit_integration}

calculates the State of a System at a later Time, $Y(t + \Delta{t})$, from the
State of the System at the current Time, $Y(t)$:
\[
  Y(t + \Delta{t}) = F(Y(t))
\]



\paragraph{Euler Method}\label{sec:euler_method}\hfill

or \emph{Forward Euler Method}

\fist cf. Symplectic (Semi-implicit) Euler (\S\ref{sec:symplectic_integrator}),
Backward (Implicit) Euler (\S\ref{sec:implicit_euler})

First-order Integrator

(Witkin-Baraff97):

$\vec{x}(t + \Delta{t}) = \vec{x}(t) + \Delta{t}f(\vec{x},t)$

Initial Value for $\vec{x}$ denoted by $\vec{x}_0 = \vec{x}(t_0)$

estimate of $\vec{x}$ at a later Time $t_0 + h$ denoted by $\vec{x}(t_0 + h)$
where $h$ is the \emph{Stepsize Parameter}, computed by taking a Step in the
Derivative Direction $\dot{\vec{x}}(t_0)$:
\[
  \vec{x}(t_0 + h) = \vec{x}_0 + h\dot{\vec{x}}(t_0)
\]
where $\dot{\vec{x}}(t) = f(\vec{x}, t)$ is computed by the Derivative Function
$f$

Instability: e.g. 1D Derivative Function $f = -kx$-- for $h > 1/k$ then
$|\Delta{x}| > |x|$ so the Solution oscillates around Zero; beyond $h = 2/k$ the
oscillation Diverges and the System ``blows up''; this is an example of a
``Stiff'' ODE (\S\ref{sec:stiff_equation}), for which Implicit Methods
(\S\ref{sec:implicit_integration}) can be used to avoid instability

the size of the Error depends Linearly on Step Size $h$

assuming $\vec{x}(t)$ is Smooth, the exact value at the end of the Step is an
Infinite Sum (Taylor Series \S\ref{sec:taylor_series}) involving the Value and
its Derivatives at the beginning of the Step:
\[
  \vec{x}(t_0 + h) = \vec{x}(t_0) + h\dot{\vec{x}}(t_0) +
    \frac{h^2}{2!}\ddot{\vec{x}}(t_0) + \cdots +
    \frac{h^n}{n!}\pderiv{^n\vec{x}}{t^n} + \cdots
\]
so the Euler Method is a truncation of this Series to its first two Terms on the
RHS, so it is only correct if all Derivatives above the First are Zero, i.e.
only if $\vec{x}(t)$ is \emph{Linear}; since the Error is dominated by the
leading Term $(h^2/2)\ddot{\vec{x}}(t_0)$, the Error can be described as Order
$h$-squared:
\[
  \vec{x}(t_0 + h) = \vec{x}(t_0) + h\dot{\vec{x}}(t_0) + O(h^2)
\]

Midpoint Method (Modified Euler \S\ref{sec:midpoint_method}) -- correct within
$O(h^3)$ but requires two Derivative Evaluations of $f$



\paragraph{Midpoint Method}\label{sec:midpoint_method}\hfill

\emph{Explicit Midpoint Method} or \emph{Modified Euler Method}

\fist Implicit Midpoint Method (\S\ref{sec:implicit_midpoint})

Second-order Runge-Kutta Integrator (\S\ref{sec:runge_kutta})

(Witkin-Baraff97):

retain an additional Term in the truncated Taylor Series
(\S\ref{sec:taylor_series}):
\[
  \vec{x}(t_0 + h) = \vec{x}(t_0) + h\dot{\vec{x}}(t_0) +
    \frac{h^2}{2}\ddot{\vec{x}}(t_0) + O(h^3)
\]

for an Autonomous Differential Equation $\dot{\vec{x}} = f(\vec{x}(t))$, using
Chain Rule (\S\ref{sec:chain_rule}) gives:
\[
  \ddot{\vec{x}} = \pderiv{f}{\vec{x}}\dot{\vec{x}} = f' f
\]
to avoid evaluating $f'$, perform a Taylor Expansion of $f$:
\[
  f(\vec{x}_0 + \Delta{\vec{x}}) =
    f(\vec{x}_0) + \Delta\vec{x}f'(\vec{x}_0) + O(\Delta\vec{x}^2)
\]
substituting $\Delta{\vec{x}} = \frac{h}{2} f(\vec{x}_0)$ and multiplying both
sides by $h$ gives an Error of $O(h^3)$:
\[
  \frac{h^2}{2}\ddot{\vec{x}} + O(h^3) =
    h(f(\vec{x}_0 + \frac{h}{2}f(\vec{x}_0)) - f(\vec{x}_0)
\]
and substituting this in the RHS of the truncated Taylor Series:
\[
  \vec{x}(t_0 + h) = \vec{x}(t_0) + h(f(\vec{x}_0 + \frac{h}{2}f(\vec{x}_0))
\]
which is a Formula that first evaluates an Euler Step and then performs a Second
Derivative Evaluation at the Midpoint of the Step and using the Midpoint
Evaluation to update $\vec{x}$



\paragraph{Runge-Kutta Method}\label{sec:runge_kutta}\hfill

\fist Implicit Runge-Kutta (\S\ref{sec:implicit_runge_kutta})

\begin{itemize}
  \item Midpoint Method (\S\ref{sec:midpoint_method}) -- 2nd-order Runge-Kutta
\end{itemize}



\paragraph{Verlet Integration}\label{sec:verlet_integration}\hfill

Second-order Integrator

\url{http://datagenetics.com/blog/july22018/index.html}



\subsubsection{Implicit Integration Methods}\label{sec:implicit_integration}

calculates the State of a System at a later Time, $Y(t + \Delta{t})$, from the
both State of the System at the current Time, $Y(t)$, and the State of the
System at a later Time, $Y(t + \Delta{t})$:
\[
  G(Y(t), Y(t + \Delta{t})) = 0
\]

(Witkin-Baraff97): used to find Solutions to ``Stiff'' ODEs
(\S\ref{sec:stiff_equation}) what would otherwise diverge with Explicit
Integration Methods (\S\ref{sec:explicit_integration})



\paragraph{Implicit Euler Method}\label{sec:implicit_euler}\hfill

or \emph{Backward Euler Method}

$y_{n+1} = y_n + \Delta{t}f(t + \Delta{t}, y_{n+1})$

the new approximation $y_{n+1}$ appears on both sides of the Equation, so the
method needs to solve an Algebraic Equation (Polynomial Equation
\S\ref{sec:polynomial_equation})-- for Non-stiff problems this can be done with
Fixed-point Iteration (\S\ref{sec:fixedpoint_iteration})

alternatively use a modification of Newton's Method (\S\ref{sec:newtons_method})

(Witkin-Baraff97): use a Linear Approximation based on $f$'s Taylor Series,
defining $\Delta{y} = y_{n+1} - y_n$:
\[
  y_n + \Delta{y} = y_n + \Delta{t}f(y_n + \Delta{y})
\]
or just:
\[
  \Delta{y} = \Delta{t}f(y_n + \Delta{y})
\]
and approximate $f(y_n + \Delta{y})$ by:
\[
  f(y_n) + f'(y_n)\Delta{y}
\]
(note $f'(y_n)$ is a Matrix), giving the approximation for $\Delta{y}$:
\[
  \Delta{y} = \Delta{t}(f(y_n) + f'(y_n)\Delta{y})
\]
rewriting as:
\[
  \Big(\frac{1}{\Delta{t}}I - f'(y_n)\Big)\Delta{y} = f(y_n)
\]
where $I$ is the Identity Matrix, solve for $\Delta{y}$:
\[
  \Delta{y} = \Big(\frac{1}{\Delta{t}}I - f'(y_n)\Big)^{-1}f(y_n)
\]
i.e. this involves solving a \emph{Linear System} at each step



\paragraph{Implicit Midpoint Method}\label{sec:implicit_midpoint}\hfill

simplest Collocation Method (\S\ref{sec:collocation_method}) and
a Symplectic Integrator (\S\ref{sec:symplectic_integrator})

Second-order Integrator

\fist Explicit Midpoint Method (Modified Euler Method
\S\ref{sec:midpoint_method})



\paragraph{Implicit Runge-Kutta Method}\label{sec:implicit_runge_kutta}\hfill

\fist Explicit Runge-Kutta (\S\ref{sec:runge_kutta})



\subsubsection{Geometric Integrator}\label{sec:geometric_integrator}

a Numerical Integration Method that preserves Geometric properties of the exact
\emph{Flow} (\S\ref{sec:integral_curve}) of a Differential Equation

by definition Geometric Integrators are \emph{Canonical Transformations}
(\S\ref{sec:canonical_coordinate}), i.e. preserving the form of Hamilton's
Equations



\paragraph{Symplectic Integrator}\label{sec:symplectic_integrator}\hfill

a Numerical Integration scheme for Hamiltonian Systems
(\S\ref{sec:hamiltonian_system})

forms the Subclass of Geometric Integrators that are by definition
\emph{Canonical Transformations}: a change of Canonical Coordinates that
preserves the form of Hamilton's Equations (\S\ref{sec:hamiltonian_system})
%FIXME xref

\begin{itemize}
  \item Implicit Midpoint Method (\S\ref{sec:implicit_midpoint})
  \item Semi-implicit (Symplectic) Euler (TODO: xref)
  \item ...
\end{itemize}



% --------------------------------------------------------------------
\subsection{Numerical Method}\label{sec:numerical_method}
% --------------------------------------------------------------------

a \emph{Numerical Algorithm} is an implementation of a Numerical Method with an
appropriate Convergence check

TODO

(Witkin-Baraff97): find \emph{Numerical Solutions} as opposed to \emph{Symbolic
  Solutions} to Differential Equations: at each Time Step, perform one or more
\emph{Derivative Evaluations} of the Derivative Function $f$ on the current
numerical values of State $\vec{x}$ and Time $t$, which returns a numerical
value for $\dot{\vec{x}}$ which is then used to increment $\vec{x}$

(Villate06): There is no \emph{general method} to finding Solutions of any
First-order ODE (\S\ref{sec:ode}) and in some cases known Solutions can only be
evaluated with approximate Numerical Methods



\subsubsection{Well-posed Problem}\label{sec:well_posed}

\subsubsection{Numerical Stability}\label{sec:numerical_stability}

Property of an Algorithm

Stiff Equations (\S\ref{sec:stiff_equation})



\paragraph{Condition Number}\label{sec:condition_number}\hfill

Property of a \emph{Problem}

\fist not to be confused with Conditioning (Probability
\S\ref{sec:conditioning})

\emph{Well-conditioned} -- a small error in Input will result in a small change
in Output

\emph{Ill-conditioned} -- a small error in Input will result in a large change
in Output

\url{https://www.youtube.com/watch?v=APh4ME3C7UI}

\begin{itemize}
  \item \emph{Normal Equations} (\S\ref{sec:normal_equation}): $(A^TA)\vec{x} =
    A^T\vec{b}$ -- Condition Number (\S\ref{sec:condition_number}) is
    $\kappa(A)^2$ (poor)
  \item \emph{QR Factorization} (\S\ref{sec:qr_factorization}): $\vec{x} =
    \hat{R}^{-1}\hat{Q}^T\vec{b}$ -- better Condition Number than Normal
    Equations
\end{itemize}



\paragraph{Absolute Condition Number}\label{sec:absolute_condition_number}
\hfill

\[
  \hat{\kappa} = \mathrm{sup}_{\Delta x} \frac{\ |\Delta f\|}{\|\Delta x\|}
\]

if $f$ is Differentiable to First Order, $\Delta f = J(x) \Delta x$ where $J$
is the Jacobian Matrix (\S\ref{sec:jacobian}), and therefore:
\begin{align*}
  \hat{\kappa} & = \mathrm{sup}_{\Delta x} \frac{\|J\Delta x\|}{\ |\Delta x\|} \\
  \hat{\kappa} & = \|J(x)\|
\end{align*}
that is, the Absolute Condition Number is equal to the Matrix Norm
(\S\ref{sec:matrix_norm}) of $J$



\subparagraph{Relative Condition Number}\label{sec:relative_condition_number}
\hfill

\[
  \kappa = \mathrm{sup}_{\Delta x} \frac{
    \Big(\frac{\ |\Delta f\|}{\|f(x)\|}\Big)
  }{
    \Big(\frac{\ |\Delta x\|}{\|x\|}\Big)
  }
\]

if $f$ is Differentiable:
\begin{align*}
  \kappa & = \mathrm{sup}_{\Delta x}
    \frac{\ |\Delta f\|}{\|\Delta x\|} \cdot \frac{\|x\|}{\|f(x)\|} \\
  \kappa & = \frac{\|J(x)\| \|x\|}{\|f(x)\|}
\end{align*}

example:

$f(x) = \frac{x}{2}$

$J(x) = f'(x) = \frac{1}{2}$

$\kappa = 1$ (Well-conditioned)

example 2:

$f(x) = \sqrt x$, $x > 0$

$J(x) = f'(x) = \frac{1}{2\sqrt{x}}$

$\kappa = 1/2$ (Well-conditioned)

$f(x,y) = x - y$

$J = \Big(
  \frac{\partial{f}}{\partial{x}}
  \frac{\partial{f}}{\partial{y}}
\Big) = [1 -1]$

$\kappa = \frac{2 \mathrm{max}(|x|,|y|)}{|x - y|}$ --
Problem is Ill-conditioned when $x$ is near $y$


\emph{Relative Condition Number for Matrix/Vector Multiplication}

$\kappa = \frac{\|A\ |\|x\|}{\|Ax\|}$

worst case is the Sharp Inequality (i.e. is an Equality for \emph{some}
$\vec{x}$):
\[
  \kappa \leq \|A\| \|A^{-1}\|
\]
where $\|A\ |\|A^{-1}\|$ is called the Condition Number $\kappa(A)$ of the
Matrix $A$


\emph{Relative Condition Number of Solving $A\vec{x} = \vec{b}$ for $\vec{x}$}:

$\vec{x} = A^{-1}\vec{b}$

Sharp Inequality (for some $\vec{b}$):
\[
  \kappa \leq \|A^{-1}\| \|A\|
\]


\emph{Relative Condition of a System of Equations}

Solving the System $A\vec{x} = \vec{b}$ for $\vec{x}$, considering
Perturbations in $A$, denoted by $\Delta A$

$(A + \Delta A)(\vec{x} + \Delta\vec{x}) = \vec{b}$

simplifies to:

$A(\Delta\vec{x}) + (\Delta A)\vec{x} = \vec{0}$

or:

$\Delta x = -A^{-1}(\Delta A)\vec{x}$

and:

$\ |\Delta\vec{x}\| \leq \|A^{-1}\| \|\Delta{A}\| \|\vec{x}\|$

giving:

\[
  \frac{
    \frac{\ |\Delta{x}\|}{\|\vec{x}\|}
  }{
    \frac{\ |\Delta{A}\|}{\|A\|}
  }
  \leq \|A^{-1}\ |\|A\| = \kappa(A)
\]

combining all the above, Solving $A\vec{x} = \vec{b}$ or Multiplying by $A$,
then it is expected to lose $\log_{10} \kappa(A)$ Digits of Accuracy; e.g. if
$\kappa(A) = 10000$, then Multiplying by $A$ will lose $4$ significant figures
of Accuracy; usually Well-conditioned Matrices have $\kappa$ in the Range $1$
to $100$



% --------------------------------------------------------------------
\subsection{Discretization Method}\label{sec:discretization_method}
% --------------------------------------------------------------------

Discretization (\S\ref{sec:discretization})



\subsubsection{Finite-Difference Method (FDM)}\label{sec:fdm}

Calculus of Finite Differences (\S\ref{sec:finite_differences_calculus})

dominant approach to Numerical Solutions of Partial Differential Equations
(\S\ref{sec:pde})



% --------------------------------------------------------------------
\subsection{Collocation Method}\label{sec:collocation_method}
% --------------------------------------------------------------------

method for Numerical Solution of Differential
(\S\ref{sec:differential_equation}) and Integral Equations
(\S\ref{sec:integral_equation})

\begin{itemize}
  \item Implicit Midpoint Method (\S\ref{sec:implicit_midpoint}) -- simplest
    Collocation Method
  \item ...
\end{itemize}



% --------------------------------------------------------------------
\subsection{Direct Method}\label{sec:direct_method}
% --------------------------------------------------------------------

attempt to solve a problem by a \emph{Finite Sequence of Operations} delivering
the exact Solution (in the absence of Rounding Errors)

cf. Iterative Methods (\S\ref{sec:iterative_method}) use an initial ``guess'' to
generate a Sequence of imporving Approximate Solutions

Solving Systems of Linear Equations (\S\ref{sec:linear_equation_system})
using Matrix Decomposition (\S\ref{sec:matrix_decomposition}):
\begin{itemize}
  \item Gaussian Elimination (\S\ref{sec:gaussian_elimination})
  \item LU Decomposition (\S\ref{sec:lu_decomposition})
  \item Cholesky Decomposition (\S\ref{sec:cholesky_decomposition})
  \item QR Decomposition (\S\ref{sec:qr_decomposition})
\end{itemize}



% --------------------------------------------------------------------
\subsection{Iterative Method}\label{sec:iterative_method}
% --------------------------------------------------------------------

cf. \emph{Direct Methods} attempt to solve a problem by a Finite Sequence of
Operations delivering the exact Solution

\fist cf. Iterative Rootfinding Methods (\S\ref{sec:iterative_rootfinding})

\fist Ellipsoid Method (\S\ref{sec:ellipsoid_method}); when specialized for
solving feasible Linear Optimization (\S\ref{sec:linear_programming}) Problems
with Rational Data, the Ellipsoid Metohd finds an Optimal Solution in a Finite
number of steps

\fist Sequential Quadratic Programming
(\S\ref{sec:sequential_quadratic_programming}): Iterative Method for
Constrained Non-linear Optimization (\S\ref{sec:nonlinear_programming})



\subsubsection{Gradient Descent}\label{sec:gradient_descent}

\url{http://ruder.io/optimizing-gradient-descent/index.html}

can be applied to Linear Regression (\S\ref{sec:linear_regression})



\paragraph{Momentum Optimization}\label{sec:momentum_optmization}\hfill

\url{https://fosterelli.co/executing-gradient-descent-on-the-earth}



\subsubsection{Conjugate Gradient Method}\label{sec:conjugate_gradient_method}

applicable to large Sparse Systems



\subsubsection{Stationary Iterative Method}
\label{sec:stationary_iterative}

or \emph{Relaxation Methods}

\fist not to be confused with Relaxation (Approximation \S\ref{sec:relaxation})

used to solve the Linear Equations resulting from a Discretization of the
Differential Equation, e.g. by Finite Differences



\paragraph{Jacobi Method}\label{sec:jacobi_method}\hfill

Diagonally Dominant (\S\ref{sec:diagonally_dominant}) System of Linear
Equations (\S\ref{sec:linear_equation_system})



\subparagraph{Projected Jacobi Method}
\label{sec:projected_jacobi_method}



\paragraph{Gauss-Seidel Method}\label{sec:gauss_seidel}\hfill

(or \emph{Liebmann Method} or \emph{Method of Successive
  Displacement})

Iterative Method for solving a Linear System of Equations
(\S\ref{sec:linear_equation_system})

Convergence only guaranteed if the Matrix is either Diagonally
Dominant (\S\ref{sec:diagonally_dominant}) or Symmetric
(\S\ref{sec:symmetric_matrix}) and Positive Definite
(\S\ref{sec:positive_definite})

Gauss-Seidel is the same as Successive Over-Relaxation
(\S\ref{sec:successive_over_relaxation}) with $\omega = 1$



\subparagraph{Projected Gauss-Seidel Method}\hfill
\label{sec:projected_gauss_seidel}

Gauss-Seidel applied to Linear Complementarity Problem (LCP
\S\ref{sec:linear_complementarity})

PGS



\subparagraph{Non-linear Gauss-Seidel Method}
\label{sec:nonlinear_gauss_seidel}

NGS



\paragraph{Successive Over-Relaxation Method}
\label{sec:successive_over_relaxation}\hfill

SOR

Gauss-Seidel (\S\ref{sec:gauss_seidel}) is the same as SOR with $\omega = 1$



\subsubsection{Krylov Subspace Method}\label{sec:krylov_subspace_method}



% --------------------------------------------------------------------
\subsection{Root-finding Algorithm}\label{sec:root_finding}
% --------------------------------------------------------------------

(wiki):

generally, the Roots of a Real or Complex Continuous Function

(Villate06 \S 2.5):

Attractive Fixed Points (Discrete Dynamical Systems
\S\ref{sec:discrete_dynamical_system}) --
Discrete First-order Dynamical Systems can be used to Numerically Solve
Single-variable Equations-- the problem to be Solved consisting of finding the
Roots of a Real Function $f$, i.e. the values of $x$ such that:
\[
  f(x) = 0
\]
For Equations that cannot be solved \emph{Analytically} (TODO: clarify, xref),
the Numerical Method involves defining a Dynamical System with Convergent
Sequences which approach Solutions to the Equation:
\begin{itemize}
  \item Iterative Rootfinding Methods (\S\ref{sec:iterative_rootfinding})
  \item Newton's Method (\S\ref{sec:newtons_method})
  \item ... MORE ?
\end{itemize}



\subsubsection{Iterative Rootfinding}\label{sec:iterative_rootfinding}

(wiki):

all Root-finding Algorithms proceed by Iteration, but an \emph{Iterative
  Root-finding Method} uses a specific type of Iteration

\fist cf. Iterative Methods (\S\ref{sec:iterative_method})

\fist Systems of Nonlinear Equations
(\S\ref{sec:nonlinear_equation_system})

\fist cf. Nonlinear Programming (\S\ref{sec:nonlinear_programming}), Nonlinear
Optimization (\S\ref{sec:nonlinear_optimization})

(Villate06 \S 2.5.1):

when the Equation $0 = f(x)$ can be written in the form of $x = g(x)$, then the
Solutions are the Fixed Points of the Discrete Dynamical System
(\S\ref{sec:discrete_dynamical_system}):
\[
  x_{n+1} = g(x_n)
\]
for Attractive Fixed Points, the System will Converge on the Solution; compare
using Newton's Method (\S\ref{sec:newtons_method}) where Fixed Points are
\emph{always} Attractive

FIXME:
if a 2-Cycle occurs, the midpoint may be chosen, which is equivalent to Newton's
Method:
\[
  x_{n+1} = \frac{1}{2}\big(x_n + \frac{5}{x_n}\big)
\]



\paragraph{Trust Region}\label{sec:trust_region}\hfill

\paragraph{Fixed-point Iteration}\label{sec:fixedpoint_iteration}\hfill

method of computing Fixed Points (\S\ref{sec:fixed_point}) of Iterated Functions
(\S\ref{sec:iterated_function})



\paragraph{Newton's Method}\label{sec:newtons_method}\hfill

or \emph{Newton-Raphson Method}

UC Math 352 Lec. 22 - \url{https://www.youtube.com/watch?v=YGq1bRFDpHo} --
Numerical Linear Algebra (Minimization Problem)

Minimize Second-order Taylor Polynomial

Convex Objective with Equality Constraints

(Villate06 \S 2.5.2):

Solve Equation $f(x) = 0$ by finding the Fixed Points of the Discrete Dynamical
System (\S\ref{sec:discrete_dynamical_system}) defined by the Difference
Equation (\S\ref{sec:difference_equation}):
\[
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\]
compared to the Iterative Method (\S\ref{sec:iterative_rootfinding}), the Fixed
Points will always be \emph{Attractive}
%FIXME: is this considered an iterative rootfinding ?



\paragraph{Line Search}\label{sec:line_search}\hfill

\subparagraph{Backtracking Line Search}\label{sec:backtracking_line_search}
\hfill



\paragraph{Secant Method}\label{sec:secant_method}\hfill

\paragraph{Inverse Interpolation Method}\label{sec:inverse_interpolation}\hfill



\subsubsection{Bracketing Method}\label{sec:bracketing_method}

\paragraph{Bisection Method}\label{sec:bisection_method}\hfill

\paragraph{False Position Method}\label{sec:false_position}\hfill



% --------------------------------------------------------------------
\subsection{Finite Volume Method}\label{sec:finite_volume_method}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Finite Element Method}\label{sec:finite_element_method}
% --------------------------------------------------------------------

\emph{Finite Element Method} or \emph{FEM} is a technique for solving Boundary
Value Problems (\S\ref{sec:boundary_value_problem}) for Partial Differential
Equations (\S\ref{sec:partial_differential})

GetFEM++ -- LGPL C++ library



% --------------------------------------------------------------------
\subsection{Meshfree Method}\label{sec:meshfree_method}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Perturbation Theory}\label{sec:perturbation_theory}
% --------------------------------------------------------------------

\subsubsection{Perturbation Method}\label{sec:perturbation_method}

used to Evaluate Functional Integrals (\S\ref{sec:functional_integration})

\emph{Perturbation Series} -- Formal Power Series
(\S\ref{sec:formal_power_series}) in some ``small'' Parameter quantifying the
deviation from the exactly Solvable problem



% --------------------------------------------------------------------
\subsection{Deformation Theory}\label{sec:deformation_theory}
% --------------------------------------------------------------------

%FIXME: move section ???



% ====================================================================
\section{Convex Analysis}\label{sec:convex_analysis}
% ====================================================================

\fist Convex Optimization (\S\ref{sec:convex_optimization})



% --------------------------------------------------------------------
\subsection{Convex Set}\label{sec:convex_set}
% --------------------------------------------------------------------

%FIXME: move section ???

\fist Convex Geometry (\S\ref{sec:convex_geometry})

every Affine Set (\S\ref{sec:affine_set}) is also Convex

Line Segment between Points $x_1$ and $x_2$:
\[
  x = \theta x_1 + (1-\theta) x_2
\]
with $0 \leq \theta \leq 1$

\fist in an Affine Set, $\theta$ can vary over the entire Real Line: $\theta
\in \reals$

a Function is Convex (\S\ref{sec:convex_function}) if and only if its Epigraph
is a Convex Set

Operations that conserve Convexity:
\begin{itemize}
  \item Intersection
  \item Affine Functions (\S\ref{sec:affine_transformation})
  \item Perspective Functions (FIXME: xref ???)
  \item Linear-fractional Functions (FIXME: xref ???)
\end{itemize}

(\emph{Seperating Hyperplane Theorem}) for two general disjoint Convex Sets,
there is a Hyperplane for which each Convex Set lies entirely in each Halfspace

(\emph{Supporting Hyperplane Theorem}) at every boundary point of a Convex Set
there is a Supporting Hyperplane

examples of Convex Sets:
\begin{itemize}
  \item Hyperplanes (\S\ref{sec:hyperplane}) are Affine and Convex
  \item Halfspaces (\S\ref{sec:half_space}) are Convex
  \item Ellipsoids (\S\ref{sec:ellipsoid}), Euclidean Balls (\S\ref{sec:ball})
  \item Norm Balls, Norm Cones (\S\ref{sec:norm})
  \item Convex Polyhedra (\S\ref{sec:convex_polyhedra})
  \item Convex Cones (\S\ref{sec:convex_cone}), Positive Semidefinite Cones
    (\S\ref{sec:positive_semidefinite_cone})
  \item the Set $\mathsf{S}^n$ of Symmetric $n \times n$ Matrices is Affine and
    Convex
  \item the Set $\mathsf{S}_+^n$ of Positive Semi-definite $n \times n$
    Matrices is a Convex Cone and hence is a Convex Set
  \item the Set $\mathsf{S}_{++}^n$ of Positive Definite $n \times n$ Matrices
    (FIXME: is also a convex cone ???)
\end{itemize}



\subsubsection{Convex Cone}\label{sec:convex_cone}

the Set $\mathsf{S}_+^n$ of Positive Semi-definite $n \times n$ Matrices is a
Convex Cone

\emph{Proper Cone}: Closed, Solid, Pointed (contains no Line)

Generalized Inequality



\paragraph{Positive Semidefinite Cone}
\label{sec:positive_semidefinite_cone}\hfill



\subsubsection{Dual Cone}\label{sec:dual_cone}

defined for any Cone (not necessarily Convex)

$\reals_+^n$ and $\mathsf{S}_+^n$, and the Euclidean Norm Cone are Self-dual

the Dual Cone of the $1$-norm Cone is the $\infty$-norm Cone

the Dual Cones of Proper Cones are Proper

for a Proper Cone, the Dual Cone of a Dual Cone is the original Cone



\subsubsection{Polar Cone}\label{sec:polar_cone}



% --------------------------------------------------------------------
\subsection{Convex Algebra}\label{sec:convex_algebra}
% --------------------------------------------------------------------

%FIXME: move this section ???

\url{https://golem.ph.utexas.edu/category/2018/03/cognition_convexity_and_catego.html}



\subsubsection{Convex Relation}\label{sec:convex_relation}



% ====================================================================
\section{Algebraic Analysis}\label{sec:algebraic_analysis}
% ====================================================================

Systems of Linear PDEs (\S\ref{sec:linear_pde_system}) using Sheaf Theory
(\S\ref{sec:sheaf_theory}) and Complex Analysis (\S\ref{sec:complex_analysis})

generalizations of Functions: Hyperfunctions, Microfunctions



% --------------------------------------------------------------------
\subsection{Generalized Function}\label{sec:generalized_function}
% --------------------------------------------------------------------

\emph{Schwartz Kernel Theorem}: if the Kernel of an Integral Transform
(\S\ref{sec:integral_transform}) is allowed to be a Generalized Function then
all Linear Operators (\S\ref{sec:linear_operator}) are Integral Transforms



\subsubsection{Distribution}\label{sec:distribution}

Continuous Linear Functional (\S\ref{sec:linear_form})



\paragraph{Dirac Delta Function}\label{sec:dirac_delta}\hfill

(wiki):

$\delta$

\emph{Unit Impulse Symbol}

\emph{heuristic} characterization: can be thought of as a Function on the Real
Line that is Zero everywhere except at the origin where it is Infinite and
Constrained to satisfy the identity:
\[
  \int_{-\infty}^\infty \delta(x) dx = 1
\]

note that \emph{no Function defined on the Real Numbers has this property}

can also be defined as a Measure (\S\ref{sec:measure})

\fist Functional Integration (\S\ref{sec:functional_integration})



% --------------------------------------------------------------------
\subsection{$D$-module}\label{sec:d_module}
% --------------------------------------------------------------------

a Module (\S\ref{sec:module}) over a Ring (\S\ref{sec:ring}) $D$ of Differential
Operators (\S\ref{sec:differential_operator})

approach to theory of Linear Partial Differential Equations
(\S\ref{sec:linear_pde})



\subsubsection{Algebraic $D$-module}\label{sec:algebraic_d_module}

Modules over the Weyl Algebra (\S\ref{sec:weyl_algebra}) $A_n(K)$ over a Field
$K$ of Characteristic Zero; relates Weyl Algebra to Differential Equations
(\S\ref{sec:differential_equation})



\paragraph{Holonomic Module}\label{sec:holonomic_module}\hfill

a \emph{Holonomic Function} (\S\ref{sec:holonomic_function}) is an Element of a
Holonomic Module of Smooth Functions

\fist cf. Holonomy (\S\ref{sec:holonomy}), Holonomic Constraints
(\S\ref{sec:holonomic_constraint})

$M$ is Holonomic if and only if $D(M)$ is ``Concentrated'' (seen as an Object
in a derived Category of $D$-modules) in Degree $0$ (FIXME: clarify)

tendency to behave like Finite Dimensional Vector Spaces

\fist Symplectic Geometry (\S\ref{sec:symplectic_geometry}): the Characteristic
Variety (FIXME: xref) $Ch(M)$ of any $D$-module $M$, when seen as a Subvariety
of the Cotangent Bundle $T^*X$ of $X$, is an \emph{Involutive Variety} (FIXME:
xref), i.e. the Module is Holonomic if and only if $Ch(M)$ is a Lagrangian
Submanifold (\S\ref{sec:lagrangian_submanifold})



% ====================================================================
\section{Ordinal Analysis}\label{sec:ordinal_analysis}
% ====================================================================

Ordinal Numbers (\S\ref{sec:ordinal_number})

Proof-theoretic Ordinal (\S\ref{sec:proof_ordinal})



% ====================================================================
\section{Non-standard Analysis}\label{sec:nonstandard_analysis}
% ====================================================================

Differentials (\S\ref{sec:differential}) as Infinitesimals
(\S\ref{sec:infinitesimal}) in Hyperreal Number Systems (\S\ref{sec:hyperreal})

Real Closed Field (\S\ref{sec:real_closed})

$\reals^\nats / \mathsf{M}$ where $\mathsf{M}$ is a Maximal Ideal
(\S\ref{sec:maximal_ideal}) not leading to a Field that is
Order-isomorphic (\S\ref{sec:order_isomorphism}) to $\reals$-- the
uniqueness of this Field is equivalent to the Continuum Hypothesis
(\S\ref{sec:continuum_hypothesis})



% ====================================================================
\section{Constructive Analysis}\label{sec:constructive_analysis}
% ====================================================================

\emph{Choice Sequence}



% ====================================================================
\section{Computable Analysis}\label{sec:computable_analysis}
% ====================================================================

% ====================================================================
\section{Algorithm Analysis}\label{sec:algorithm_analysis}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Linear Dominance}\label{sec:linear_dominance}
% --------------------------------------------------------------------

\[
    f \lesssim g \Leftrightarrow
    \exists x_0 \exists c : \forall x > x_0, |f(x)| \leq c |g(x)|
\]

Pointwise Domination implies Linear Dominance:
\[
    f \leq g \Rightarrow f \lesssim g
\]

\[
    f \lesssim g \wedge g \lesssim f \Leftrightarrow f \sim g
\]



\subsubsection{Big-O Notation}\label{sec:bigo_notation}

\[
    O(f) = \{ g : g \lesssim f \}
\]

\[
    \Omega(f) = \{ g : g \gtrsim f \}
\]

\[
    \Theta(f) = \{ g : g \sim f \}
\]

\[
    O(1) \subset O(x) \subset O(x^2) \subset O(x^2) \ldots
\]



% ====================================================================
\section{Spatial Analysis}\label{sec:spatial_analysis}
% ====================================================================

% FIXME

% --------------------------------------------------------------------
\subsection{Boundary Problem}\label{sec:boundary_problem}
% --------------------------------------------------------------------

% FIXME



% ====================================================================
\section{Differential Calculus}\label{sec:differential_calculus}
% ====================================================================

\emph{Differential Calculus in One Variable}

\fist Multivariable Calculus (\S\ref{sec:multivariable_calculus}):
\begin{itemize}
  \item Partial Derivatives (\S\ref{sec:partial_derivative})
  \item Differential Forms (\S\ref{sec:differential_form})
\end{itemize}

\fist Differential Equations (\S\ref{sec:differential_equation})

\fist Differential Geometry (\S\ref{sec:differential_geometry})

\fist Synthetic Differential Geometry
(\S\ref{sec:synthetic_differential_geometry}) --
Differential Geometry formalized in the language of Topos Theory
(\S\ref{sec:topos_theory})

\fist Differential Topology (\S\ref{sec:differential_topology})

\fist Differential Algebra (\S\ref{sec:differential_algebra})

\fist Differential Graded Category (\S\ref{sec:differential_graded_category})

Derivative (\S\ref{sec:derivative}) $f : \reals \rightarrow \reals$

Vector Derivative (\S\ref{sec:vector_derivative})
$f : \reals \rightarrow \reals^n$

Partial Derivatives (\S\ref{sec:partial_derivative}), Gradient
(\S\ref{sec:gradient}) $f : \reals^m \rightarrow \reals$

Jacobian (\S\ref{sec:jacobian}) $f : \reals^m \rightarrow \reals^n$

see also:

\begin{itemize}
  \item Directional Derivative (\S\ref{sec:directional_derivative})
  \item Matrix Derivative (\S\ref{sec:matrix_derivative})
\end{itemize}

(\url{https://www.youtube.com/watch?v=uN3hyzywzZk}): Conor McBride --
(Co-monadic) notion of \emph{Derivative} for Polynomial Functors (TODO)

some terminology:

\emph{Differential} (\emph{Infinitesimal Difference} \S\ref{sec:differential})
-- $\diffy{x}, \diffy{y}$

\fist cf. \emph{Boundary Operator} (or \emph{Differential}) -- Morphism of a
Differential Object (\S\ref{sec:differential_object}) defining a Chain Complex.
Concretely, a Boundary Operator on a Chain Complex is called a ``Differential''
if it is part of the structure of a Differential Graded Algebra
(\S\ref{sec:differential_graded_algebra}) on the Complex.
(FIXME: relation between the two uses of ``differential'' ???)

\emph{Differential Quotient}: $\deriv{y}{x}$ -- often used to represent the
Derivative of a Function at a Point

\emph{Derivative of a Function} (\S\ref{sec:derivative}): $\diffy{f} : X
\rightarrow Y$ -- maps Points $x$ in the Domain to the Derivative of the
Function at the Point: a Linear Transformation
$\diffy{f}_x : \reals^n \rightarrow \reals$ mapping Differentials $\diffy{x}$ to
Differentials $\diffy{y}$

\emph{Derivative of a Function at a Point}: $\diffy{f}_x : X \rightarrow Y$ --
Linear Transformation mapping Differentials $\diffy{x}$ to Differentials
$\diffy{y}$; the (Real) Analytic Definition is as a unique Linear Transformation
$\diffy{f}_x = L : \reals^n \rightarrow \reals$ found by a Limiting Proces
(\S\ref{sec:sequence_limit}):
\[
  \lim_{\delta\rightarrow0} \frac{f(x + \delta) - f(x) - L\delta}{|\delta|} = 0
\]
when such a Limit exists, $f$ is said to be \emph{Differentiable}
(\S\ref{sec:differentiable_function}) at $x$ with Derivative $\diffy{f}_x = L$;
when $n=1$, the Linear Transformation $L : \reals \rightarrow \reals$ is Scalar
Multiplication by $\lambda \in \reals$, denoted $f'(x)$

\emph{Differential of a Function at a Point}:
$\diffy{y} = \diffy{f}_x(\diffy{x})$ -- the Numerator $\diffy{y}$ of the
Differential Quotient representation $\deriv{y}{x}$ of the Derivative of the
Function at a Point $x$

\emph{Differentiation} -- the process of assigning a Function $f$ to its
Derivative $\diffy{f}$; on the $n+1$-times Differentiable Functions this is a
Function:
\[
  D : C^{n+1}(\reals^k) \rightarrow C^n(\reals^k \times \reals^k)
\]
and restricting to the Smooth case:
\[
  D : C^\infty(\reals^k) \rightarrow C^\infty(\reals^k \times \reals^k)
\]
this can also be defined as an \emph{(Endo-)functor} on the Category of Smooth
Manifolds and Smooth Maps:
\[
  \mathrm{d} : \cat{Diff} \rightarrow \cat{Diff}
\]
or in the ``Context'' (FIXME: clarify) $H$ of Synthetic Differential Geometry
(\S\ref{sec:synthetic_differential_geometry}), $\mathrm{d}$ is the Internal Hom
(\S\ref{sec:internal_hom}) out of the Infinitesimal Interval
(\S\ref{sec:infinitesimal_interval}) $D$:
\[
  \mathrm{d} = [D,-] : H \rightarrow H
\]
where $D \hookrightarrow \reals$ is the Smooth Locus (\S\ref{sec:smooth_locus})
defined by $x^2 = 0$ (in Toposes that Model Synthetic Differential Geometry this
is regarded as an Infinitesimal Neighborhood about a Point)



% --------------------------------------------------------------------
\subsection{Differential}\label{sec:differential}
% --------------------------------------------------------------------

an \emph{Infinitesimal Difference} $\mathrm{d}x$

the term ``Differential'' sometimes is used to refer to the \emph{Derivatives}
(\S\ref{sec:derivative}) of Functions (often represented as Quotients of
Differentials); here Differential is used to mean specifically the Numerator of
the Derivative at a Point

\fist cf. Infinitesimal Object (\S\ref{sec:infinitesimal_object})

\fist Finite Differences (\S\ref{sec:finite_difference}) -- Discrete equivalent
of Differentials

\fist the Derivative (\S\ref{sec:derivative}) is often viewed as a
\emph{Quotient} of Differentials $\deriv{f}{x}$ where the ``Numerator''
$\mathrm{d}f$ is the \emph{Differential} of the Function at $x$;
\emph{Differentiation} is the process of ``finding'' a Derivative (FIXME:
clarify)

\fist cf. Pushforward (\S\ref{sec:pushforward}) -- Differential or Derivative of
a Smooth Map

\fist cf. Differential Forms (\S\ref{sec:differential_form})

\fist cf. \emph{Boundary Operator} (or \emph{Differential}) -- Morphism of a
Differential Object (\S\ref{sec:differential_object}) defining a Chain Complex.
Concretely, a Boundary Operator on a Chain Complex is called a ``Differential''
if it is part of the structure of a Differential Graded Algebra
(\S\ref{sec:differential_graded_algebra}) on the Complex; archetypical example:
the Differential in the de Rham Complex (\S\ref{sec:derham_complex})
$\Omega^\bullet(X)$ of a Smooth manifold $X$, give by Differentiation
(\S\ref{sec:derivative}) of Smooth Functions and Differential Forms
(\S\ref{sec:differential_form})

for a Real Function $f : \reals \rightarrow \reals$ of a Real Variable $x$,
$dx_p$ ($dx$ at a Point $p$) can be viewed as the \emph{Identity Function} on
the Tangent Space $T_p\reals$ and $df_p : T_p\reals \rightarrow \reals$ is the
Function that gives a Linear Approximation to $f$ at $p$ (FIXME: clarify)

(wiki):

different approaches to treating Differentials \emph{quantitatively}:
\begin{itemize}
  \item Differentials as \emph{Linear Maps} (\S\ref{sec:linear_transformation})
    -- Differential Geometry (\S\ref{sec:differential_geometry})
  \item Differentials in \emph{Smooth Models of Set Theory} -- Synthetic
    Differential Geometry (\S\ref{sec:synthetic_differential_geometry}); Smooth
    Infinitesimal Analysis (\S\ref{sec:smooth_infinitesimal_analysis})
  \item Differentials as \emph{Infinitesimals in Hyperreal Number Systems} --
    Nonstandard Analysis (\S\ref{sec:nonstandard_analysis})
  \item Differentials as \emph{Nilpotent Elements of Commutative Rings} --
    Algebraic Geometry (Part \ref{part:algebraic_geometry})
\end{itemize}

the Differential of a Real-valued Function $f(x)$ of a single Real Variable $x$
is the Function $\mathrm{d}f$ of two Independent Real Variables $x$ and
$\mathrm{d}x$:
\[
  \mathrm{d}f(x, \mathrm{d}x) \defeq f'(x)\mathrm{d}x
\]

\asterism

Mathview - \emph{Differentials 101} -
\url{https://www.youtube.com/watch?v=KrTQ0Rz3qBo}

Multiplicative Algebra of Differentials

Closed Interval $[-1, 1]$ is Closed under Multiplication

for an Epsilon Neighborhood (\S\ref{sec:epsilon_neighborhood}) around $0$:
\[
  N_\epsilon(0) = \{ \delta \ |\ -\epsilon < \delta < \epsilon \}
\]
note also that the Cardinality (\S\ref{sec:cardinality}) of $N_\epsilon(0)$ is
equal to the Cardinality of the Real Line so there is a Bijection between them

$N_\epsilon(0)$ is a Commutative Semigroup (\S\ref{sec:commutative_semigroup})
under Multiplication

Inverse Differentials $\frac{1}{\delta} \in \overline{N}_\epsilon^*(0)
\subset \reals$ are the Set of arbitrarily large Real Numbers formed by the
Multiplicative Inverses of the Differentials in $\overline{N}_\epsilon(0)$ (that
is the ``Punctured'' Neighborhood excluding $0$); this is the Dual Commutative
Semigroup to the Commutative Semigroup of Differentials

Fiber Bundle of Differentials on the Real Numbers; each Fiber is an attached a
``Triangle'' to each Point; Multiplicative and Additive Structure (TODO)

the ``\emph{Dual Algebra}''
(FIXME: is this a standard name ???)

the entire Real Line, excluding $0$, can be recovered by Multiplying
Differentials and Inverse Differentials $\langle \delta^* \ |\ \delta \rangle$

Dual Product:
\[
  \langle \cdot \ |\ \cdot\rangle :
    N_\epsilon^*(0) \times N_\epsilon(0) \rightarrow \reals - \{0\}
\]

***

Mathview - \emph{An Algebra of Differentials} -
Part 1: \url{https://www.youtube.com/watch?v=2Ga8RFwwGcg}, Part 2:
\url{https://www.youtube.com/watch?v=oMe1gi5egiI}

Additive Algebra of Differentials

$N_L(0)$ -- arbitrarily large $L = \frac{1}{\epsilon}$

for $0 < \epsilon << 1$ observe:
\begin{align*}
  N_{\frac{1}{\epsilon}}(0) & =
    \{ \Delta \ |\ -\frac{1}{\epsilon} < x < \frac{1}{\epsilon} \} = \reals \\
  N_\epsilon(0) & = \{ \delta \ |\ -\epsilon < \delta < \epsilon \} \\
  \epsilon^2 N_{\frac{1}{\epsilon}}(0) & = N_\epsilon(0)
\end{align*}

$\epsilon^2$ is an Operator that maps any $x \in \reals$ back into the Interval
$[-\epsilon, \epsilon]$, and with Inverse $\frac{1}{\epsilon^2}$ defines an
Isomorphism between the Reals and the Epsilon Neighborhood

define a Closed Addition Operation $g_A(\delta, \delta') = \delta''$ by:
\begin{align*}
  g_A : N_\epsilon(0) \times N_\epsilon(0) \rightarrow N_\epsilon(0) \\
  g_A = h \circ (+) \circ (h^{-1}, h^{-1})
\end{align*}

***

Mathview - Manifolds - \url{https://www.youtube.com/watch?v=jvotAfSUxWU}
--
Differentiable Manifolds (\S\ref{sec:differentiable_manifold}) -- Epsilon
Neighborhoods constructed from Coordinates in $\reals^n$ are Vector Spaces of
\emph{Differentials}; Open Neighborhoods (\S\ref{sec:open_neighborhood}) in
Differentiable Manifolds are Vector Spaces of Differentials



\subsubsection{Implicit Differential}\label{sec:implicit_differential}

\paragraph{Implicit Equation}\label{sec:implicit_equation}\hfill

%FIXME: move ???

$f(x_1, \ldots, x_n) = 0$ where $f$ is often a Polynomial

\fist an Implicit Curve (\S\ref{sec:implicit_curve}) is a Plane Curve defined by
an Implicit Equation relating the $x$ and $y$ Coordinate Variables:
\[
  F(x,y) = 0
\]



\subparagraph{Implicit Function}\label{sec:implicit_function}\hfill

\emph{Implicit Function Theorem}

allows Relations to be converted to Functions of severl Real Variables by
representing the Relation as the \emph{Graph} of a Function; there may not be a
\emph{single} Function whose Graph can represent the entire Relation, but there
may be such a Function on a \emph{restriction} of the Domain of the Relation and
the Implicit Function Theorem gives a \emph{sufficient} condition to ensure that
such a Function exists



\subparagraph{Implicitization}\label{sec:implicitization}\hfill

the process of finding the Implicit Equation for a Manifold or Variety from its
Parametric Representation (Parameterization \S\ref{sec:parameterization})



% --------------------------------------------------------------------
\subsection{Derivative}\label{sec:derivative}
% --------------------------------------------------------------------

\emph{(Analytic) Derivative}

\fist cf. Formal Derivative (\S\ref{sec:formal_derivative})

the term ``Differential'' is sometimes is used to
refer to the Derivatives of Functions, but here Differential will always mean an
Infinitesimal Difference (\S\ref{sec:differential})

(wiki):

Geometrically, the \emph{Derivative} at a Point of the Graph of a Function is
the \emph{Slope} of the Tangent Line to the Graph at that Point, often viewed as
a Quotient of \emph{Differentials} (\S\ref{sec:differential}):
\[
  \deriv{y}{x}
\]
where the ``Numerator'' $\diffy{y}$ is the \emph{Differential} of $y$ at $x$;
really this Quotient is defined by a \emph{Limiting Process}
(\S\ref{sec:sequence_limit}). %TODO

\fist Difference Quotient (\S\ref{sec:difference_quotient}) -- Discrete
equivalent of Derivative as a Quotient of Differentials (Infinitesimal
Differences)

the Derivative at a Point can also be seen as a Function $\diffy{f}$ that maps
Differentials $\diffy{x}$ in $X$ to Differentials $\diffy{y}$ in $Y$

\fist cf. Pushforward (\S\ref{sec:pushforward}) -- Differential or Derivative of
a Smooth Map

\fist cf. Derivation (Differential Algebra \S\ref{sec:derivation}) --
generalizes features of the Derivative Operator

\fist Exterior Derivative (\S\ref{sec:exterior_derivative}) -- Operation acting
on a Differential $k$-form (\S\ref{sec:differential_form}) and produces a
Differential $(k+1)$-form

\fist Interior Derivative (\S\ref{sec:interior_derivative}) -- TODO

\fist Functional Derivative (\S\ref{sec:functional_derivative})

The process of ``finding'' (FIXME: or ``taking'' or computing ???) a Derivative
is called \emph{Differentiation}.

\fist cf. Antiderivative (Indefinite Integral \S\ref{sec:antiderivative})

The \emph{Differentiation Operator} $D$, is a Linear Operator
(\S\ref{sec:linear_operator}) with a Domain of all Differentiable Functions
(\S\ref{sec:differentiable_function}) and a Range of ``a Set of Functions'':
\[
  D(f) = f'
\]
(FIXME: clarify)

\fist Partial Derivative (\S\ref{sec:partial_derivative})

\fist Vector Derivative (\S\ref{sec:vector_derivative}), Matrix Derivative
(\S\ref{sec:matrix_derivative})

\fist the \emph{Gradient} (\S\ref{sec:gradient}) generalizes the Derivative to
Multi-variable Scalar-valued Functions and the the Jacobian
(\S\ref{sec:jacobian}) generalizes the Gradient to the Vector-valued
(\S\ref{sec:vector_function}) case: the Jacobian of a Scalar-valued
Multivariable Function is the Gradient, and the Jacobian of a Scalar-valued
Single Variable Function is its Derivative

notations:
\begin{itemize}
  \item \emph{Leibniz Notation}: $\deriv{y}{x}, \deriv{^2{y}}{x^2}, \ldots$
    -- more useful for Differentiation and Integration
  \item \emph{Lagrange Notation}: $f'(x), f''(x), \ldots$
    -- more useful for representing Derivatives of any Order compactly
  \item \emph{Newton Notation}: $\dot{y}, \ddot{y}, \ldots$ (with respect to
    Implicit Independent Variable, usually Time $t$)
  \item \emph{Euler Notation}: $D_x{f}, D^2_x{f}, \ldots$ (with $D_x^n$
    representing a Differential Operator \S\ref{sec:differential_operator})
\end{itemize}

\fist a \emph{Differential Equation} (\S\ref{sec:differential_equation})
describes the Relation between an \emph{Unknown Function}
(\S\ref{sec:unknown_function}) of one or more Variables and its
\emph{Derivatives}.

Differentiation is a \emph{Product-preserving Functor}
(\S\ref{sec:product_preserving_functor})

Differential $df$ as a Morphism of Tangent Bundles (\S\ref{sec:tangent_bundle})
$df : T\reals \rightarrow T\reals$ (FIXME: explain)

\asterism

$(\frac{g}{f})' = \frac{f g' - f' g}{f^2}$

\asterism

(nlab):

\emph{Differentiation} assigns to a Function $f : X \rightarrow Y$ its
\emph{Derivative}, $\diffy{f}$, which maps Points $x \in X$ to \emph{Linear
  Transformations} (\S\ref{sec:linear_transformation}), $\diffy{f}_x$, i.e. the
Derivative of $f$ \emph{at} $x$, that maps Differentials $\diffy{x}$ in $X$ to
Differentials $\diffy{y}$ in $Y$

\fist the archetypical example of \emph{Differential} (\S\ref{sec:differential})
is the Differential in the de Rham Complex $\Omega^\bullet(X)$ of a Smooth
manifold $X$, give by Differentiation of Smooth Functions and Differential Forms
(\S\ref{sec:differential_form})

(Real) Analytic Definition -- if $f : \reals^n \rightarrow \reals$ is a
Function, the Derivative $\diffy{f}_x$ of $f$ at Point $x$ is the Linear
Transformation $\diffy{f}_x = L : \reals^n \rightarrow \reals$ found by the
Limiting Process (\S\ref{sec:sequence_limit}):
\[
  \lim_{h\rightarrow{0}} \frac{f(x + h) - f(x) - Lh}{|h|} = 0
\]
when such a Limit exists, $f$ is said to be \emph{Differentiable} at $x$ with
Derivative $\diffy{f}_x = L$; when $n=1$, the Linear Transformation $L : \reals
\rightarrow \reals$ is Scalar Multiplication by $\lambda \in \reals$, denoted
$f'(x)$

for $f : \reals^n \rightarrow \reals$ Differentiable at every Point $x$ of its
Domain (i.e. $f$ is a \emph{Differentiable Function}
\S\ref{sec:differentiable_function}), there is a \emph{Global Derivative
  Function} of type $\reals^n \times \reals^n \rightarrow \reals$ mapping Pairs
$(x,\delta) \mapsto \diffy{f}_h(\delta)$; asking whether this Function is itself
Differentiable is asking whether the Differentiation process can be Iterated
(FIXME: clarify); a Function which is $n$-times Differentiable for all $n \in
\nats$ is called \emph{Infinitely Differentiable} or \emph{Smooth}; every
Differentiable Function is necessarily Continuous (Locally Lipschitz)

the Class of Functions with Continuous $n$-th Derivatives is denoted:
\[
  C^n(\reals^k)
\]
(FIXME: \url{https://ncatlab.org/nlab/show/differentiation} shows this as
$C^n(R^n) instead of C^n(R^k)$ ???)
and Continuous Functions $f : \reals^n \rightarrow \reals$ form a Filtration
(\S\ref{sec:filtration}):
\[
  \cdots \subset C^n(\reals^k) \subset \cdots \subset C^1(\reals^k) \subset
    C^0(\reals^k)
\]
with Intersection:
\[
  C^\infty(\reals^k) := \bigcap_{n\geq{0}} C^n(\reals^k)
\]

\emph{Differentiation} then gives a Function:
\[
  D : C^{n+1}(\reals^k) \rightarrow C^n(\reals^k \times \reals^k)
\]
and restricting to the Smooth case:
\[
  D : C^\infty(\reals^k) \rightarrow C^\infty(\reals^k \times \reals^k)
\]
this can also be defined as an \emph{(Endo-)functor} on the Category of Smooth
Manifolds and Smooth Maps:
\[
  \mathrm{d} : \cat{Diff} \rightarrow \cat{Diff}
\]
sending:
\begin{itemize}
  \item Smooth Manifolds $X$ to Tangent Bundles (\S\ref{sec:tangent_bundle}) $T
    X$, with Points of $T X$ being Ordered Pairs $(x, v)$ where $v$ is a Tangent
    Vector at $x$, i.e. an (Augmented) Derivation
    (\S\ref{sec:augmented_derivation}) $v : C^\infty(X) \rightarrow \reals$ on
    the Algebra of Smooth Functions, Augmented by Evaluation $\mathrm{ev}_x :
    C^\infty(X) \rightarrow \reals$ at $x$ (FIXME: clarify)
  \item Smooth Functions $f : X \rightarrow Y$ to Derivatives (FIXME: nlab calls
    this here a ``Differential'' of $f$ but that seems to be inconsistent with
    the terminology used elsewhere) $\diffy{f} : TX \rightarrow TY$; if $\gamma
    : [-1,1] \rightarrow X$ is a Path in $X$ representing a Vector
    $v \in T_x{X}$ then $(\diffy{f})(v) \in T_{f(x)}Y$ is the Vector represented
    by the Path $[-1, 1] \xrightarrow{\gamma} X \xrightarrow{f} Y$
    (FIXME: clarify)
\end{itemize}
Equivalently, a Smooth map $f : X \rightarrow Y$ induces an \emph{Algebra Map}:
\[
  f^* = - \circ f : C^\infty(Y) \rightarrow C^\infty(X)
\]
by Composition sending a Derivation $v : C^\infty(X) \rightarrow \reals$ to a
(Augmented) Derivation $v \circ f^* : C^\infty(Y) \rightarrow \reals$ Augmented
by $\mathrm{ev}_{f(x)} : C^\infty(Y) \rightarrow \reals$; concretely:
\[
  v \circ f^*(\psi) = v(\psi \circ f)
\]
for $\psi \in C^\infty(Y)$; then the Derivative (FIXME: nlab uses
``Differential'' here ???) $\diffy{f} : T X \rightarrow T Y$ is defined by:
\[
  \diffy{f}(x,v) = (f(x), v \circ f^*)
\]
By either description Differentiation is \emph{Functorial}, a fact expressed in
the \emph{Chain Rule} (\S\ref{sec:chain_rule}):
\[
  \diffy{(f \circ g)}_x = \diffy{f}_{g(x)} \circ \diffy{g}_x
\]

in the ``Context'' $H$ (FIXME: clarify) of Synthetic Differential Geometry
(\S\ref{sec:synthetic_differential_geometry}), the Differentiation
(Endo-)functor $\mathrm{d} : \cat{Diff} \rightarrow \cat{Diff}$ on the Category
of Smooth Manifolds and Smooth Maps is the Internal Hom
(\S\ref{sec:internal_hom}) out of the Infinitesimal Interval
(\S\ref{sec:infinitesimal_interval}) $D$:
\[
  \mathrm{d} = [D,-] : H \rightarrow H
\]
where $D \hookrightarrow \reals$ is the Smooth Locus (\S\ref{sec:smooth_locus})
defined by $x^2 = 0$ (in Toposes that Model Synthetic Differential Geometry this
is regarded as an Infinitesimal Neighborhood about a Point)

``this description'' as an Internally Representable Functor (FIXME: clarify)
leads to the Theorem that \emph{Differentiation is a Product-preserving
  Functor}. (\S\ref{sec:product_preserving_functor})

TODO: Differential $\diffy{f}$ as a Morphism of Tangent Bundles



\subsubsection{Higher Derivative}\label{sec:higher_derivative}

\emph{Higher-order}



\subsubsection{Time Derivative}\label{sec:time_derivative}

\begin{itemize}
  \item Velocity -- (First) Time Derivative of Position
  \item Force -- Time Derivative of Momentum
  \item Power -- Time Derivative of Energy
  \item (Electric) Current -- Time Derivative of Electric Charge
\end{itemize}

In a Dynamical System (\S\ref{sec:dynamical_system}), the \emph{Evolution
  Function} $\Phi^t$ is often the Solution of a Differential Equation of Motion
$\dot{x} = v(x)$ giving the Time Derivative of a Trajectory (Integral Curve)
$x(t)$ on the Phase Space starting at some Point $x_0$.



% --------------------------------------------------------------------
\subsection{Differentiable Function}\label{sec:differentiable_function}
% --------------------------------------------------------------------

For $f$ defined on Open Interval $(a,b) \subset \reals$, $f$ is
Differentiable at $x \in (a,b)$ if Limit $f'(x) = \lim_{h \rightarrow
  0} \frac{f (x+h) - f(x)}{h}$ exists.

Differentiable at $a$ Implies Continuous (\S\ref{sec:continuous_function}) at
$a$

a Complex Differentiable Function is called \emph{Holomorphic}
(\S\ref{sec:holomorphic_function})

\fist Differential Equation (\S\ref{sec:differential_equation})

\fist cf. Locally Linear Transformation (\S\ref{sec:locally_linear})

A Polynomial (\S\ref{sec:polynomial}), being the Sum of Differentiable
Functions, is Differentiable everywhere

For $f$ Differentiable on an Interval $I$, if $\forall x \in I, f'(x)
> 0$, $f$ is Strictly Increasing and if $\forall x \in I, f'(x) < 0$,
$f$ is Strictly Decreasing (see Monotonic Functions
\S\ref{sec:monotonic_function}).



\subsubsection{Critical Point}\label{sec:critical_point}

or \emph{Stationary Point}



\paragraph{Maximum}\label{sec:maximum}\hfill

Upper Bound (\S\ref{sec:upper_bound})

Least Upper Bound (\S\ref{sec:least_upperbound})

Local Maxima

Second Derivative Test

Multivariable -- Gradient (\S\ref{sec:gradient}) equal to the Zero Vector:
$\nabla{f} = \vec{0}$

\fist \emph{Saddle Points} (\S\ref{sec:saddle_point}) are specific to
Multivariable Functions



\paragraph{Minimum}\label{sec:minimum}\hfill

Lower Bound (\S\ref{sec:lower_bound})

Greatest Lower Bound (\S\ref{sec:greatest_lowerbound})

Local Minima

Second Derivative Test

Multivariable -- Gradient (\S\ref{sec:gradient}) equal to the Zero Vector:
$\nabla{f} = \vec{0}$

\fist \emph{Saddle Points} (\S\ref{sec:saddle_point}) are specific to
Multivariable Functions

Convex Functions (\S\ref{sec:convex_function}) have the property that Local
Minimum is necessarily a Global Minimum



\paragraph{Second Derivative Test}\label{sec:second_derivative_test}\hfill

\fist multivariable Local Optimality Conditions (\S\ref{sec:local_optimality})

the Second Derivative Test is equivalent to testing the Determinants of the two
Submatrices of the Hessian Matrix (\S\ref{sec:hessian_matrix}) of Second-order
Partial Derivatives: a Matrix is Positive Definite
(\S\ref{sec:positive_definite}) if all Principal Minors (\S\ref{sec:minor}) are
Positive



\subsubsection{Differentiability Class}\label{sec:differentiability_class}

$\mathcal{C}^k$

$\mathcal{C}^0$ -- all Continuous Functions (\S\ref{sec:continuous_functions})

$\mathcal{C}^1$ -- Continuous First Derivative: Continuously Differentiable
Function (\S\ref{sec:continuously_differentiable})

$\mathcal{C}^2$ -- Continuous Second Derivative

$\mathcal{C}^\infty$ -- Continuous for all Derivatives: Smooth Function
(\S\ref{sec:smooth_function})



\subsubsection{Continuously Differentiable}
\label{sec:continuously_differentiable}

A Function $f$ is \emph{Continuously Differentiable} if the Derivative $f'(x)$
exists and is a Continuous Function.

Differentiability Class (\S\ref{sec:differentiability_class}) $C^1$

Solutions to the One-dimensional Dynamical System
(\S\ref{sec:dynamical_system}) $\dot{x} = f(x)$ exist and are unique if $f(x)$
and $f'(x)$ are Continuous ($f$ is ``Continuously Differentiable'')



\subsubsection{Smooth Function}\label{sec:smooth_function}

Differentiability Class (\S\ref{sec:differentiability_class}) $C^{\infty}$

the Commutative Algebra of Smooth Functions on a Manifold $M$ is denoted
$C^\infty(M)$ %FIXME

any Closed Subset of $\reals^n$ is the Zero Set (\S\ref{sec:zero_set}) of a
Smooth Function on $\reals^n$



\paragraph{Analytic Function}\label{sec:analytic_function}\hfill

A Function is \emph{Analytic} if and only if its Taylor Series
(\S\ref{sec:taylor_series}) about $x_0$ Converges to the Function in some
Neighborhood for every $x_0$ in its Domain.

Differentiability Class (\S\ref{sec:differentiability_class}) $C^{\omega}$

Complex Dynamics (\S\ref{sec:complex_dynamics})

Trigonometric Functions (\S\ref{sec:trigonometric_function})

a Complex Function is Analytic if and only if it is Holomorphic
(\S\ref{sec:holomorphic_function}), i.e. it is Complex Differentiable \fist
Cauchy-Riemann Conditions (\S\ref{sec:cauchy_riemann})

the Real and Imaginary parts of any Analytic Function define a \emph{Conformal
  Mapping} (\S\ref{sec:conformal_map}) --FIXME: clarify

if a Sequence of Analytic Functions Converges Uniformly
(\S\ref{sec:uniform_convergence}) in a region $S$ of the Complex Plane then the
Limit is Analytic in $S$-- this demonstrates that the Complex Functions are
more ``well-behaved'' than the Real Functions since the Uniform Limit of
Analytic Functions on a Real Interval do not need to be Differentiable
(\S\ref{sec:nowhere_differentiable})

Analytic Variety (\S\ref{sec:analytic_variety}) -- defined locally as the Set of
common Solutions of several Equations involving Analytic Functions



\subparagraph{Analytic Continuation}\label{sec:analytic_continuation}



\subsubsection{Chain Rule}\label{sec:chain_rule}

(nlab):

the statement that \emph{Differentiation} (\S\ref{sec:derivative}) $\mathrm{d} :
\cat{Diff} \rightarrow \cat{Diff}$ is a \emph{(Endo-)Functor} on $\cat{Diff}$,
the Category of Smooth Manifolds and Smooth Maps

(wiki):

Formula for computing the Derivative of the \emph{Composition} of two or more
Functions:
\[
  (f \circ g)' = (f' \circ g) g'
\]
or equivalently when $F(x) = f(g(x))$:
\[
  F'(x) = f'(g(x))g'(x)
\]

Leibniz Notation for Variable $z$ Dependent on Variable $y$ Dependent on
Independent Variable $x$:
\[
  \frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
\]
and if $z = f(y)$ and $y = g(x)$ then:
\[
  \frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
    = f'(y)g'(x) = f'(g(x))g'(x)
\]

\fist cf. \emph{Substitution Rule} (\S\ref{sec:substitution_rule}) for
Integration

Multivariable Chain Rule

(nlab):

consequence of the \emph{Functorial} nature of Differentiation
(\S\ref{sec:derivative})

Differentiation as an \emph{(Endo-)functor} on the Category of Smooth Manifolds
and Smooth Maps:
\[
  \mathrm{d} : \cat{Diff} \rightarrow \cat{Diff}
\]
sending:
\begin{itemize}
  \item Smooth Manifolds $X$ to Tangent Bundles (\S\ref{sec:tangent_bundle}) $T
    X$, with Points of $T X$ being Ordered Pairs $(x, v)$ where $v$ is a Tangent
    Vector at $x$, i.e. an (Augmented) Derivation
    (\S\ref{sec:augmented_derivation}) $v : C^\infty(X) \rightarrow \reals$ on
    the Algebra of Smooth Functions, Augmented by Evaluation $\mathrm{ev}_x :
    C^\infty(X) \rightarrow \reals$ at $x$ (FIXME: clarify)
  \item Smooth Functions $f : X \rightarrow Y$ to Derivatives (FIXME: nlab calls
    this here a ``Differential'' of $f$ but that seems to be inconsistent with
    the terminology used elsewhere) $\diffy{f} : TX \rightarrow TY$; if $\gamma
    : [-1,1] \rightarrow X$ is a Path in $X$ representing a Vector
    $v \in T_x{X}$ then $(\diffy{f})(v) \in T_{f(x)}Y$ is the Vector represented
    by the Path $[-1, 1] \xrightarrow{\gamma} X \xrightarrow{f} Y$
    (FIXME: clarify)
\end{itemize}
Equivalently, a Smooth map $f : X \rightarrow Y$ induces an \emph{Algebra Map}:
\[
  f^* = - \circ f : C^\infty(Y) \rightarrow C^\infty(X)
\]
by Composition sending a Derivation $v : C^\infty(X) \rightarrow \reals$ to a
(Augmented) Derivation $v \circ f^* : C^\infty(Y) \rightarrow \reals$ Augmented
by $\mathrm{ev}_{f(x)} : C^\infty(Y) \rightarrow \reals$; concretely:
\[
  v \circ f^*(\psi) = v(\psi \circ f)
\]
for $\psi \in C^\infty(Y)$; then the Derivative (FIXME: nlab uses
``Differential'' here ???) $\diffy{f} : T X \rightarrow T Y$ is defined by:
\[
  \diffy{f}(x,v) = (f(x), v \circ f^*)
\]



\paragraph{Product Rule}\label{sec:product_rule}\hfill

\emph{Product Law} or \emph{Leibniz Rule}

(wiki):

can be considered a special case of the Chain Rule

(nlab):

for Differentiable Functions $f, g : X \rightarrow \reals$, the
\emph{Derivative} (\S\ref{sec:derivative}) of their Pointwise Product
(\S\ref{sec:pointwise_product}) $fg$ is given by:
\[
  \diffy{(fg)} = (\diffy{f})g + f(\diffy{g})
\]

generalized to Differential Forms (\S\ref{sec:differential_form}), the Product
Law states that $\mathrm{d}$ is a \emph{Derivation} (\S\ref{sec:derivation}) of
Degree $+1$ on the Graded Commutative Algebra
(\S\ref{sec:graded_commutative_algebra}) of Differential Forms:
\[
\diffy{f \wedge g} = (\diffy{f}) \wedge g +
  (-1)^{\mathrm{deg}\ f}f \wedge (\diffy{g})
\]
(FIXME: clarify derivation degree)

Categorification of Product Rule: Combinatorial Species
(\S\ref{sec:combinatorial_species}) %TODO

\fist Integration by Parts (\S\ref{sec:integration_by_parts})

\fist Derivation (Differential Algebra \S\ref{sec:derivation}) -- an Unary
Linear Function that satisfies the Leibniz Product Rule



\subsubsection{Cauchy Mean Value Theorem}
\label{sec:cauchy_mean_value}

For Functions $f$, $g$ Continuous on $[a,b]$, Differentiable on
$(a,b)$ then $\exists c \in (a,b)$ such that:
\[
  f'(c) (g(b) - g(a)) = g'(c) (f(b) - f(a))
\]



\subsubsection{Lagrange Mean Value Theorem}
\label{sec:lagrange_mean_value}

$f$ Continuous on $[a,b]$ and Differentiable on $(a,b)$ then $\exists
c \in (a,b)$ such that $f(b) - f(a) = f'(c)(b-a)$ or:
\[
  f'(c) = \frac{f(b) - f(a)}{b - a}
\]



\subsubsection{Rolle's Theorem}\label{sec:rolles_theorem}

for $f$ Continuous on $[a,b]$ and Differentiable on $(a,b)$ with $f(a)
= f(b)$, then $\exists c \in (a,b)$ such that $f'(c) = 0$



\subsubsection{Nowhere Differentiable Function}
\label{sec:nowhere_differentiable}

the Set of Nowhere-differentiable Real-valued Functions on the Closed Interval
$[0,1]$ is Comeagre (\S\ref{sec:comeagre_set}) in the Vector Space $C([0,1];
\reals)$ of Continuous Real-valued Functions on $[0,1]$ with the Topology of
Uniform Convergence (\S\ref{sec:uniform_convergence}); the collection of
Functions that are Differentiable at a single point of $[0,1]$ has Wiener
Measure (\S\ref{sec:wiener_measure}) $0$, even when taking Finite-dimensional
``slices'' of $C([0,1];\reals)$, in the sense that the Nowhere-differentiable
Functions form a Prevalent Subset (\S\ref{sec:prevalent_set}) of
$C([0,1]; \reals)$

if a Sequence of Analytic Functions Converges Uniformly
(\S\ref{sec:uniform_convergence}) in a region $S$ of the Complex Plane then the
Limit is Analytic in $S$-- this demonstrates that the Complex Functions are
more ``well-behaved'' than the Real Functions since the Uniform Limit of
Analytic Functions on a Real Interval do not need to be Differentiable
(\S\ref{sec:nowhere_differentiable})



\paragraph{Weierstrass Function}\label{sec:weierstrass_function}\hfill

Continuous but Nowhere Differentiable

Fourier Series (\S\ref{sec:fourier_series})



% --------------------------------------------------------------------
\subsection{Differential Operator}\label{sec:differential_operator}
% --------------------------------------------------------------------

$\nabla$

an Operator ``built'' from the Differentiation Operator (\S\ref{sec:derivative})

Operator defined as a Function of the Differentiation Operator

(FIXME: clarify)

\fist see also Integral Operator (\S\ref{sec:integral_operator})

\emph{Basic Differential Operator} is a mapping from Differentiable Functions
to the Differentiable Functions consisting of Deriving the Function one or
several times:
\[
  \frac{d^i}{dx^i}
\]

\begin{itemize}
  \item Del (Gradient) Operator (\S\ref{sec:gradient})
  \item Laplacian Operator (\S\ref{sec:laplace_operator})
  \item $\Theta$ (Homogeneity) Operator (TODO: xref)
\end{itemize}

\fist a $D$-module (\S\ref{sec:d_module}) is a Module over a Ring $D$ of
Differential Operators; an approach to the theory of Linear Partial Differential
Equations (\S\ref{sec:linear_pde})

\fist a \emph{Weyl Algebra} (\S\ref{sec:weyl_algebra}) is a Ring of
Differential Operators with Polynomial Coefficients in one Variable:
\[
  f_m(X)\partial^m_X + f_m{-1}(X)\partial^{m-1}_X + \cdots +
    f_1(X)\partial_X + f_0(X)
\]
and is Isomorphic to Quotient of the Free Algebra (\S\ref{sec:free_algebra}) on
two Generators, $X$ and $Y$, by the Ideal (\S\ref{sec:ring_ideal}) generated by
the Element $YX - XY - 1$



\subsubsection{Theta Operator}\label{sec:theta_operator}

\subsubsection{Elliptic Operator}\label{sec:elliptic_operator}

\paragraph{Laplace Operator}\label{sec:laplace_operator}\hfill

simplest Elliptic Operator

$\Delta = \nabla^2$

Discrete Laplace Operator (Graph Theory \S\ref{sec:discrete_laplace})

Harmonic Functions (\S\ref{sec:harmonic_function}) are exactly those Functions
that which lie in the Kernel of the Laplace Operator

\fist cf. Laplacian (\S\ref{sec:laplacian}), Laplace Transform
(\S\ref{sec:laplace_transform})
%FIXME same concepts?



\subsubsection{Schwarzian Derivative}\label{sec:schwarzian_derivative}

Non-linear Differential Operator; invariant under all Linear Fractional
Transformations (TODO: xref)



\subsubsection{Linear Differential Operator}
\label{sec:linear_differential_operator}

a \emph{Linear Differential Operator} is a Linear Combination of Basic
Differential Operators with Differentiable Functions
(\S\ref{sec:differentiable_function}) as Coefficients

is a Linear Transformation (\S\ref{sec:linear_transformation})

Linear Differential Operators form a Vector Space over the Real or Complex
Numbers, and also a Free Module over the Ring of Differentiable Functions



% ====================================================================
\section{Integral Calculus}\label{sec:integral_calculus}
% ====================================================================

\emph{Integral Calculus in One Variable}

\fist Multivariable Calculus (\S\ref{sec:multivariable_calculus})

(wiki):

in Integration Theory, specifying a Measure (\S\ref{sec:measure}) allows the
definition of Integrals on Spaces more general than Subsets of Euclidean Space



% --------------------------------------------------------------------
\subsection{Antiderivative}\label{sec:antiderivative}
% --------------------------------------------------------------------

\emph{Indefinite Integral} or \emph{Primitive Integral}

\fist \emph{Antidifference} (Indefinite Sum \S\ref{sec:antidifference})
$\Delta^{-1}$ -- Discrete equivalent of the Antiderivative

\fist cf. Derivative (\S\ref{sec:derivative}), Antiderivation
(\S\ref{sec:antiderivation})

\fist related to \emph{Definite Integrals} (\S\ref{sec:integral}) through the
Fundamental Theorem of Calculus (\S\ref{sec:fundamental_calculus_theorem})-- the
Fundamental Theorem of Calculus states that Antidifferentiation \emph{is the
  same as} Integration

If $f$ is Continuous on $[a,b]$ then:
\[
  F(x) = \int_a^x f
\]
for all $x \in [a,b]$ and $F$ is Differentiable on $(a,b)$ and $F' = f$.

\emph{Antidifferentiation Operator} (\emph{Indefinite Integration Operator})

is a Linear Operator (\S\ref{sec:linear_operator})

\fist Integral Operators (\S\ref{sec:integral_operator}) are ``built'' from the
Antidifferentiation Operator (FIXME: clarify)

notations:
\begin{itemize}
  \item \emph{Leibniz Notation} --
    $\int \cdot \diffy{x}, \iint \cdot \diffy{x^2}$
  \item \emph{Lagrange Notation} -- $f^{(-1)}(x), f^{(-2)}(x), \ldots$
  \item \emph{Newton Notation} -- $y^{'}, y^{''}, \ldots$ (with respect
    to Implicit Independent Variable, usually Time $t$)
  \item \emph{Euler Notation} -- $D^{-1}_x{f}, D^{-2}_x{f}, \ldots$ (with
    $D_x^{-n}$ a Linear  Operator \S\ref{sec:linear_operator})
\end{itemize}

solutions to Homogeneous Linear Differential Equations
(\S\ref{sec:homogeneous_linear_differential}) may be expressed in
terms of Integrals

Richardson's Theorem (\S\ref{sec:richardsons_theorem}) makes a statement on the
solvability of the ``Integration Problem'' for a certain class of Expressions



% --------------------------------------------------------------------
\subsection{Integral}\label{sec:integral}
% --------------------------------------------------------------------

\emph{Definite Integral}

\emph{Integrand} -- the Function to be Integrated

notation:
... TODO

\emph{Standard Integral} of a Real-valued Function $f(x)$ of a Real Variable $x$
on the Interval $[a,b]$:
\[
  \int_a^b f(x) \mathrm{d}x
\]

\fist Finite Summation (\S\ref{sec:summation}) -- Discrete quivalent of
Definite Integration

\fist related to Antiderivative (Indefinite Integral \S\ref{sec:antiderivative})
through the Fundamental Theorem of Calculus
(\S\ref{sec:fundamental_calculus_theorem})--
the Fundamental Theorem of Calculus states that Antidifferentiation \emph{is the
  same as} Integration

\fist Multiple Integral (\S\ref{sec:multiple_integral}) in Multivariable
Calculus

\fist Numerical Anslysis (\S\ref{sec:numerical_analysis}) applied to Evaluating
Integrals -- Numerical Integration (\emph{Quadrature}
\S\ref{sec:numerical_integration})

\fist It\^o Integral (Stochastic Calculus \S\ref{sec:ito_integral})

is a Linear Operator (\S\ref{sec:linear_operator}) --FIXME: clarify

(wiki):

generally a Differential $k$-form (\S\ref{sec:differential_form}) can be
Integrated over $k$-dimensional Chains (\S\ref{sec:k_chain}); for $k=0$ this is
just Evaluation of a Function at Points

in Integration Theory, specifying a Measure (\S\ref{sec:measure}) allows the
definition of Integrals on Spaces more general than Subsets of Euclidean Space

\asterism

\emph{Integration by Substitution} (\emph{$u$-substitution})



\subsubsection{Fundamental Theorem of Calculus}
\label{sec:fundamental_calculus_theorem}

(wiki):

the Fundamental Theorem of Calculus states that Antidifferentiation
(\S\ref{sec:antiderivative}) \emph{is the
  same as} Integration (\S\ref{sec:integral})

for Function $f$ Continuous (\S\ref{sec:continuous_function}) on the Interval
$[a,b]$, and $F$ with Derivative $f$:
\[
  F' = f
\]
on the Interval $(a,b)$, then:
\[
  \int_a^b f(x) dx = F(b) - F(a)
\]

Additionally, for every $x$ in the Interval $(a,b)$:
\[
  \frac{d}{dx}\int^x_a f(t) dt = f(x)
\]

\fist Subsitution Rule (\S\ref{sec:substitution_rule})

\fist Integral Theorems of Vector Calculus
(\S\ref{sec:integral_theorems}) in Multivariable Calculus

(Villate06): Solutions of an ODE (\S\ref{sec:ode}) of the form $y' = f(x)$
follow from the Fundemental Theorem of Integral Calculus:
\[
  y(x) = \int f(x) \diffy{x} + c
\]
where $c$ is an arbitrary Constant



\subsubsection{Darboux Integral}\label{sec:darboux_integral}

Upper Darboux Sum

Lower Darboux Sum



\subsubsection{Riemann Integral}\label{sec:riemann_integral}

$\int_a^b f(x) dx$

Interval Partition (\S\ref{sec:interval_partition})

Monotone Bounded Functions are Riemann Integrable

Continuous Functions are Reiemann Integrable

Riemann Integrable on $[a,b]$, Least Upper Bound of Lower Darboux Sums
is equal to the Greatest Lower Bound of the Upper Darboux Sums, value
denoted by $\int_a^b f$



\subsubsection{Lebesgue Integral}\label{sec:lebesgue_integral}

\fist Measure Theory (Part \ref{part:measure_theory})



\subsubsection{Wallis Integral}\label{sec:wallis_integral}

\subsubsection{Gaussian Integral}\label{sec:gaussian_integral}

or \emph{Euler-Poisson Integral}

Integral of the Gaussian Function (TODO) $e^{-x^2}$ over the entire Real Line

\fist Functional Integral (\S\ref{sec:functional_integration})



\subsubsection{Integration by Parts}\label{sec:integration_by_parts}

TODO

\fist Product Rule (\S\ref{sec:product_rule})

$\int{uv'} = uv - \int{u'v}$



% --------------------------------------------------------------------
\subsection{Integrable Function}\label{sec:integrable_function}
% --------------------------------------------------------------------

%FIXME: is this the same as Riemann-integrable ???

a Bounded Function (\S\ref{sec:bounded_function}) on a Compact Interval $[a,b]$
is Riemann Integrable if and only if it is Continuous
(\S\ref{sec:continuous_function}) ``Almost Everywhere'', i.e. Set of Points of
Discontinuity has Lebesgue Measure Zero (\S\ref{sec:lebesgue_measure})

cf. Integral Equation (\S\ref{sec:integral_equation})

\fist Integrable System (\S\ref{sec:integrable_system})

\fist Functional Integration (\S\ref{sec:functional_integration}): Integration
is done over a Domain which is a Function Space instead of a Domain of an
``ordinary'' Space

$f$ Bounded on Closed Bounded $[a,b]$, $\forall \varepsilon >0$, there
exists a Partition (\S\ref{sec:interval_partition}) $P$ of $[a,b]$
such that $0 \leq U(f,P) - L(f,P) < \varepsilon$ %FIXME xref upper lower

Monotone Functions (\S\ref{sec:monotonic_function}) and Continuous
Functions (\S\ref{sec:continuous_function}) are always (Riemann)
Integrable

$L(f,P) \leq \int_a^b f \leq U(f,P)$

For $c$ a Constant, if $f$ is Integrable then $cf$ is Integrable: $c
\int_a^b f = \int_a^b c f$

For $f$, $g$ Integrable on $[a,b]$, then $f + g$ is Integrable:
$\int_a^b (f+g) = \int_a^b f + \int_a^b g$

For $f$ Integrable on $[a,b]$ and $a < c < b$ then $f$ is Integrable
on $[a,c]$ and $[c,b$ and $\int_a^b f = \int_a^c f + \int_c^b f$

For $f$, $g$ Integrable on $[a,b]$ and $f \leq g$ on $[a,b]$, then
$\int_a^b f \leq \int_a^b g$

For Continuous $g$ and Integrable $f$, then $g \circ f$ is Integrable

For Integrable $f$, $|f|$ is Integrable and $|\int_a^b f| \leq
\int_a^b |f|$



\subsubsection{Substitution Rule}\label{sec:substitution_rule}

Fundamental Theorem of Calculus (\S\ref{sec:fundamental_calculus_theorem})

\fist cf. \emph{Chain Rule} (\S\ref{sec:chain_rule}) for Differentiation



\subsubsection{Integral Mean Value Theorem}
\label{sec:integral_mean_value}

For $f$ Continuous on $[a,b]$, there is a $c \in [a,b]$ such that
$\int_a^b f = f(c)(b - a)$



\subsubsection{Integral Transform}\label{sec:integral_transform}

(wiki):

Transformation of an Equation from original Domain into another Domain where it
is solved more easily and then mapped back into the original Domain using the
Inverse of the Integral Transform

every Integral Transform is a Linear Operator (\S\ref{sec:linear_operator})

\emph{Schwartz Kernel Theorem}: if the Kernel of an Integral Transform is
allowed to be a Generalized Function (\S\ref{sec:generalized_function}) then
all Linear Operators are Integral Transforms

an \emph{Integral Transform} $T$ has the form:
\[
  (T f)(u) = \int_{t_1}^{t_2} f(t) K(t,u) dt
\]
mapping an input Function $f$ to another Function $T f$:
\[
  f \mapsto T f
\]
where $K$ is the \emph{Kernel Function} (or \emph{Integral Kernel} or
\emph{Nucleus}) of the Transform

some Kernels have associated an Inverse Kernel $K^{-1}(u,t)$, yielding an
Inverse Transform:
\[
  f(t) = \int_{u_1}^{u_2} (T f)(u) K^{-1}(u,t) du
\]

a \emph{Symmetric Kernel} is one that is unchanged when permuting the two
Variables



\paragraph{Fourier Transform}\label{sec:fourier_transform}\hfill

Fourier Transform is a single Projection ``out'' of a Laplace Transform
(\S\ref{sec:laplace_transform}) --FIXME: clarify

removes the requirement of Fourier Series (\S\ref{sec:fourier_series}) being
restricted to expressing functions on Finite Intervals to being able to express
functions on Infinite Intervals

\fist Frequency Domain (\S\ref{sec:frequency_domain})

\fist Signal Processing (\S\ref{sec:signal_processing})

\fist Wavelet Transform (Image Processing \S\ref{sec:wavelet_transform})

(wiki):

a \emph{Periodic Signal} can be analyzed using a \emph{Discrete Frequency
  Domain}, i.e. the Fourier Transform of a Periodic Signal only has energy at a
\emph{Base Frequency} and its \emph{Harmonics}

a \emph{Periodic Signal} has a \emph{Discrete Frequency Spectrum}

a \emph{Discrete-time Signal} has a \emph{Periodic Frequency Spectrum}

a \emph{Discrete, Periodic Signal} has a \emph{Periodic and Discrete Frequency
  Spectrum}



\subparagraph{Continuous Fourier Trasform}
\label{sec:continuous_fourier_transform}\hfill

\subparagraph{Discrete Fourier Trasform (DFT)}\label{sec:dft}\hfill

\subparagraph{Discrete-Time Fourier Trasform (DTFT)}\label{sec:dtft}\hfill



\paragraph{Laplace Transform}\label{sec:laplace_transform}\hfill

$\mathcal{L}\{f(t)\} = F(s)$

takes a Function $f(t)$ of a Real Variable $t$ (usually Time) to a Function
$F(s)$ of a Complex Variable $s$ (Complex Frequency
\S\ref{sec:complex_frequency})

\fist Frequency Domain (\S\ref{sec:frequency_domain})

\fist Fourier Transform (\S\ref{sec:fourier_transform}) is a single Projection
``out'' of a Laplace Transform (FIXME: clarify)

\fist cf. Laplacian (\S\ref{sec:laplacian}), Laplace Operator
(\S\ref{sec:laplace_operator}) %FIXME same concepts?

\fist Linear Time-invariant Systems (\S\ref{sec:lti_system})

\url{https://www.quora.com/Intuitively-speaking-what-does-a-Laplace-transformation-represent}:

``A convenient way of turning a calculus into algebra, sin/cos/exp functions
into rational polynomials, and altogether making it faster to solve problems
that come up in differential equations.

Why? Because it turns differentiation into multiplication by $s$, integration
into division by $s$, and the sin/cos/exp functions into simple rational
polynomials in $s$.''

\[
  \mathcal{L} \{ f(t) \} = F(s) = \int_0^\infty f(t) e^{-st} dt
\]

$\mathcal{L}\{t^0\} = \frac{1}{s}$, $s > 0$

$\mathcal{L}\{t\} = \frac{1}{s^2}$

$\mathcal{L}\{t^2\} = \frac{2}{s^3}$

$\mathcal{L}\{t^3\} = \frac{6}{s^4}$

$\vdots$

$\mathcal{L}\{t^n\} = \frac{n!}{s^{n+1}}$

$\mathcal{L}\{e^{at}\} = \frac{1}{s-a}$, $s > a$

$\mathcal{L}\{e^{at}f(t)\} = F(s-a)$

$\mathcal{L}\{\sin at\} = \frac{a}{s^2+a^2}$

$\mathcal{L}\{ f'(t) \} = s \mathcal{L} \{f(t)\} - f(0)$

$\mathcal{L}\{ f''(t) \} = s^2 \mathcal{L} \{f(t)\} - sf(0) - f'(0)$

$\vdots$

Unit Step Function (\S\ref{sec:unit_step_function}) $u_c(t)$

$\mathcal{L}\{ u_c(t)f(t-c) \} = e^{-cs}\mathcal{L}\{f(t)\}$

Dirac Delta Function (\S\ref{sec:dirac_delta}) $\delta(t)$

$\mathcal{L}\{ \delta(t-c) \} = e^{-cs}$

$\mathcal{L}\{ \delta(t-c)f(t) \} = e^{-cs}f(c) $

Inverse Laplace Transform $\mathcal{L}^{-1}$

the Laplace Transform is a \emph{Linear Operator}
(\S\ref{sec:linear_operator}):
\[
  \mathcal{L} \{ c_1 f(t) + c_2 g(t) \}
    = c_1\mathcal{L}\{f(t)\} + c_2\mathcal{L}\{g(t)\}
\]

the Laplace Transform is a Holomorphic Function
(\S\ref{sec:holomorphic_function}) of the Variable $s$ with a Power Series
representation

Frequency-domain approach for Continuous Time Signals where the System may be
Stable \emph{or} Unstable (TODO: clarify)

in Control Theory (\S\ref{sec:control_theory}), Systems are often Transformed
from the Time Domain to the Frequency Domain using Laplace Transform and the
Transformed System's Zeros (\S\ref{sec:complex_zero}) and Poles
(\S\ref{sec:complex_pole}) are Analyzed in the Complex Plane:

Imaginary Component of Poles indicates what Frequency the System will Resonate
at, and Real Component of Poles indicates the magnitude of the Resonance, and
Zeros indicate Non-resonating Exponential Decays

Convolution (\S\ref{sec:convolution})



\subparagraph{Z-transform}\label{sec:z_transform}

Discrete-time Laplace Transform

Digital Signal Processing (\S\ref{sec:dsp})



\paragraph{Legendre Transform}\label{sec:legendre_transform}\hfill

the Hamiltonian (\S\ref{sec:hamiltonian_system}) and the Routhian (TODO) can be
obtained from the Lagrangian (\S\ref{sec:lagrangian_system}) by the Legendre
Transform



% --------------------------------------------------------------------
\subsection{Integral Operator}\label{sec:integral_operator}
% --------------------------------------------------------------------

an Operator ``built'' from the Antidifferentiation Operator (Indefinite
Integration Operator \S\ref{sec:antiderivative})

\fist see also Differential Operator (\S\ref{sec:differential_operator})



% --------------------------------------------------------------------
\subsection{Integral Equation}\label{sec:integral_equation}
% --------------------------------------------------------------------

generalization of Eigenvalue Equations (FIXME)

cf. Integrable Function (\S\ref{sec:integrable_function})

Fredholm Equation -- both Limits of Integration are Fixed

Volterra Equation -- one Limit of Integration is Variable

First Kind -- Unknown Function only occurs inside Integral

Second Kind -- Unknown Function occurs both inside and outside Integral

Homogeneous -- Known Function $f$ is identically Zero

Inhomogeneous -- Known Function $f$ is not identically Zero



\subsubsection{Fredholm Integral Equation}\label{sec:fredholm_integral}

both Limits of Integration are \emph{Fixed}

Fredholm Equation of the First Kind:
\[
  f(x) = \int_a^b K(x,t) \varphi(t) dt
\]
where $\varphi$ is an Unknown Function and $f$ and $K$ are Known Functions



\subsubsection{Integro-differential Equation}\label{sec:integro_differential}



% ====================================================================
\section{Multivariable Calculus}\label{sec:multivariable_calculus}
% ====================================================================

extension of (Infinitesimal) Calculus in one Variable to Functions with several
Variables

Calculus in one Variable:
\begin{itemize}
\item Differential Calculus (\S\ref{sec:differential_calculus}): Differentiable
  Functions (\S\ref{sec:differentiable_function}), Differential Equations
  (\S\ref{sec:differential_equation})
\item Integral Calculus (\S\ref{sec:integral_calculus}): Integrable Functions
  (\S\ref{sec:integrable_function}), Integral Equation
  (\S\ref{sec:integral_equation})
\end{itemize}

\fist Vector Fields (\S\ref{sec:vector_field})

\fist Differential Forms (\S\ref{sec:differential_form}): approach to
Multivariable Calculus that is \emph{independent} of Coordinates

\emph{Three-dimensional Graph}: Two-dimensional Input with One-dimensional
Output; Contour Plots (\S\ref{sec:contour})

Multivariable Chain Rule

2 Variables:
\[
  \frac{d}{dt} f(x(t),y(t)) =
    \frac{\partial{f}}{\partial{x}} \cdot \frac{dx}{dt}
      + \frac{\partial{f}}{\partial{y}} \cdot \frac{dy}{dt}
\]
Vector form where $\vec{v}(t) = [x(t),y(t)]$:
\[
  \frac{d}{dt}f(\vec{v}(t)) = \nabla{f(\vec{v}(t)}\bullet{\vec{v}'(t)}
\]
where $\nabla{f}$ is the Gradient (\S\ref{sec:gradient}) of $f$; this the same
as taking the Directional Derivative (\S\ref{sec:directional_derivative}):
\[
  \nabla_{\vec{v}'(t)}f(\vec{v}(t))
\]



% --------------------------------------------------------------------
\subsection{Real Multivariate Function}\label{sec:real_multivariate_function}
% --------------------------------------------------------------------

a Real-valued Function (\S\ref{sec:real_valued}) of several Real Variables

$f : \reals^m \rightarrow \reals$

cf. Real Functions (\S\ref{sec:real_function}) $f : \reals \rightarrow \reals$,
Vector-valued Functions (\S\ref{sec:vector_function})
$f : \reals \rightarrow \reals^n$



\subsubsection{Multivariate Continuity}\label{sec:multivariate_continuity}



% --------------------------------------------------------------------
\subsection{Contour}\label{sec:contour}
% --------------------------------------------------------------------

Contour Map



% --------------------------------------------------------------------
\subsection{Partial Derivative}\label{sec:partial_derivative}
% --------------------------------------------------------------------

the Matrix of all First-order Partial Derivatives of a Vector Function
(\S\ref{sec:vector_function}) is the \emph{Jacobian Matrix}
(\S\ref{sec:jacobian})

(\emph{Schwarz's Theorem}) if the Second Partial Derivative is Continuous at the
given Point, then the Partial Differentiations of the Function are Commutative
at that Point:
\[
  \frac{\partial^2f}{\partial{x}\partial{y}}
    = \frac{\partial^2f}{\partial{y}\partial{x}}
\]

Gradient (\S\ref{sec:gradient}) -- for a Multivariate Scalar-valued Function
(\S\ref{sec:scalar_function}) $f(x_1, \ldots, x_n)$ on $\reals^n$ for which a
Partial Derivative $\pderiv{f}{x_j}$ exists for each Variable $x_j$ at a Point
$a = (a_1, \ldots, a_n)$ the Partial Derivatives define the \emph{Gradient
  Vector} of $f$ as $a$:
\[
  \nabla{f}(a_1, \ldots, a_n) = \big( \pderiv{f}{x_1}(a_1, \ldots, a_n), \ldots,
  \pderiv{f}{x_n}(a_1, \ldots, a_n) \big)
\]
and consequently if $f$ is Differentiable at every Point in some Domain, the
Gradient determines a Vector Field (\S\ref{sec:vector_field})

Partial Derivative of Vector Fields ... TODO

Partial Derivative of a Multivariable Vector-valued Function (TODO); Parametric
Surface (\S\ref{sec:parametric_surface})

an example of a Derivation (Differential Algebra \S\ref{sec:derivation})-- an
Unary Linear Function that satisfies the Leibniz Product Rule
(\S\ref{sec:product_rule})

the \emph{Hessian Matrix} (\S\ref{sec:hessian_matrix}) is the Square Matrix of
Second-order Partial Derivatives of a Scalar-valued Function

if all Partial Derivatives exist at a given Point, the Function need not be
Continuous there, but if al Partial Derivatives exist in a \emph{Neighborhood}
of a Point and are Continuous there, then the Function is Totally Differentiable
(\S\ref{sec:total_derivative}) in that Neighborhood and the Total Derivative is
Continuous



\subsubsection{Partial Differential}\label{sec:partial_differential}

three canonical Partial Differential Operators:
\begin{itemize}
  \item Laplacian (\S\ref{sec:laplacian})
  \item Heat Operator (TODO)
  \item Wave Operator (TODO)
\end{itemize}



\subsubsection{Total Differential}\label{sec:total_differential}

Sum of Partial Differentials with respect to all of the Independent
Variables



\subsubsection{Exact Differential}\label{sec:exact_differential}

the Line Integral (\S\ref{sec:line_integral}) is determined only by the
endpoints



\subsubsection{Inexact Differential}\label{sec:inexact_differential}

\subsubsection{Covariant Derivative}\label{sec:covariant_derivative}

generalization of Directional Derivative (\S\ref{sec:directional_derivative})

\fist Affine Connection (\S\ref{sec:affine_connection}): means of
``Transporting'' Vectors Tangent to a Manifold from one Point to another along
a Curve; a way of specifying a Derivative of a Vector Field along another
Vector Field on a Manifold



% --------------------------------------------------------------------
\subsubsection{Differential Form}\label{sec:differential_form}
% --------------------------------------------------------------------

%FIXME: move to differential geometry

approach to Multivariable Calculus (\S\ref{sec:multivariable_calculus})
that is \emph{independent} of Coordinates

(wiki): generally a Differential $k$-form can be Integrated
(\S\ref{sec:integral}) over $k$-dimensional Chains (\S\ref{sec:k_chain}); for
$k=0$ this is just Evaluation of a Function at Points

Differential $1$-forms are naturally Dual to Vector Fields
(\S\ref{sec:vector_field}) on a Manifold

the Exterior Derivative (\S\ref{sec:exterior_derivative}) is an Operation on
Differential Forms that acts on a $k$-form to produce a $(k+1)$-form

Exterior Algebra (\S\ref{sec:exterior_algebra}) of Differential Forms on a
Differentiable Manifold (\S\ref{sec:differentiable_manifold})

pairing between Vector Fields and $1$-forms is extended to arbitrary
Differential Forms by the Interior Derivative (Interior Product
\S\ref{sec:interior_derivative}), a Degree $-1$ Antiderivation on the Exterior
Algebra of Differential Forms on a Smooth Manifold

Exterior Product (\S\ref{sec:exterior_product}) -- TODO

\fist Contact Geometry (\S\ref{sec:contact_geometry}): geometric structure on
Smooth Manifolds given by a Hyperplane Distribution
(\S\ref{sec:tangent_bundle_distribution}) may be given (Locally) as the Kernel
of a Differential $1$-form and the \emph{Maximal Non-degeneracy Condition}) on
the Form

Bott-Tu1982 - \emph{Differential Forms in Algebraic Topology}

(nlab):

Differential $1$-forms are Functions on the Space of Infinitesimal Paths
(Synthetic Differential Geometry \S\ref{sec:infinitesimal_space}) -- (Stel13 -
\emph{Cosimplicial $C^\infty$ rings and the de Rham complex of Euclidean space})

a Differential $n$-form on $X$ is a Smooth $n$-functor $P_n(X) \rightarrow
\mathbf{B}^n\reals$ from the Path $n$-groupoid f $X$ to the $n$-fold Delooping
of the Additive Lie Group of Real numbers (FIXME: explain, xref)

Product Law (\S\ref{sec:product_rule}) generalized to Differential Forms: the
Product Law states that $\mathrm{d}$ is a \emph{Derivation}
(\S\ref{sec:derivation}) of Degree $+1$ on the Graded Commutative Algebra
(\S\ref{sec:differential_graded_algebra}) of Differential Forms:
\[
\diffy{f \wedge g} = (\diffy{f}) \wedge g +
  (-1)^{\mathrm{deg}\ f}f \wedge (\diffy{g})
\]
(FIXME: clarify derivation degree)

\asterism

The \emph{de Rham Complex} (\S\ref{sec:derham_complex}) is the Cochain Complex
of Differential Forms on some Smooth Manifold (\S\ref{sec:smooth_manifold}) $M$
with the Exterior Derivative (\S\ref{sec:exterior_derivative}) as the
Differential (\S\ref{sec:differential}):
\[
  0 \rightarrow \Omega^0(M) \xrightarrow{d} \Omega^1(M) \xrightarrow{d}
  \Omega^2(M) \xrightarrow{d} \Omega^3(M) \rightarrow \cdots
\]
where $\Omega^0(M)$ is the Space of Smooth Functions on $M$, $\Omega^1(M)$ is
the Space of $1$-forms, etc.; Differential Forms which are the Image of other
Forms under the Exterior Derivative plus the Constant $0$ Function in
$\Omega^0(M)$ are called \emph{Exact} (\S\ref{sec:exact_differential_form}) and
Differential Forms whose Exterior Derivative is $0$ are called \emph{Closed}
(\S\ref{sec:closed_differential_form}); the relationship $d^2 = 0$ says that
Exact Forms are Closed (FIXME: clarify), but the converse is not generally true
(Closed Differential Forms need not be Exact, e.g. for the $1$-form of Angle
Measurement on the Unit Circle $\diffy{\theta}$ there is no actual Function
$\theta$ defined on the whole Circle of which $\diffy{\theta}$ is the
Derivative)

Coboundary Map (i.e. the ``Differential'') in the de Rham Complex is the de Rham
Differential (\S\ref{sec:derham_differential}), ``Exterior Derivative'' acting
on Differential Forms

two Closed Differential Forms $\alpha, \beta \in \Omega^k(M)$ are
\emph{Cohomologous} if they differ by an Exact Differential Form, i.e. $\alpha -
\beta$ is Exact



\paragraph{Exact Differential Form}\label{sec:exact_differential_form}\hfill

\paragraph{Closed Differential Form}\label{sec:closed_differential_form}\hfill

\paragraph{Tautological $1$-form}\label{sec:tautological_1form}\hfill

a \emph{Tautological $1$-form} is a $1$-form defined on the Cotangent Bundle
(\S\ref{sec:cotangent_bundle}) $T * Q$ of a Manifold $Q$

the Exterior Derivative of a Tautological $1$-form defines a \emph{Symplectic
  Form} (\S\ref{sec:symplectic_form}) giving $T * Q$ the structure of a
Symplectic Manifold (\S\ref{sec:symplectic_manifold})

relates the formalisms of Hamiltonian (\S\ref{sec:hamiltonian_system}) and
Lagrangian (\S\ref{sec:lagrangian_system}) Mechanics

in Canonical Coordinates:
\[
  \theta = \sum_i p_i dq^i
\]



\paragraph{Interior Derivative}\label{sec:interior_derivative}\hfill

pairing between Vector Fields (\S\ref{sec:vector_field}) and $1$-forms
(Differential $1$-forms are naturally Dual to Vector Fields on a Manifold)
extended to arbitrary Differential Forms



\paragraph{Darboux's Theorem}\label{sec:darbouxs_theorem}\hfill

partial generalization of Frobenius Integration Theorem (TODO)

Symplectic Geometry (\S\ref{sec:symplectic_geometry})

any two Symplectic Manifolds of the same Dimension are locally Symplectomorphic
to one another, i.e. every $2n$-dimensional Symplectic Manifold can be made to
look Locally like the Linear Symplectic Space $\comps^n$ with its canonical
Symplectic Form; analogous consequence for Contact Geometry (FIXME: explain,
xrefs)



\paragraph{Symplectic Form}\label{sec:symplectic_form}\hfill

%FIXME: move this section to bilinear forms ???

a Symplectic Manifold (\S\ref{sec:symplectic_manifold}) is a Differentiable
Manifold equipped with a Closed Non-degenerate $2$-form called the
\emph{Symplectic Form}

the Symplectic Form in Symplectic Geometry (\S\ref{sec:symplectic_geometry})
plays the role analagous to Metric Tensor (\S\ref{sec:metric_tensor}) in
Riemannian Geometry (\S\ref{sec:riemannian_geometry})

a Symplectic Bilinear Form (\S\ref{sec:symplectic_bilinear}) is a
Non-degenerate Alternating (Skew-symmetric) Bilinear Form
(\S\ref{sec:alternating_form})

Symplectic Vector Spaces (\S\ref{sec:symplectic_vectorspace}) are equipped with
a Symplectic Bilinear Form. If the underlying Field has Characteristic
(\S\ref{sec:ring_characteristic}) $\neq 2$, Alternation is \emph{equivalent} to
Skew-symmetry; for Characteristic $=2$, the Skew-symmetry is \emph{implied by}
(but does not imply) \emph{Alternation}-- every Symplectic Form is a
\emph{Symmetric (Bilinear) Form} (\S\ref{sec:symmetric_bilinear}) but not
every Symmetric Form is Symplectic.

$Sp(V)$ -- Symplectic Group (\S\ref{sec:symplectic_group}): Subgroup of the
General Linear Group $GL(V)$ which Preserves a Symplectic Form on $V$ (i.e. a
Non-degenerate Alternating Form \S\ref{sec:alternating_form})

the Exterior Derivative of a Tautological $1$-form
(\S\ref{sec:tautological_1form}) defined on the Cotangent Bundle
(\S\ref{sec:cotangent_bundle}) $T * Q$ of a Manifold $Q$ defines a Symplectic
Form giving $T * Q$ the Structure of a Symplectic Manifold

Canonical Symplectic Form (Poincar\'e $2$-form):
\[
  \omega = -d\theta = \sum_i dq^i \wedge dp_i
\]

\asterism

\url{https://mathoverflow.net/questions/19932/what-is-a-symplectic-form-intuitively}:

``...given a Direction that we want to think of as Position, it tells us what
the Momentum Direction is. What it gives us are pairs: given one Coordinate,
there's a second so that the pair are \emph{Canonically Conjugate}, which means
that, with respect to each other, they will \emph{act like} Position and
Momentum and that they'll ignore the 'other direction' (that is, things will
Poisson Commute)''

\asterism

Clifford Algebras (\S\ref{sec:clifford_algebra}) represent the same structure
for Non-degenerate Symmetric Bilinear Forms (\S\ref{sec:symmetric_bilinear})
that Weyl Algebras (Symplectic Clifford Algebras \S\ref{sec:weyl_algebra})
represent for Symplectic Bilinear Forms (\S\ref{sec:symplectic_form})



% --------------------------------------------------------------------
\subsection{Total Derivative}\label{sec:total_derivative}
% --------------------------------------------------------------------

if all Partial Derivatives (\S\ref{sec:partial_derivative}) exist at a given
Point, the Function need not be Continuous there, but if al Partial Derivatives
exist in a \emph{Neighborhood} of a Point and are Continuous there, then the
Function is Totally Differentiable in that Neighborhood and the Total Derivative
is Continuous



% --------------------------------------------------------------------
\subsection{Iterated Integral}\label{sec:iterated_integral}
% --------------------------------------------------------------------

\fist Fubini's Theorem (\S\ref{sec:fubinis_theorem}): conditions under which it
is possible to compute a Double Integral (\S\ref{sec:double_integral}) using
Iterated Integrals



% --------------------------------------------------------------------
\subsection{Multiple Integral}\label{sec:multiple_integral}
% --------------------------------------------------------------------

Integral (\S\ref{sec:integral})

Multiple Integration



\subsubsection{Double Integral}\label{sec:double_integral}

Volume under a Surface

Area Differential, Volume Differential

Bounded Domain



\paragraph{Fubini's Theorem}\label{sec:fubinis_theorem}\hfill

conditions under which it is possible to compute a Double Integral using
Iterated Integrals (\S\ref{sec:iterated_integral})



\subsubsection{Triple Integral}\label{sec:triple_integral}

Volume Differential, Mass Differential

e.g. compute the Mass of a Volume of non-uniform Density with Density Function
$\rho(x,y,z)$



\subsubsection{Line Integral}\label{sec:line_integral}

Parametric Equation (\S\ref{sec:parametric_equation}) $\vec{c}(t) = p(t)\hat{i}
+ q(t)\hat{j}$

Parametric Surface (\S\ref{sec:parametric_surface}) defined by Multivariable
Real-valued Function $f(x,y)$

change in Arclength $ds = \sqrt{dx^2 + dy^2}$

\[
  \int_{t=a}^{t=b} f(p(t),q(t)) \sqrt{p'(t)^2 + q'(t)^2} dt
\]

Closed Line Integral $\oint$

Integrating Complex Functions (\S\ref{sec:complex_function})

for Exact Differentials (\S\ref{sec:exact_differential}), the Line Integral is
determined only by the endpoints



\paragraph{Green's Theorem}\label{sec:greens_theorem}\hfill

Curl (\S\ref{sec:curl})

for Vector Field $\vec{f}(x,y) = P(x,y)\hat{i} + Q(x,y)\hat{j}$ and Closed
Counter-clockwise Path $C$ in the Plane and $R$ the enclosed Region:
\[
  \oint_C \vec{f} d\vec{r} = \iint_R (\nabla \times \vec{f}) dA
\]

for a Clockwise Path:
\[
  \iint_R (\frac{\partial{P}}{\partial{y}} - \frac{\partial{Q}}{\partial{x}}) dA
\]

if $\vec{f}$ is Conservative (\S\ref{sec:conservative_vector_field}), $\oint_C
\vec{f} d\vec{r} = 0$

\fist 2D Divergence Theorem (\S\ref{sec:divergence_theorem})

\fist Stokes' Theorem (\S\ref{sec:stokes_theorem})

\[
  \oint_C \vec{f} \bullet \hat{n} dS = \iint_R (\nabla \cdot \vec{f}) dA
\]

the amount of the Flux flowing ``out'' of the closed Region is equal to the sum
of the Divergence over the Region; add a Mass Function $\rho$ so that $\vec{f}$
becomes:
\[
  \vec{f} = \rho(P\hat{i}) + \rho(Q\hat{j})
\]



\subsubsection{Surface Integral}\label{sec:surface_integral}

Parametric Surface (\S\ref{sec:parametric_surface})
$\vec{r}(s,t)$

Double Integral analog of the Line Integral

\[
  d\sigma =
    |\frac{\partial{\vec{r}}}{\partial{s}}
      \times \frac{\partial{\vec{r}}}{\partial{t}}| ds dt
\]

Surface Area

\[
  {\iint}_{\Sigma} d\sigma
\]

more generally:

\[
  {\iint}_{\Sigma} f(x,y,z) d\sigma
\]

``Flux Integral'' -- Flux (\S\ref{sec:flux}) through a 2D Surface; equivalent
notations:
\[
  \iint_{S} \vec{F} \bullet \hat{n} dS
\]

\[
  \iint_{S} \vec{F} \bullet d\vec{S}
\]

\[
  \iint_{R} \vec{F} \bullet (\vec{r}_u \times \vec{r}_v) du dv
\]
where $\vec{r}_u$ and $\vec{r}_v$ are the Partial Derivatives of the Parametric
Surface $\vec{r}(u,v)$



\subsubsection{Volume Integral}\label{sec:volume_integral}

used to calculate ``\emph{Flux Densities}'' (\S\ref{sec:flux}) %FIXME



% --------------------------------------------------------------------
\subsection{Saddle Point}\label{sec:saddle_point}
% --------------------------------------------------------------------

Gradient (\S\ref{sec:gradient}) equal to the Zero Vector: $\nabla{f} = \vec{0}$

Maximum (\S\ref{sec:maximum}), Minimum (\S\ref{sec:minimum})

in the $x$ direction looks like a Maximum and in the $y$ direction looks like a
Minimum, or \emph{vice versa}

\emph{Second Partial Derivative Test} -- additionally need to look at
\emph{mixed} Partial Derivative Term
$\frac{\partial^2{f}}{\partial{y}\partial{x}}$:
\[
  H = f_{xx}(x_0,y_0)f_{yy}(x_0,y_0) - f_{xy}(x_0,y_0)^2
\]
if $H > 0$, then it is a Maximum or a Minimum

if $H < 0$, then it is a Saddle Point

if $H = 0$, then ??? ... FIXME



% --------------------------------------------------------------------
\subsection{Boundary Value Problem}\label{sec:boundary_value_problem}
% --------------------------------------------------------------------

%FIXME harmonic analysis? dirichlet problem?

\fist Intial Value Problem (IVP \S\ref{sec:ivp}) -- specifies Value in Solution
at only one Point (Initial Conditions)


\fist the \emph{Finite Element Method} (or \emph{FEM}
\S\ref{sec:finite_element_method}) is a technique for solving Boundary Value
Problems for Partial Differential Equations (\S\ref{sec:partial_differential})



% ====================================================================
\section{Time-scale Calculus}\label{sec:timescale_calculus}
% ====================================================================

(wiki):

unification of Theory of Difference Equations (\S\ref{sec:difference_equation})
with that of Differential Equations (\S\ref{sec:differential_equation}),
unifying Differential (\S\ref{sec:differential_calculus}) and Integral
(\S\ref{sec:integral_calculus}) Calculus with the \emph{Calculus of Finite
  Differences} (\S\ref{sec:finite_differences_calculus})

offers a formalism for studying hybrid Discrete-Continuous Dynamical Systems
(\S\ref{sec:dynamical_system})



% --------------------------------------------------------------------
\subsection{Recurrence Relation}\label{sec:recurrence_relation}
% --------------------------------------------------------------------

Recursive Definition (\S\ref{sec:recursive_definition})

``Polynomial Mapping''

\begin{itemize}
  \item Dyadic Transformation (Bit-shift Map) -- TODO
  \item Tent Map -- TODO
\end{itemize}

Non-linear Difference Equations (\S\ref{sec:difference_equation}):
\begin{itemize}
  \item Logistic Map (\S\ref{sec:logistic_map}):\[
    x_{n+1} = rx_n(1-x_n)
  \]
\end{itemize}



\subsubsection{Difference Equation}\label{sec:difference_equation}

\fist Matrix Difference Equation (System of Difference Equations
\S\ref{sec:difference_equation_system})

\begin{itemize}
  \item Evolution Equation of a Discrete-time Dynamical System
    (\S\ref{sec:discrete_dynamical_system}) is a First-order Difference Equation
    $y_{n+1} = F(y_n)$ -- given an Initial State $y_0$, successive applications
    of the Unknown Function $F$ will generate the Sequence of States $y_n$
    determining the evolution of the System; a \emph{Fixed Point} of such a
    System is a $y0$ such that $F(y0) = y0$
\end{itemize}

Matrix Difference Equation, System of Difference Equations (TODO)

Non-linear:
\begin{itemize}
  \item Logistic Map (\S\ref{sec:logistic_map}):\[
    x_{n+1} = rx_n(1-x_n)
  \]
\end{itemize}



\paragraph{Linear Difference Equation}\label{sec:linear_difference_equation}
\hfill

\paragraph{Homogeneous Difference Equation}
\label{sec:homogeneous_difference_equation}
\hfill

cf. Homogeneous Differential Equation
(\S\ref{sec:homogeneous_differential_equation}), Homogeneous Polynomial
(\S\ref{sec:homogeneous_polynomial}), Homogeneous System of Linear Equations
(\S\ref{sec:homogeneous_linear_equation_system})



\subparagraph{Characteristic Difference Equation}
\label{sec:characteristic_difference_equation}\hfill

cf. Characteristic Equation of a Homogeneous Linear Differential Equation
(\S\ref{sec:characteristic_equation})



\paragraph{Matrix Difference Equation}\label{sec:matrix_difference_equation}
\hfill

Difference Equation in which the Value of a Vector or Matrix of Variables at
one Point in Time is related to its own Value at one or more previous Points in
Time, using Matrices



\subsubsection{Logistic Map}\label{sec:logistic_map}

2nd-degree Polynomial Mapping (Recurrence Relation):
\[
  x_{n+1} = rx_n(1-x_n)
\]
where $x_n \in (0.0, 1.0)$ represents the Ratio of existing Population to the
Maximum possible Population and $r$ is in the Interval $[0,4]$:
\begin{itemize}
  \item $r$ between 0 and 1 -- the Population will eventually die independent of
    the Initial Population
  \item $r$ between 1 and 2 -- the Population will approach the value
    $\frac{r-1}{r}$, independent of the initial Population
  \item $r$ between 2 and 3 -- the Population will approach $\frac{r-1}{r}$ but
    will fluctuate around that value for some time
  \item $r$ between 3 and $1 + \sqrt{6} \approx 3.44949$ from ``almost all''
    Initial Conditions will approach permanent oscillations between two values
    dependent on $r$
  \item ... MORE
  \item $r \approx 3.56995$ -- onset of Chaos
  \item ... MORE
  \item beyond $r=4$ almost all values wil eventually diverge
\end{itemize}

Non-linear Difference Equation capturing effects of \emph{Reproduction} and
\emph{Starvation} in Populations

\fist cf. Logistic Function (Sigmoid Curve \S\ref{sec:logistic_function}) --
\emph{Continuous} version of Logistic Map: Solution to the First-order
Non-linear Ordinary Differential Equation:
\[
  \frac{d}{dx}f(x) = f(x)(1 - f(x))
\]
with Boundary Condition (\S\ref{sec:boundary_value_problem}) $f(0) =
\frac{1}{2}$



% --------------------------------------------------------------------
\subsection{Calculus of Finite Differences}
\label{sec:finite_differences_calculus}
% --------------------------------------------------------------------

\fist Finite-Difference Methods (FDMs \S\ref{sec:fdm}) -- Numerical Methods for
Solving Differential Equations by Approximating them with Difference Equations
(\S\ref{sec:difference_equation})



\subsubsection{Finite Difference}\label{sec:finite_difference}

Discrete equivalent of (Infinitesimal) Differentials (\S\ref{sec:differential})



\paragraph{Forward Difference}\label{sec:forward_difference}\hfill

$\Delta_h$

\fist Antidifference Operator (Indefinite Sum Operator
\S\ref{sec:antidifference}) $\Delta^{-1}_h$ -- Inverse Operator of the Forward
Difference Operator

as a ``\emph{Difference Operator}'':
\[
  \Delta = T_h - I
\]
where $T_h$ is a Shift Operator (\S\ref{sec:shift_operator}) with Step $h$
(defined by $T_h[f](x) = f(x + h)$), and $I$ is the Identity Operator



\paragraph{Backward Difference}\label{sec:backward_difference}\hfill

$\nabla_h$



\paragraph{Central Difference}\label{sec:central_difference}\hfill

$\delta_h$



\paragraph{Difference Quotient}\label{sec:difference_quotient}\hfill

\fist Discrete equivalent of a \emph{Derivative} (\S\ref{sec:derivative}) viewed
as a Quotient of Differentials (Infinitesimal Differences)

$\frac{\Delta_h[f](x)}{h}$ -- Forward Difference
(\S\ref{sec:forward_difference}) Divided by $h$ Approximates the Derivative when
$h$ is small, and the same Formula holds for the Backward Difference
$\nabla_h[f](x)$, but for Central Differences the Approximation is more accurate
if $f$ is twice Differentiable (note however that Oscillating Functions can
yield a Zero Derivative for the Central Difference)

Linear Approximation (\S\ref{sec:linear_approximation})



\subsubsection{Antidifference}\label{sec:antidifference}

or \emph{Indefinite Sum} -- Discrete equivalent to Antidifferentiation
(Indefinite Integration \S\ref{sec:antiderivative})

cf. (Definite) Summation (\S\ref{sec:summation}) -- Discrete equivalent to
(Definite) Integration (\S\ref{sec:integral})

$\Delta^{-1}_h$



% --------------------------------------------------------------------
\subsection{Differential Equation}\label{sec:differential_equation}
% --------------------------------------------------------------------

2010 - Kelley, Peterson - \emph{The Theory of Differential Equations} -
\url{https://www.win.tue.nl/~rvhassel/Onderwijs/Old-Onderwijs/2WA23-2011/Arzela-Ascoli/Kelly_Theory_Differentiual_Equations/}

(wiki):

A \emph{Differential Equation} is an Equation relating the Values of a number of
\emph{Unknown Functions} (\S\ref{sec:unknown_function}) of one or more
Independent Variables (\S\ref{sec:independent_variable}) to their
\emph{Derivatives} (\S\ref{sec:derivative}) of various Orders. The Unknown
Functions are commonly referred to through an equal number of Dependent
Variables (\S\ref{sec:dependent_variable}).

types of Differential Equations:
\begin{itemize}
  \item Ordinary (ODE \S\ref{sec:ode}) / Partial (PDE \S\ref{sec:partial}) --
    an ODE is an Equation containing one (or more) Unknown Functions $f$ of a
    single Real or Complex Variable, $x$
  \item Linear (\S\ref{sec:linear_differential_equation}) /
    Non-linear (\S\ref{sec:nonlinear_differential_Equation})
  \item Homogeneous (\S\ref{sec:homogeneous_differential_equation}) /
    Inhomogeneous (\S\ref{sec:inhomogeneous_differential_equation})
  \item ... MORE?
\end{itemize}

The \emph{Order} of a Differential Equation is equal to the Highest-order
Derivative (\S\ref{sec:higher_derivative}) appearing in any of its Terms. Any
ODE of Order greater than one can be rewritten as a System of (Coupled)
First-order ODEs (\S\ref{sec:ode_system}).

Examples of Infinite-order PDEs may be found in the Quantum Hamilton's Equations
for Trajectories of Quantum Particles (Phase Space Formulation) or the Evolution
Equation of the Wigner Function.

an unspecified Constant in a Differential Equation may be called a
\emph{Parameter} (Kelley-Peterson10)

Implicit vs. Explicit -- TODO

ODE examples -- single Unknown Function $u$ of a single Independent Variable,
$x$:
\begin{itemize}
\item $u'(x) = cu(x) + x^2$ -- First-order Linear Constant Coefficient ODE
\item $u''(x) - xu'(x) + u(x) = 0$ -- Second-order Homogeneous Linear ODE
\item $u''(x) - \omega^2u(x) = 0$ -- Second-order Homogeneous Linear Constant
  Coefficient ODE (description of the Harmonic Oscillator)
\item $u'(x) = u(x)^2 + 4$ -- First-order Inhomogeneous Non-linear ODE
\item $Lu''(x) + g\sin{u(x)} = 0$ -- Second-order Non-linear ODE (description of
  Pendulum of length $L$) --FIXME: what is $g$ ?
\end{itemize}

PDE examples -- single Unknown Function $u$ of two Independent Variables $x$ and
$y$:
\begin{itemize}
\item $\pderiv{u}{x} + x\pderiv{u}{y} = 0$ -- First-order Homogeneous Linear PDE
\item $\pderiv{^2u}{x^2} + \pderiv{^2u}{y^2} = 0$ -- Second-order Homogeneous
  Linear Constant Coefficient PDE (the Laplace Equation
  \S\ref{sec:laplace_equation})
\item $\pderiv{u}{t} - \alpha\Big(
      \pderiv{^2u}{x^2} + \pderiv{^2u}{y^2} + \pderiv{^2u}{z^2}
    \Big) = 0$ -- Heat Equation
\end{itemize}

Differential Equations may also involve multiple Unknown Functions
(\url{https://math.stackexchange.com/questions/562167/i-need-an-example-of-a-differential-equation-nontrivial-that-involves-more-tha}):
\begin{itemize}
  \item $x''(t) + y''(t) + z''(t) = 1$ -- Unit Speed Curves
    (\S\ref{sec:unit_speed_curve})
  \item ... MORE ?
\end{itemize}

see also \emph{Systems of (Coupled) Differential Equations}:
\begin{itemize}
  \item System of ODEs (\S\ref{sec:ode_system})
  \item System of PDEs (\S\ref{sec:pde_system})
  \item System of DAEs (Differential-algebraic Equations \S\ref{sec:dae_system})
\end{itemize}

(Villate06 \S 3.1): a Solution of an ODE is any Function $f$ of a single
Variable which Satisfies the Differential Equation when \emph{substituted} for
the Dependent Variable; Implicit form (TODO)

Witkin, Baraff - Siggraph '97 Course:

Differential Equations describe the Relation between an \emph{Unknown Function}
and its \emph{Derivatives} (\S\ref{sec:derivative}) and \emph{Solving} a
Differential Equation is to find a Function that Satisfies the Relation, and
typically while Satisfying ``some additional conditions as well''. (FIXME:
clarify)

\asterism

The \emph{Solution} to a Differential Equation $F(x,y,y',\ldots,y^{(n)}) = 0$
of Order $n$ is a \emph{Function} or \emph{Class of Functions} of the form $u :
I \subset \reals \rightarrow \reals$ where $u$ is an $n$-times Differentiable
Function (\S\ref{sec:differentiable_function}) on $I$ and $\forall x \in I$:
\[
  F(x,u,u',\ldots,u^{(n)}) = 0
\]
Such a $u$ defines an \emph{Integral Curve} (\S\ref{sec:integral_curve}) for
$F$.

\fist Family of Curves (\S\ref{sec:curve_family})

\fist cf. the Solution of a Polynomial Equation
(\S\ref{sec:polynomial_equation}) is a Value or Set of Values

\fist Algebraic $D$-modules (\S\ref{sec:algebraic_d_module}): Modules over the
Weyl Algebra (\S\ref{sec:weyl_algebra}) $A_n(K)$ over a Field $K$ of
Characteristic Zero; relates Weyl Algebra to Differential Equations

In a Dynamical System (\S\ref{sec:dynamical_system}), the \emph{Evolution
  Function} $\Phi^t$ is often the Solution of a Differential Equation of Motion
$\dot{x} = v(x)$ giving the Time Derivative (\S\ref{sec:time_derivative}) of a
Trajectory (Integral Curve) $x(t)$ on the Phase Space starting at some Point
$x_0$.

Extension, Maximal Solution, Global Solution, General Solution

\fist Numerical Integration (\S\ref{sec:numerical_integration}), Collocation
Methods (\S\ref{sec:collocation_method})

\fist Difference Equation (\S\ref{sec:difference_equation})

\fist Differential System (\S\ref{sec:differential_system})

\fist Integral Equations (\S\ref{sec:integral_equation}), Integro-differential
Equations (\S\ref{sec:integro_differential})

Exponential Models $y = Ce^{-kt}$, $y' = ky$

Logistic Models $y = \frac{y_0 k}{n_0 + (k-n_0)e^{-rt}}$,
  $y' = ky (1 - \frac{y}{k})$

\fist Stochastic Differential Equation (SDE \S\ref{sec:sde})

\fist Numerical Analysis (\S\ref{sec:numerical_analysis}) applied to Solving
Differential Equations: Finite Element Method
(\S\ref{sec:finite_element_method}), Finite Volume Method
(\S\ref{sec:finite_volume_method})



\subsubsection{Unknown Function}\label{sec:unknown_function}

\subsubsection{Ordinary Differential Equation}\label{sec:ode}

An \emph{Ordinary Differential Equation (ODE)} is a Differential Equation an
Unknown Function ($y$) of one Independent Real or Complex Variable ($x$) and its
Derivatives. An equation of the form:
\[
  F\Big(x,y,y',\ldots,y^{(n-1)}\Big) = y^{(n)}
\]
is an \emph{Order-$n$ Explicit Ordinary Differential Equation}, and:
\[
  F\Big(x,y,y',\ldots,y^{(n)}\Big) = 0
\]
is an \emph{Order-$n$ Implicit Ordinary Differential Equation}.

any ODE of Order greater than one can be rewritten as a (Coupled) System of
First-order ODEs (\S\ref{sec:ode_system})

(Villate06): Solutions of an ODE of the form $y' = f(x)$ follow from the
Fundemental Theorem of Integral Calculus
(\S\ref{sec:fundamental_calculus_theorem}):
\[
y(x) = \int f(x) \diffy{x} + c
\]
where $c$ is an arbitrary Constant. There is no \emph{general method} to finding
Solutions of any First-order ODE and in some cases known Solutions can only be
evaluated with approximate Numerical Methods (\S\ref{sec:numerical_method})

\fist a \emph{Linear Differential Equation}
(\S\ref{sec:linear_differential_equation}) is a Differential Equation defined
by a Linear Polynomial (a Polynomial of Degree $1$) in the Unknown Function and
its Derivatives:
\[
  y^{(n)} = \sum_{i=0}^{n-1} a_i (x) y^{(i)} + r(x)
\]
where $a_i(x)$ and $r(x)$ are Continuous Functions in $x$

\fist A \emph{Partial} Differential Equation (PDE
\S\ref{sec:pde}) is a Differential Equation
containing Unknown Multivariable Functions and their Partial Derivatives.

\fist Flow (\S\ref{sec:flow})

A \emph{Solution} of an ODE $F$ is a Function $u : I \subset \reals \rightarrow
\reals$ called an \emph{Integral Curve} (\S\ref{sec:integral_curve}) for $F$
which is Tangent to the Slope Field (\S\ref{sec:slope_field}) $f(x,y)$ defined
by the ODE at every Point.

For $F$ a Function of $t$, $y$, and Derivatives (\S\ref{sec:derivative}) of
$y$, an Equation of the form:
\[
  F(t,y,y',\ldots,y^{(n-1)}) = y^{(n)}
\]
is a \emph{Explicit Ordinary Differential Equation} of \emph{Order $n$}, and an
\emph{Implicit Ordinary Differential Equation} (or \emph{Homogeneous Ordinary
  Differential Equation}) of Order $n$ has the form:
\[
  F(t,y,y',\ldots,y^{(n)}) = 0
\]
if $f(t)$ is a Solution to a Homogeneous ODE $F$ then $c\cdot{f(t)}$ is a
Solution to $F$ for any Constant $c$, and if $g(t)$ is also a Solution then
$f(t) + g(t)$ is a Solution


Domain of Solutions: there is no way in general to know what the Domain of the
Solution will be (MIT 18.03 2006 Lec. 1)

example:

for the Differential Equation $y' = \frac{-x}{y}$, setting $\frac{-x}{y}$ equal
to a Constant, $\frac{-x}{y} = C$, gives the \emph{Isoclines}
(\S\ref{sec:isocline}) of the Slope Field defined by the Differential Equation,
i.e. they are curves where the Family of Solutions are intersect the
curve with the indicated slope $C$ along its length

Standard First-order Form:
\[
  y' = p(x) y + q(x)
\]
cf. Standard Linear Form (\S\ref{sec:linear_differential_equation}):
\[
  y' + p(x) y = q(x)
\]

\emph{Local Existence and Uniqueness Theorem}: for a Differential Equation
\emph{in Standard Form}, when $f$ and $f_y$ are Continuous near $x_0$ and
$y_0$, there is one and only one Solution through a Point $(x_0, y_0)$

\emph{Global Existence and Uniqueness Theorem} (TODO)


\emph{Characteristic Equation}

an example of a System modelled by a First-order Ordinary Differential Equation
is an RC Circuit

an example of a System modelled by a Second-order Ordinary Differential
Equation is a Simple Harmonic Oscillator: the Phase Space consists of a
Velocity Dimension and a Position Dimension and the Orbit is Periodic

\asterism

\begin{itemize}
  \item \emph{Logistic Differential Equation} -- the Logistic Function
    (\S\ref{sec:logistic_function}) $f(x) = \frac{L}{1 + e^{-k(x-x_0)}}$ is the
    Solution to the First-order Non-linear Ordinary Differential Equation:
  \[
    \frac{d}{dx}f(x) = f(x)(1 - f(x))
  \]
  with Boundary Condition (\S\ref{sec:boundary_value_problem}) $f(0) =
  \frac{1}{2}$
\end{itemize}

\asterism

(Kelley-Peterson10, Ch.1)

given a First-order ODE:
\[
  x' = f(t,x)
\]
where $f : \reals^2 \rightarrow \reals$ and $x : \reals \rightarrow \reals$, a
\emph{Solution} is a Continuously Differentiable Function $x$ such that:
\[
  x'(t) = f(t, x(t))
\]
An \emph{Initial Value Problem} (IVP \S\ref{sec:ivp}) is formed by specifying a
Point $(t_0, x_0) \in \reals^2$ in the Domain of $f$ called the \emph{Initial
  Condition} (\S\ref{sec:initial_condition}) with Initial Point (Time) $t_0$ and
Initial Value $x_0$.
A Solution to this IVP is Solution $x$ of the ODE $x' = f(t,x)$ such that the
Initial Condition $x(t_0) = x_0$ is also satisfied. Such a Solution can be
visualized as finding the Integral Curve (\S\ref{sec:integral_curve}) passing
through the given Initial Condition Point.
A Solution that goes through an Initial Condition Point is also called a
\emph{Particular Solution}.

A First-order Linear Differential Equation
(\S\ref{sec:linear_differential_equation}) has the form:
\[
  x' = p(t)x + q(t)
\]
where $p, q: \reals \rightarrow \reals$.
A single Linear ODE can always be Solved in terms of Integrals
(\S\ref{sec:integral})-- \emph{Variation of Constants Formula} (Thm.).

An Autonomous (\S\ref{sec:autonomous_differential_equation}) ODE has the form:
\[
  x' = f(x)
\]
that is, $f$ depends only on $x$ and not \emph{directly} on $t$. Translating a
Solution of such an Autonomous ODE along the $t$-axis will produce another
Solution.



\paragraph{Slope Field}\label{sec:slope_field}\hfill

or \emph{Direction Field}

$f(x,y)$

Points are shown with Slope equal to the Value of $f(x,y)$ at that Point

When the Differential Equation is represented as a Vector Field
(\S\ref{sec:vector_field}) or Slope Field, the corresponding Integral Curves
(\S\ref{sec:integral_curve}) are Tangent to the Field at each point.



\subparagraph{Integral Curve}\label{sec:integral_curve}\hfill

An \emph{Integral Curve} is a Parametric Curve that represents a
\emph{specific} Solution of an Ordinary Differential Equation
(\S\ref{sec:ode}), or System of ODEs (\S\ref{sec:ode_system}).

When the Differential Equation is represented as a Vector Field or Slope Field
(\S\ref{sec:slope_field}), the corresponding Integral Curves are Tangent to the
Field at each point.

Integral Curves intersect Isoclines (\S\ref{sec:isocline}) with the slope
indicated by the Isocline

(\emph{Existence and Uniqueness Theorem}) two Integral Curves cannot cross (at
an Angle) or touch (be Tangent)

In a Dynamical System (\S\ref{sec:dynamical_system}), the Integral Curves for
the Differential Equation governing the System are referred to as
\emph{Trajectories} (\S\ref{sec:trajectory}).

e.g. Electric Field or Magnetic Field Lines, Streamlines (Velocity Field of a
Fluid)

\emph{Flow}



\subparagraph{Isocline}\label{sec:isocline}\hfill

Isoclines are intersected by Integral Curves (\S\ref{sec:integral_curve}) at
the slope indicated by the Isocline

example:

for the Differential Equation (\S\ref{sec:differential_equation}) $y' =
\frac{-x}{y}$, setting $\frac{-x}{y}$ equal to a Constant, $\frac{-x}{y} = C$
gives a Family of \emph{Isoclines} of the Slope Field defined by the
Differential Equation, i.e. they are curves where the Family of Solutions
intersect the curve with the indicated slope $C$ along its length



\paragraph{Initial Value Problem (IVP)}\label{sec:ivp}\hfill

\fist cf. Boundary Value Problem (\S\ref{sec:boundary_value_problem}) defines
Values of the Solution at more than one Point

(wiki):

an ODE together with a Value of the Unknown Function called the \emph{Initial
  Condition} (\S\ref{sec:initial_condition}) at a given Point in the Domain of
the Solution

Differential Equation:
\[
  y'(t) = f(t, y(t))
\]
with $f : \Omega \subset \reals \times \reals^n \rightarrow \reals^n$ where
$\Omega$ is an Open Set of $\reals \times \reals^n$ and Initial Condition
\[
  (t_0, y_0) \in \Omega
\]
in the Domain of $f$

a \emph{Solution} to an Initial Value Problem is a Function $y : \reals
\rightarrow \reals^n$ that is a Solution to the Differential Equation and
satisfies $y(t_0) = y_0$

\fist Time Evolution (\S\ref{sec:time_evolution})

\fist Dynamical Systems (\S\ref{sec:dynamical_system})

\begin{itemize}
  \item the Trajectory $\vec{s}(t)$ of a Hamiltonian System is a Solution to
    the IVP defined by Hamilton's Equations and the Initial Condition
    $\vec{s}(0) = \vec{s}_0 \in \reals^{2N}$
\end{itemize}

First-order Initial Value Problems: \emph{Peano Existence Theorem} gives a set
of circumstances in which a solution exists

\emph{Picard-Lindel\"of Theorem}

Higher Dimensions (TODO)

for an $n$-th Order Linear Initial Value Problem:
\[
  f_n(x)\frac{d^ny}{dx^n} + \cdots + f_1(x)\frac{dy}{dx} + f_0(x)y = g(x)
\]
such that:
\begin{align*}
  y(x_0)   & = y_0   \\
  y'(x_0)  & = y_0'  \\
  y''(x_0) & = y_0''
\end{align*}
then for any Non-zero $f_n(x)$ and $\{f_0,f_1,\ldots\}$ and $g$ are Continuous
on some Interval containing $x_0$, $y$ is unique and exists

(Witkin, Baraff - Siggraph '97 Course):

canonical IVP:
\[
  \dot{\vec{x}} = f(\vec{x}, t)
\]
with known Function $f$, State $\vec{x}$, and $\dot{\vec{x}} =
\deriv{\vec{x}}{t}$ the Time Derivative (Velocity \S\ref{sec:time_derivative})
of $\vec{x}$

Initial Conditions:
\[
  \vec{x}(t_0) = \vec{x}_0
\]
of Initial Value (State) $\vec{x}_0$ at Initial Point (Time) $t_0$

a Trajectory of a Point $\vec{p}$ by $f$ forms an Integral Curve
(\S\ref{sec:integral_curve}) of the Vector Field (\S\ref{sec:vector_field})
defined by $f$

note that if the Derivative Function $f$ depends directly on Time, then the
Vector Field defined by $f$ will change over Time

\emph{Numerical Methods}

find \emph{Numerical Solutions} as opposed to \emph{Symbolic Solutions}

at each Time Step, perform one or more \emph{Derivative Evaluations} of the
Derivative Function $f$ on the current numerical values of State $\vec{x}$ and
Time $t$, which returns a numerical value for $\dot{\vec{x}}$ which is then used
to increment $\vec{x}$

\begin{itemize}
\item Euler's Method (\S\ref{sec:euler_method}) -- First-order
\item Midpoint Method (\S\ref{sec:midpoint_method}) -- Second-order
\item Runge-Kutta Integration (\S\ref{sec:midpoint_method}) -- Higher-orders
\end{itemize}

Adaptive Stepsizing



\subsubsection{Separable Differential Equation}\label{sec:separable}

a Differential Equation is \emph{Separable} if the Derivative $\frac{dy}{dx}$
can be written in terms of the product of separate functions of $x$ and $y$ as:
\[
  \frac{dy}{dx} = g(x)h(y)
\]
or equivalently, as long as $h(y) \neq 0$:
\[
  \frac{1}{h(y)} dy = g(x) dx
\]



\paragraph{Separation of Variables}\label{sec:separation_of_variables}\hfill

\paragraph{Homogeneous Differential Equation}
\label{sec:homogeneous_differential_equation}

If the Differential Equation:
\[
  \frac{dy}{dx} = f(x,y)
\]
can be rewritten as:
\[
  \frac{dy}{dx} = F\Big(\frac{y}{x}\Big)
\]
then it is \emph{Homogeneous} and a Variable Substitution allows the Equation to
be made \emph{Separable}.

cf. Homogeneous Difference Equation
(\S\ref{sec:homogeneous_difference_equation}), Homogeneous Polynomial
(\S\ref{sec:homogeneous_polynomial}), Homogeneous System of Linear Equations
(\S\ref{sec:homogeneous_linear_equation_system})

\emph{not} ``Homogeneous Linear Differential Equation''
(\S\ref{sec:homogeneous_linear_differential})



\paragraph{Inhomogeneous Differential Equation}
\label{sec:inhomogeneous_differential_equation}



\subsubsection{Exact Differential Equation}\label{sec:exact_equation}

$\frac{d}{dx}\Psi(x,y) = \frac{\partial{\Psi}}{\partial{x}}
  + \frac{\partial{\Psi}}{\partial{x}}\frac{dy}{dx}$

$\Psi_{xy} = \Psi_{yx}$ -- if $\Psi$ and its Derivatives are Continuous over
some Domain

$\Psi_x + \Psi_y \frac{dy}{dx} = 0$

$\frac{d}{dx}\Psi(x,y) = 0$

$\Psi(x,y) = c$

$M(x,y) + N(x,y)\frac{dy}{dx} = 0$

$M_y = N_x$ if and only if $M(x,y) + N(x,y)\frac{dy}{dx} = 0$ is an Exact
Equation, implies: $\Psi_{xy} = M_y$, $\Psi_{yx} = N_x$

\emph{Potential Function}



\paragraph{Integrating Factor}\label{sec:integrating_factor}\hfill

$u(x)$



\subsubsection{Linear Differential Equation}
\label{sec:linear_differential_equation}

An Ordinary Differential Equation is \emph{Linear} if $F$ can be written as a
Linear Polynomial (a Polynomial of Degree $1$) of the \emph{Unknown Function}
($y$ below) and its Derivatives:
\[
  a_0(x)y + a_1(x)y' + a_2(x)y'' + \cdots + a_n(x)y^{(n)} + r(x) = 0
\]
or:
\[
  y^{(n)} = \sum_{i=0}^{n-1} a_i(x) y^{(i)} + r(x)
\]
where $a_i(x)$ and $r(x)$ are Differentiable Functions in $x$ (not necessarily
Linear) and $r(x)$ is called the \emph{Source Term} (or sometimes
\emph{Constant Term}, even though it can be a non-constant Function).

First-order Linear Differential Equations (with non-constant coefficients) and
Homogeneous Linear Differential Equations
(\S\ref{sec:homogeneous_linear_differential}) have solutions that may
be expressed in terms of Integrals (\S\ref{sec:antiderivative})

Second-order and higher Linear Differential Equations with non-constant
coefficients cannot in general be solved by Quadratures (i.e. Integrals --
Kovacic Algorithm for determining if this is possible)

Hypergeometric Functions (\S\ref{sec:hypergeometric_function}) are solutions of
Second-order Linear Differential Equations

Solutions of a Homogeneous Linear Differential Equation form a Vector Space,
ordinarily of Dimension equal to that of Order of the Equation; all Solutions
of a Linear Differential Equation are found by adding to a particular Solution
any Solution of the Associated Homgeneous Equation (FIXME: clarify)

cf. Linear Difference Equation (\S\ref{sec:linear_difference_equation})

\emph{First-order Linear Differential Equations}:
\[
  a(x) y' + b(x) y = c(x)
\]
Homogeneous:
\[
  a(x) y' + b(x) y = 0
\]
Standard Linear Form:
\[
  y' + p(x) y = q(x)
\]
cf. Standard First-order Form (\S\ref{sec:ode}):
\[
  y' = p(x) y + q(x)
\]

\emph{Second-order Linear Differential Equations}:
\[
  a(x) y'' + b(x) y' + c(x) y = d(x)
\]

\asterism

(Kelley-Peterson10, Ch.2)

a System of Linear ODEs (\S\ref{sec:linear_ode_system}) has the form:
\begin{align*}
  x_1' & = a_{11}(t)x_1 + a_{12}(t)x_2 + \cdots + a_{1n}(t)x_n + b_1(t) \\
  x_2' & = a_{21}(t)x_1 + a_{22}(t)x_2 + \cdots + a_{2n}(t)x_n + b_2(t) \\
    & \vdots \\
  x_n' & = a_{n1}(t)x_1 + a_{n2}(t)x_2 + \cdots + a_{nn}(t)x_n + b_n(t) \\
\end{align*}
and can be written as an Inhomogeneous Linear Vector Differential Equation:
\[
  \vec{x}' = A(t)\vec{x} + \vec{b}(t)
\]
where $\vec{x} = [x_1\ x_2\ \cdots\ x_n]^T$ and $x' =
[x_1'\ x_2'\ \cdots\ x_n]^t$ and the entries of $A(t)$ and $\vec{b}(t)$ are
taken from the corresponding $a_ij(t)$ and $b_i(t)$ for $i,j \in
\{1,\ldots,n\}$.
A collection of $n$ Functions $x_1, x_2, \ldots, x_n$ is a Solution of the
System when:
\begin{align*}
  x_1'(t) & = a_{11}(t)x_1(t) + a_{12}(t)x_2(t) + \cdots +
    a_{1n}(t)x_n(t) + b_1(t) \\
  x_2'(t) & = a_{21}(t)x_1(t) + a_{22}(t)x_2(t) + \cdots +
    a_{2n}(t)x_n(t) + b_2(t) \\
    & \vdots \\
  x_n'(t) & = a_{n1}(t)x_1(t) + a_{n2}(t)x_2(t) + \cdots +
    a_{nn}(t)x_n(t) + b_n(t) \\
\end{align*}
or equivalently an $n\times{1}$ Vector Function $\vec{x}$ is a Solution when:
\[
  \vec{x}'(t) = A(t)\vec{x}(t) + \vec{b}(t)
\]

In order to Solve the Inhomogeneous Linear Vector Differential Equation
$\vec{x}' = A(t)\vec{x} + \vec{b}(t)$, one must first Solve the corresponding
\emph{Homogeneous} Linear Vector Differential Equation:
\[
  \vec{x}' = A(t)\vec{x}
\]
which has the property that any Linear Combination of Solutions is also a
Solution (i.e. the Set of all Solutions forms a \emph{Vector Space}). Therefore
to Solve it requires finding $n$ Linearly Independent Solutions.

Autonomous Linear Vector Differential Equation $\vec{x}' = A\vec{x}$ where $A$
is a Constant $n\times{n}$ Matrix



\paragraph{Homogeneous Linear Differential Equation}
\label{sec:homogeneous_linear_differential}\hfill

\emph{not} the same concept as general ``Homogeneous Differential Equations''
(\S\ref{sec:homogeneous_differential_equation})

Homogeneous Linear Differential Equations have the Constant Term $0$, i.e. it
is a Homogeneous Polynomial (\S\ref{sec:homogeneous_polynomial}) in the Unknown
Function and its Derivatives

Solutions of a Homogeneous Linear Differential Equation form a Vector Space,
ordinarily of Dimension equal to that of Order of the Equation; all Solutions
of a Linear Differential Equation are found by adding to a particular Solution
any Solution of the Associated Homgeneous Equation (FIXME: clarify)

an $n$-th Order Homogeneous Linear Differential Equation has Constant
Coefficients if it has the form:
\[
  a_0y + a_1y' + a_2y'' + \cdots + a_ny^{(n)} = 0
\]
(FIXME: otherwise it has polynomial coefficients ???)

e.g. $e^x$ is the unique solution to the Equation $f' = f$ such that $e^0 = 1$,
and it follows that the $n$th Derivative of $e^{cx}$ is $c^n e^{cx}$

Homogeneous Linear Differential Equations have solutions that may be expressed
in terms of Integrals (\S\ref{sec:antiderivative})

Solutions to (Systems of) Homogeneous Linear Differential Equations with
Polynomial Coefficients (FIXME: i.e. non-constant ???) are called
\emph{Holonomic Functions} (\S\ref{sec:holonomic_function}).

\emph{Second-order Homogeneous Linear Differential Equation}:

\[
  A y'' + B y' + C y = 0
\]

if $g(x)$ is a Solution, then $c\cdot{g(x)}$ is also a Solution

if $h(x)$ is also a Solution, then $g(x) + h(x)$ is also a Solution



\subparagraph{Characteristic Equation}\label{sec:characteristic_equation}\hfill

cf. Characteristic Equation of a Homogeneous Difference Equation
(\S\ref{sec:characteristic_difference_equation})

$A y'' + B y' + C y = 0$

$y = e^{rx}$

Characteristic Equation: $Ar^2 + Br + C = 0$

$r = \frac{-B \pm \sqrt{B^2 - 4AC}}{2A}$

Real Roots: $r_1, r_2$

General Solution: $y = c_1e^{r_1x} + c_2e^{r_2x}$

Complex Roots: $\lambda \pm \mu i$

General Solution: $y = e^{\lambda x}(c_1 \cos\mu{x} + c_2 \sin\mu{x})$

Repeated Roots: $r$

\emph{Reduction of Order}: $y = c_1 x e^{rx} + c_2 e^{rx}$



\subparagraph{Holonomic Function}\label{sec:holonomic_function}\hfill

a \emph{Holonomic Function} is an Element of a Holonomic Module
(\S\ref{sec:holonomic_module}) of Smooth Functions

\fist cf. Holonomy (\S\ref{sec:holonomy}), Holonomic Constraints
(\S\ref{sec:holonomic_constraint})

a Multivariable Smooth Function that is a solution to a (System of) Homogeneous
Linear Differential Equation(s) with Polynomial Coefficients

the Class of Holonomic Functions is ``stable'' (FIXME: closed ???) under Sums,
Products, Derivation, and Integration

Strict Superset of the Hypergeometric Functions
(\S\ref{sec:hypergeometric_function})

Holonomic Functions include all Algebraic Functions
(\S\ref{sec:algebraic_function}), some Transcendental Functions ($\sin x$,
$\cos x$, $e^x$, $\log x$), the Generalized Hypergeometric Function
(\S\ref{sec:generalized_hypergeometric_function}), Bessel Functions
(\S\ref{sec:bessel_function}), Airy Functions (TODO: xref), ``classical''
Orthogonal Polynomials (\S\ref{sec:orthogonal_polynomial_sequence})

Holonomic Functions form a Ring (\S\ref{sec:ring}), but are not closed under
Division (so they do not form a Field)



\paragraph{Non-homogeneous Linear Differential Equation}
\label{sec:nonhomogeneous_linear_differential}\hfill

an \emph{$n$-order Non-homogeneous Linear Differential Equation} with Constant
Coefficients may be written:
\[
  y^{(n)}(x) + a_1y^{(n-1)}(x) + \cdots + a_{n-1}y'(x) + a_ny(x) = f(x)
\]
where $a_1,\ldots,a_n$ are Real or Complex Numbers, $f$ is a given Function of
$x$ and $y$ is the Unknown Function

Second-order Non-homogeneous Linear Differential Equation:
\[
  A y'' + B y' + C y = g(x)
\]

for $h$ a Solution for the Homogeneous Linear Differential Equation $Ah'' + Bh'
+ Ch = 0$

let $j(x)$ be a \emph{particular} Solution of $Aj'' + Bj' + Cj = g(x)$

then $k(x) = h(x) + j(x)$ is a Solution of $A y'' + B y' + C y = g(x)$

\emph{Method of Undetermined Coefficients}

for $A y'' + B y' + C y = f(x) + g(x) + \cdots$, the Solution is the General
Solution of the Homogeneous Equation plus the particular Solutions to each of
the individual RHS (FIXME: clarify)



\subsubsection{Non-linear Differential Equation}
\label{sec:nonlinear_differential_equation}

\subsubsection{Delay Differential Equation (DDE)}\label{sec:dde}

Time-delay Systems, Differential-difference Equations



\subsubsection{Autonomous Differential Equation}
\label{sec:autonomous_differential_equation}

An Autonomous ODE (\S\ref{sec:ode}) has the form:
\[
  x' = f(x)
\]
where $x$ is an Unknown Function of $t$. That is, $f$ depends only on $x$ and
not \emph{directly} on $t$. Translating a Solution of such an Autonomous ODE
along the $t$-axis will produce another Solution.

\fist Autonomous System of ODEs (\S\ref{sec:autonomous_ode_system})

\fist Autonomous Dynamical Systems (\S\ref{sec:autonomous_dynamical_system})



\subsubsection{Stochastic Differential Equation (SDE)}\label{sec:sde}

a Differential Equation in which one or more Terms is a \emph{Stochastic
  Process} (\S\ref{sec:stochastic_process})

\fist Stochastic Partial Differential Equation (\S\ref{sec:spde})

(wiki): Chaos (\S\ref{sec:chaos_theory}) as a Spontaneous Breakdown of
Topological Supersymmetry (FIXME: xref) which is an intrinsic property of
\emph{Evolution Operators} of all Stochastic and Deterministic (Partial)
Differential Equations; the long-range Dynamical Behavior associated with
\emph{Chaotic Dynamics} is a consequence of Goldstone's Theorem
(\S\ref{sec:goldstones_theorem}) in the application of Spontaneous Topological
Supersymmetry Breaking



\subsubsection{Functional Differential Equation}\label{sec:fde}

a Differential Equation with \emph{Deviating Argument}

TODO



\paragraph{Differential Difference Equation}\label{sec:dde}\hfill



\subsubsection{Stiff Equation}\label{sec:stiff_equation}

Numerically Unstable (\S\ref{sec:numerical_stability}) when Solved using
Explicit Methods (\S\ref{sec:explicit_integration});
Solve using Implicit Methods (\S\ref{sec:implicit_integration}) instead



\subsubsection{Singular Solution}\label{sec:singular_solution}

\subsubsection{Bessel Function}\label{sec:bessel_function}

cf. Reverse Bessel Polynomials (\S\ref{sec:reverse_bessel_polynomial})

\fist Holonomic Function (\S\ref{sec:holonomic_function})



\paragraph{Cylinder Function}\label{sec:cylinder_function}\hfill

\fist Cylindrical Harmonics (\S\ref{sec:cylindrical_harmonics})



\paragraph{Spherical Bessel Function}\label{sec:spherical_bessel}\hfill

\fist cf. Spherical Harmonics (\S\ref{sec:spherical_harmonics})



\subsubsection{Partial Differential Equation}\label{sec:pde}

A \emph{Partial Differential Equation (PDE)} is a Differential Equation
containing Unknown Multivariable Functions and their Partial Derivatives.

\fist An \emph{Ordinary} Differential Equation (ODE \S\ref{sec:ode}) is a
Differential Equation with a single Independent Variable.

Examples of Infinite-order PDEs may be found in the Quantum Hamilton's Equations
for Trajectories of Quantum Particles (Phase Space Formulation) or the Evolution
Equation of the Wigner Function.

Finite Element Method (\S\ref{sec:finite_element_method}): solving Boundary
Value Problems (\S\ref{sec:boundary_value_problem}) for Partial Differential
Equations

\fist Numerical Integration (\S\ref{sec:numerical_integration})

\fist Finite-Difference Methods (FDMs \S\ref{sec:fdm}) -- Numerical Methods for
Solving Differential Equations by Approximating them with Difference Equations
(\S\ref{sec:difference_equation}); dominant approach to Numerical Solutions of
PDEs

\fist Hamilton-Jacobi-Bellman Equation (Optimal Control Theory
\S\ref{sec:hamilton_jacobi_bellman})

\fist Hodge Theory (\S\ref{sec:hodge_theory}) uses Partial Differential
Equations to study the Cohomology Groups (\S\ref{sec:cohomology_group}) of
Smooth Manifolds (\S\ref{sec:smooth_manifold})



\paragraph{Linear Partial Differential Equation}\label{sec:linear_pde}\hfill

\fist $D$-modules (\S\ref{sec:d_module}) -- Modules over a Ring $D$ of
Differential Operators (\S\ref{sec:differential_operator}); an approach to the
theory of Linear Partial Differential Equations



\paragraph{Wave Equation}\label{sec:wave_equation}\hfill

a Second-order Linear Partial Differential Equation for the description of
\emph{Waves} as they occur in Classical Physics

Scalar Wave Equation in one Space Dimension:
\[
  \frac{\partial^2 u}{\partial{t}^2} = c^2 \frac{\partial^2 u}{\partial{x}^2}
\]

Scalar Wave Equation in two Space Dimensions

in general Dimensions



\subparagraph{Spherical Wave}\label{sec:spherical_wave}\hfill

Scalar Wave Equation in three Space Dimensions

Separation of Variables (Fourier Method \S\ref{sec:separation_of_variables})

Spherical Wave Transformation (\S\ref{sec:spherical_wave_transformation})
leaves the form of Spherical Waves Invariant in all Inertial Frames
(\S\ref{sec:inertial_frame})



\paragraph{Elliptic Partial Differential Equation}
\label{sec:elliptic_partial_differential}\hfill

\subparagraph{Poisson Equation}\label{sec:poisson_equation}\hfill

$\nabla^2 u = u_{xx} + u{yy} = f(x,y)$



\subparagraph{Laplace's Equation}\label{sec:laplaces_equation}\hfill

Second-order Elliptic Partial Differential Equation

written:
\[ \nabla^2 \varphi = 0 \]
or:
\[ \Delta \varphi = 0 \]

where $\Delta = \nabla^2$ is the Laplace Operator
(\S\ref{sec:laplace_operator}) and $\varphi$ is a Scalar Function
(\S\ref{sec:scalar_function})

a Conformal (Angle Preserving) Mapping (\S\ref{sec:conformal_map}) preserves
Laplace's Equation (\S\ref{sec:laplaces_equation})-- only changes the Laplacian
by a non-negative factor

a Harmonic Function (\S\ref{sec:harmonic_function}) is a Twice-continuously
Differentiable Function (\S\ref{sec:continuously_differentiable}) $f : U
\rightarrow Reals$, where $U$ is an Open Subset of $\reals^n$, satisfying
Laplace's Equation

$\nabla^2 u = u_{xx} + u_{yy} = 0$ %FIXME

Steady State Condition (TODO)



\paragraph{Dirichlet Problem}\label{sec:dirichlet_problem}\hfill

\paragraph{Stochastic Partial Differential Equation}\label{sec:spde}\hfill

\fist Stochastic Differential Equation (\S\ref{sec:sde})



% ====================================================================
\section{Vector Calculus}\label{sec:vector_calculus}
% ====================================================================

or \emph{Vector Analysis}

\fist cf. Linear Algebra (Part \ref{part:linear_algebra})

Gradient, Divergence, Curl

$\nabla$

FIXME:

Directional Derivative

Laplacian

Tensor Derivative



% --------------------------------------------------------------------
\subsection{Vector}\label{sec:linear_vector}
% --------------------------------------------------------------------

or \emph{Polar Vector}

\fist Vector (Abstract Algebra \S\ref{sec:vector})



% --------------------------------------------------------------------
\subsection{Pseudovector}\label{sec:pseudovector}
% --------------------------------------------------------------------

or \emph{Axial Vector}

\fist Pure Imaginary Quaternion (\S\ref{sec:quaternion})

Angular Velocity behaves like a Pseudovector

(3 Dimensions) associated with Curl of a Polar Vector or Cross Product of two
Polar Vectors

equivalent to Three-dimensional Bivectors (\S\ref{sec:bivector})



% --------------------------------------------------------------------
\subsection{Dot Product}\label{sec:dot_product}
% --------------------------------------------------------------------

or \emph{Scalar Product}

Inner Product (\S\ref{sec:inner_product})



% --------------------------------------------------------------------
\subsection{Cross Product}\label{sec:cross_product}
% --------------------------------------------------------------------

or \emph{Vector Product}



\subsubsection{Scalar Triple Product}\label{sec:scalar_triple_product}



% --------------------------------------------------------------------
\subsection{Vector Projection}\label{sec:vector_projection}
% --------------------------------------------------------------------

Projection of Vector $\vec{a}$ onto a Non-zero Vector $\vec{b}$ is the
Orthogonal Projection (\S\ref{sec:orthogonal_projection}) of $\vec{a}$ onto a
Straight Line Parallel to $\vec{b}$, resulting in a Vector Parallel to
$\vec{b}$:
\[
  \big(\vec{a} \bullet \frac{\vec{b}}{|\vec{b}|}\big)
  \big(\frac{\vec{b}}{|\vec{b}|}\big)
\]
where $\vec{b}/|\vec{b}|$ is the Unit Vector in the Direction of $\vec{b}$

or equivalently:
\[
\frac{\vec{b} \bullet \vec{a}}
  {\vec{b} \bullet \vec{b}} \cdot \vec{b}
\]



% --------------------------------------------------------------------
\subsection{Vector Function}\label{sec:vector_function}
% --------------------------------------------------------------------

\url{https://math.stackexchange.com/questions/662009/what-is-the-difference-between-vector-valued-functions-and-parametric-equations}:

A (Real, potentially Multivariate) \emph{$n$-dimensional Vector Function} (or
\emph{Vector-valued Function}) is a Map:
\[
  f : U \subset \reals^m \rightarrow V \subset \reals^n
\]

(wiki):

For $U \subset \reals$, $f$ can be written as a Function of a Real Number $x$ or
equivalently as individual Component Functions $f_1, f_2, \ldots, f_n$:
\[
  f(x) = (f_1(t), f_2(t), \ldots, f_n(t))
\]
Then the Derivative (\S\ref{sec:derivative}) can be written in terms of the
Derivatives of the Coordinate Functions:
\[
  f'(x) = (f_1'(t), f_2'(t), \ldots, f_n'(t))
\]

For $f$ a Function of several variables $\vec{x} \in \reals^m$, the First-order
Partial Derivatives of the Components of $f$ form an $n \times m$ Matrix called
the \emph{Jacobian Matrix} (\S\ref{sec:jacobian}) of $f$.

the Matrix of all First-order Partial Derivatives
(\S\ref{sec:partial_derivative}) of a Vector Function is the \emph{Jacobian
  Matrix} (\S\ref{sec:jacobian})

\fist cf. a \emph{Parameterization} (\S\ref{sec:parameterization}) for a
portion of a Submanifold $M$ in Euclidean Space is a Map:
\[
  \varphi : U \subset R^m \rightarrow M \subset R^n
\]
with additional Properties:
\begin{itemize}
  \item $U$ is an Open Set (\S\ref{sec:open_set})
  \item $\varphi$ is a Homeomorphism (\S\ref{sec:homeomorphism}) onto its Image
  \item everywhere the Rank of $D\varphi = m$ (FIXME: clarify)
\end{itemize}
i.e. a Parameterization is always in the form of a Vector Valued Function, but
conversely Vector-valued Functions are used to Parameterize Varieties
(\S\ref{sec:algebraic_variety}) %FIXME: clarify

Partial Derivative of a Multivariable Vector-valued Function (TODO)



\subsubsection{Root}\label{sec:function_root}

A \emph{Root} (or \emph{Zero}) of a Vector-valued Function (including Real- or
Complex-valued Functions) $f$ is an Element $x$ of the Domain of $f$ for which
$f(x)$ is \emph{Zero}.

\fist Root (Equations \S\ref{sec:equation_root})

\fist Root-finding Algorithms (Numerical Analysis \S\ref{sec:root_finding}) for
Solving Non-linear Equations:
\begin{itemize}
  \item Newton's Method (\S\ref{sec:newtons_method})
  \item Linearization (\S\ref{sec:linearization})
\end{itemize}

Any Polynomial (\S\ref{sec:polynomial}) with Odd Degree has at least
one Real Root.

(wiki):

The Fundamental Theorem of Algebra (\S\ref{sec:fundamental_algebra_theorem})
states that every Polynomial of Degree $n$ has $n$ Complex Roots, counted with
their Multiplicities (??? FIXME).

\emph{Link between ``Algebra'' and ``Geometry''}: a Monic Polynomial (one
Variable with Complex Coefficients \S\ref{sec:monic_polynomial}), an Algebraic
Object, is determined by the Set of its Roots, a Geometric Object, in the
Complex Plane (\S\ref{sec:complex_plane}).

\asterism

\fist cf. Discrete First-order Dynamical Systems
(\S\ref{sec:discrete_dynamical_system}) can be used to Numerically Solve
Single-variable Equations-- the problem to be Solved consisting of finding the
Roots of a Real Function $f$, i.e. the values of $x$ such that:
\[
  f(x) = 0
\]
For Equations that cannot be solved Analytically, the Numerical Method involves
defining a Dynamical System with Convergent Sequences which approach Solutions
to the Equation



\paragraph{Zero Set}\label{sec:zero_set}\hfill

\fist Algebraic Geometry (Part \ref{part:algebraic_geometry}): study of Zero
Sets of Systems of Polynomial Equations
(\S\ref{sec:polynomial_equation_system})

e.g. the Circle (\S\ref{sec:n_sphere}) $S^1 = \{ (x,y) \in \reals^2 | x^2 + y^2
- 1 = 0 \}$

(wiki):

any Closed Subset of $\reals^n$ is the Zero Set of a Smooth Function
(\S\ref{sec:smooth_function}) on $\reals^n$

an Affine Algebraic Set (Affine Variety \S\ref{sec:affine_variety}) is the
Intersection of the Zero Sets of a number of Polynomials in a Polynomial Ring
(\S\ref{sec:polynomial_ring}) $k[x_1,\ldots,x_n]$ over a Field



\subsubsection{Vector Derivative}\label{sec:vector_derivative}

Derivative (\S\ref{sec:derivative}) of a Vector-valued Function of a single Real
Variable

$r : \reals \rightarrow \reals^n$

$\deriv{\vec{r}(t)}{t}$

when the parameter is interpreted as Time, then the First Vector Derivative
represents the \emph{Velocity} and the Second Vector Derivative represents the
\emph{Acceleration}

the Vector Derivative represents a change of \emph{Length} (cf. Norm
\S\ref{sec:norm}) of the Vector

cf. Directional Derivative (\S\ref{sec:directional_derivative})



% --------------------------------------------------------------------
\subsection{Directional Derivative}\label{sec:directional_derivative}
% --------------------------------------------------------------------

$\frac{\partial{f}}{\partial{\vec{v}}}$

$\nabla_{\vec{v}} f = \vec{v}\bullet \nabla{f}$

Gradient (\S\ref{sec:gradient})

for a Complex Function (\S\ref{sec:complex_function}) to have a Derivative, the
Directional Derivative must exist in every direction and be the same

Derivation of Quaternion Functions (\S\ref{sec:quaternion_function}) requires a
Direction-dependent Derivative (FIXME: clarify)

\fist generalized as Covariant Derivative (\S\ref{sec:covariant_derivative})

cf. Vector Derivative (\S\ref{sec:vector_derivative})



% --------------------------------------------------------------------
\subsection{Gradient}\label{sec:gradient}
% --------------------------------------------------------------------

Partial Derivative (\S\ref{sec:partial_derivative})

Directional Derivative (\S\ref{sec:directional_derivative})

Differential Operator (\S\ref{sec:differential_operator})

(wiki):

for a Multivariate Scalar-valued Function (\S\ref{sec:scalar_function}) $f(x_1,
\ldots, x_n)$ on $\reals^n$ for which a Partial Derivative $\pderiv{f}{x_j}$
exists for each Variable $x_j$ at a Point $a = (a_1, \ldots, a_n)$ the Partial
Derivatives define the \emph{Gradient Vector} of $f$ as $a$:
\[
  \nabla{f}(a_1, \ldots, a_n) = \big( \pderiv{f}{x_1}(a_1, \ldots, a_n), \ldots,
  \pderiv{f}{x_n}(a_1, \ldots, a_n) \big)
\]

the Gradient points in the \emph{direction} of greatest inscrease of the
Function, and its magnitude is the \emph{slope} of the Graph in that direction

the Gradient of a Scalar Field (\S\ref{sec:scalar_field}) $f$ is the Vector
Derivative (\S\ref{sec:vector_derivative}):
\[
  \nabla f =
    \frac{\partial f}{\partial x}\vec{e}_x +
    \frac{\partial f}{\partial y}\vec{e}_y +
    \frac{\partial f}{\partial z}\vec{e}_z
\]
that is, the Gradient of $f$ is a Vector Field (\S\ref{sec:vector_field})

when a Vector Field is the Gradient Field of some Scalar Field
(\S\ref{sec:scalar_field}), it is a \emph{Conservative} (\emph{Path
  Independent} \S\ref{sec:conservative_vector_field}) Vector Field and the Line
Integral of any Path between two Points is equivalent and the Line Integral of
a Closed Path from a Point to itself is Zero

FIXME: Matrix of Derivatives, Vector Field of Row Vectors

the Jacobian (\S\ref{sec:jacobian}) generalizes the Gradient of a Scalar-valued
Function of multiple Variables to the Vector-valued
(\S\ref{sec:vector_function}) case: the Jacobian of a Scalar-valued
Multivariable Function is the Gradient, and the Jacobian of a Scalar-valued
Single Variable Function is its Derivative (\S\ref{sec:derivative})

Divergence (\S\ref{sec:divergence}) can be expressed as:
\[
  \nabla \cdot \vec{v} =
    \frac{\partial v_x}{\partial x} +
    \frac{\partial v_y}{\partial y} +
    \frac{\partial v_z}{\partial z}
\]

the Laplacian (\S\ref{sec:laplacian}) is the Divergence of the Gradient Field
of $f$: $\nabla \bullet \nabla f$

Curl (\S\ref{sec:curl}) can be expressed as:
\[
  \nabla\times\vec{v} =
    (\frac{\partial v_z}{\partial y}-\frac{\partial v_y}{\partial z})\vec{e}_x +
    (\frac{\partial v_x}{\partial z}-\frac{\partial v_z}{\partial x})\vec{e}_y +
    (\frac{\partial v_y}{\partial x}-\frac{\partial v_x}{\partial y})\vec{e}_z
\]

\emph{Critical Point} (\S\ref{sec:critical_point}): Gradient equal to the Zero
Vector; cf. Maxima, Minima, Saddle Points (\S\ref{sec:saddle_point})

(\emph{Local Optimality Conditions} \S\ref{sec:objective_function}) generally a
Real Function $f$ on $n$ Real Variables has local Minimum at $\vec{x}^*$ if its
Gradient $\nabla f$ is Zero and its Hessian Matrix (\S\ref{sec:hessian_matrix})
$H(f(\vec{x}^*))$ of Second-order Partial Derivatives is Positive Semi-definite
(\S\ref{sec:positive_semidefinite}) at that Point:
\begin{align*}
  \nabla f(\vec{x}^*) & =    \vec{0} \\
  H(f(\vec{x}^*))     & \geq \vec{0} \\
\end{align*}



\subsubsection{Gradient Flow}\label{sec:gradient_flow}

cf. Flow (\S\ref{sec:flow}), Vector Flow (\S\ref{sec:vector_flow})

Geometric Flow (\S\ref{sec:geometric_flow})



% --------------------------------------------------------------------
\subsection{Jacobian}\label{sec:jacobian}
% --------------------------------------------------------------------

For $f$ a Function of several variables $\vec{x} \in \reals^m$, the First-order
Partial Derivatives of the Components of $f$ form an $n \times m$ Matrix called
the \emph{Jacobian Matrix} (\S\ref{sec:jacobian}) of $f$.

a Square Jacobian Matrix and its Determinant (\S\ref{sec:jacobian_determinant})
are both called ``the Jacobian''

generalizes the Gradient (\S\ref{sec:gradient}) of a Scalar-valued Function of
multiple Variables: the Jacobian of a Scalar-valued Multivariable Function is
the Gradient, and the Jacobian of a Scalar-valued Single Variable Function is
its Derivative

for a Multivariate Real-valued Function, the Jacobian Matrix
(\S\ref{sec:jacobian}) reduces to the Gradient Vector

\fist the Jacobian Matrix for a System of DAEs (\S\ref{sec:dae_system}) is
a Singular (Non-invertible) Matrix (\S\ref{sec:singular_matrix})

\fist Locally Linear Transformation (\S\ref{sec:locally_linear})

\fist Local Linearization (\S\ref{sec:linearization})

the Absolute Condition Number (\S\ref{sec:absolute_condition_number}),
$\hat{k}$, of a Differentiable Function $f(x)$ is equal to the Matrix Norm
(\S\ref{sec:matrix_norm}) of its Jacobian Matrix $J$:
\[
  \hat{k} = \|J(x)\|
\]



\subsubsection{Jacobian Determinant}\label{sec:jacobian_determinant}



% --------------------------------------------------------------------
\subsection{Hessian}\label{sec:hessian}
% --------------------------------------------------------------------

Matrix of all Second-order Partial Derivatives (\S\ref{sec:partial_derivative})
of a Scalar-valued Function or Scalar Field

\fist Quadratic Approximation (\S\ref{sec:quadratic_approximation})

(\emph{Local Optimality Conditions} \S\ref{sec:objective_function}) generally a
Real Function $f$ on $n$ Real Variables has local Minimum at $\vec{x}^*$ if its
Gradient (\S\ref{sec:gradient}) $\nabla f$ is Zero and its Hessian Matrix
$H(f(\vec{x}^*))$ is Positive Semi-definite (\S\ref{sec:positive_semidefinite})
at that Point:
\begin{align*}
  \nabla f(\vec{x}^*) & = \vec{0}    \\
  H(f(\vec{x}^*))     & \geq \vec{0} \\
\end{align*}



% --------------------------------------------------------------------
\subsection{Divergence}\label{sec:divergence}
% --------------------------------------------------------------------

wiki:

the \emph{Divergence} of a Vector Field (\S\ref{sec:vector_field})
$\vec{v}(x,y,z) = v_x\vec{e}_x + v_y\vec{e}_y + v_z\vec{e}_z$ is a
Scalar-valued Function (\S\ref{sec:scalar_function}):
\[
  \nabla \cdot \vec{v} =
    \frac{\partial v_x}{\partial x} +
    \frac{\partial v_y}{\partial y} +
    \frac{\partial v_z}{\partial z}
\]
where $\nabla$ is the Gradient Operator (\S\ref{sec:gradient})

\emph{Convergence}

...

\[
  \vec{v}(x,y,z) = \begin{bmatrix}
    p(x,y,z) \\
    q(x,y,z) \\
    r(x,y,z)
  \end{bmatrix}
\]

\[
  div\vec{v}(x,y,z)
    = \frac{\partial{p}}{\partial{x}}
    + \frac{\partial{q}}{\partial{y}}
    + \frac{\partial{r}}{\partial{z}}
\]

the Laplacian (\S\ref{sec:laplacian}) is the Divergence of the Gradient Field
of $f$: $\nabla \bullet \nabla f$

Green's Theorem (\S\ref{sec:greens_theorem}):

TODO



\subsubsection{Laplacian}\label{sec:laplacian}

\fist cf. Laplace Operator (\S\ref{sec:laplace_operator}), Laplace Transform
(\S\ref{sec:laplace_transform}) %FIXME: same concepts?

the \emph{Laplacian}, $\Delta$, is the Divergence (\S\ref{sec:divergence}) of
the Gradient Field of $f$:
\[
  \Delta f = \nabla \bullet \nabla f
\]

analog of Second Derivative for Scalar-valued Multivariable Functions

\[
  \frac{\partial^2 f}{\partial{x_1^2}} +
  \frac{\partial^2 f}{\partial{x_2^2}} +
  \cdots +
  \frac{\partial^2 f}{\partial{x_n^2}}
\]

the Laplacian of a \emph{Harmonic Function} (\S\ref{sec:harmonic_function}) is
Zero everywhere; intuitively this means that the average value of Neighbors for
a given Point is equal to that Point

a Conformal Map (\S\ref{sec:conformal_map}) only changes the Laplacian by a
non-negative factor



% --------------------------------------------------------------------
\subsection{Curl}\label{sec:curl}
% --------------------------------------------------------------------

wiki:

the \emph{Curl} of a Vector Field (\S\ref{sec:vector_field}) $\vec{v}(x,y,z) =
v_x\vec{e}_x + v_y\vec{e}_y + v_z\vec{e}_z$ is a Vector-valued Function
(\S\ref{sec:vector_function}):
\[
  \nabla\times\vec{v} =
    (\frac{\partial v_z}{\partial y}-\frac{\partial v_y}{\partial z})\vec{e}_x -
    (\frac{\partial v_x}{\partial z}-\frac{\partial v_z}{\partial x})\vec{e}_y +
    (\frac{\partial v_y}{\partial x}-\frac{\partial v_x}{\partial y})\vec{e}_z
\]
where $\nabla$ is the Gradient Operator (\S\ref{sec:gradient}) associating each
Point in the Vector Field with the proportional ``on-axis'' Torque to which a
``tiny pinwheel'' would be subjected if it were centered at the Point and can
be realized as a Pseudo-determinant (\S\ref{sec:pseudo_determinant}):

TODO

Right-hand Rule

Counter-clockwise: Positive

Clockwise: Negative

Tangency ?

Green's Theorem (\S\ref{sec:greens_theorem}) -- FIXME: relation to curl ???

Stokes' Theorem (\S\ref{sec:stokes_theorem})

\[
  \oint_C \vec{f} d\vec{r}
    = \iint_S ((\nabla \times \vec{f}) \bullet \hat{n}_S) d\vec{S}
\]



% --------------------------------------------------------------------
\subsection{Local Linearization}\label{sec:linearization}
% --------------------------------------------------------------------

\emph{Linearization} -- finding the Linear Approximation
(\S\ref{sec:linear_approximation}) of a Function at a given Point

\fist cf. Locally Linear Transformation (\S\ref{sec:locally_linear})

$L(x_0,y_0) = f(x_0,y_0)$

$\frac{\partial{L}}{\partial{x}} = f_x(x_0,y_0)$

$\frac{\partial{L}}{\partial{y}} = f_y(x_0,y_0)$

a Quadratic Approximation (\S\ref{sec:quadratic_approximation}) adds Quadratic
Terms so that the Second Partial Derivative matches the original Function at
that Point

\fist Jacobian Matrix (\S\ref{sec:jacobian})

Matrix form:
\[
  \nabla f(\vec{x}_0) \bullet (\vec{x}-\vec{x}_0)
\]
(FIXME: correct ???)



% --------------------------------------------------------------------
\subsection{Quadratic Approximation}\label{sec:quadratic_approximation}
% --------------------------------------------------------------------

add Quadratic Terms to a Local Linearization so that Second Derivative matches
the original Function at that Point

$\frac{\partial^2{Q}}{\partial{x^2}}(x_0,y_0) =
  \frac{\partial^2{f}}{\partial{x^2}}(x_0,y_0)$

$\frac{\partial^2{Q}}{\partial{y}\partial{x}}(x_0,y_0) =
  \frac{\partial^2{f}}{\partial{y}\partial{x}}(x_0,y_0)$

$\frac{\partial^2{Q}}{\partial{y^2}}(x_0,y_0) =
  \frac{\partial^2{f}}{\partial{y^2}}(x_0,y_0)$

\fist Hessian Matrix (\S\ref{sec:hessian_matrix}) $\mathbf{H}$ -- Matrix of
Second-order Partial Derivatives

\fist Definite Quadratic (\S\ref{sec:definite_quadratic}), Positive-definite
Matrices (\S\ref{sec:positive_definite})

Matrix form:
\[
  Q_f(\vec{x}) = f(\vec{x}_0) + \nabla{f(\vec{x}_0)}\bullet (\vec{x}-\vec{x}_0)
  + \frac{1}{2}(\vec{x}-\vec{x}_0)^T\mathbf{H}_f(\vec{x}_0)(\vec{x}-\vec{x}_0)
\]



% --------------------------------------------------------------------
\subsection{Integral Theorems of Vector Calculus}
\label{sec:integral_theorems}
% --------------------------------------------------------------------

cf. Fundamental Theorem of Calculus (\S\ref{sec:fundamental_calculus_theorem})



\subsubsection{Gradient Theorem}\label{sec:gradient_theorem}

\subsubsection{Stoke's Theorem}\label{sec:stokes_theorem}

\fist Green's Theorem (\S\ref{sec:greens_theorem})

Curl (\S\ref{sec:curl})

Simple Closed Piecewise Smooth Boundary $C$

Piecewise Smooth Surface $S$

\[
  \oint_C \vec{f} d\vec{r}
    = \iint_S ((\nabla \times \vec{f}) \bullet \hat{n}) d\vec{S}
\]

\fist 3D Divergence Theorem (\S\ref{sec:divergence_theorem})



\subsubsection{Divergence Theorem}\label{sec:divergence_theorem}

2D Divergence Theorem \fist cf. Green's Theorem (\S\ref{sec:greens_theorem})

3D Divergence Theorem \fist cf. Stokes' Theorem (\S\ref{sec:stokes_theorem})

``simple solid regions''



% ====================================================================
\section{Matrix Calculus}\label{sec:matrix_calculus}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Matrix Derivative}\label{sec:matrix_derivative}
% --------------------------------------------------------------------

(wiki):

e.g. for a Quadratic Form augmented with Linear Terms, $x^TAx + 2b^Tx$, the
First-order Conditions for a Maximum or a Minimum are found by setting the
Matrix Derivative to the Zero Vector:
\[
  2Ax + 2b = 0
\]
giving:
\[
  x = -A^{-1}b
\]
assuming $A$ is Non-singular (Invertible); if the Quadratic Form (and hence
$A$) are Positive Definite, then the Second-order Conditions for a
Minimum are met at the Point



% ====================================================================
\section{Tensor Calculus}\label{sec:tensor_calculus}
% ====================================================================

or \emph{Tensor Analysis}



% --------------------------------------------------------------------
\subsection{Tensor}\label{sec:linear_tensor}
% --------------------------------------------------------------------

wikipedia: Geometric Objects (???) that describe Linear Relations
between Scalars, Vectors and other Tensors; Linear Relations such as
Dot Product, Cross Product, Linear Maps

%FIXME: merge with sec:tensor ???

\fist Tensor (Abstract Algebra \S\ref{sec:tensor})

\fist Metric Tensor (\S\ref{sec:metric_tensor})

must be independent of a particular choice of Coordinate System
(Coordinate-free \S\ref{sec:coordinate_free}): the particular Covariant
Transformation Law (\S\ref{sec:covariant_transformation}) determines
the \emph{Valence} (\S\ref{sec:valence}) of a Tensor

may be represented by:
\begin{itemize}
  \item Multidimensional Arrays (Basis Independence not apparent)
  \item Multilinear Maps (intrinsic Basis Independence)
  \item Elements of (Abstract) Tensor Products
\end{itemize}

\fist
\url{https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/}
%FIXME cite

Tensors are Elements (Vectors) of a Vector Space given by
combining two ``smaller'' Vector Spaces via a Tensor Product

$v \otimes w \in V \otimes W$

Scalar Multiplication: $s(v \otimes w) = (sv \otimes w) = (v \otimes
sw)$; generalizing to  $n$-fold Tensor Products, Scalars can be moved
around all the coordinates freely

Addition Operation: $(v \otimes w) + (v' \otimes w) = (v + v') \otimes
w)$ \emph{or} $(v \otimes w) + (v \otimes w') = v \otimes (w + w')$,
otherwise $(x \otimes y) + (z \otimes w)$ is a ``new'' Tensor
(Vector); generalizing to $n$-fold Tensor Products, Addition can be
combined if all but one of the coordinates are the same in the
Addends.

Element of a Tensor Space $V_1 \otimes \cdots \otimes V_n$ as a Sum:
\[
  \Sigma_k a_{1,k} \otimes a_{2,k} \otimes cdots \otimes a_{n,k}
\]

A Rank $1$ or \emph{Pure Tensor} is one that can be expressed as a
One-term Sum, i.e. just $a_1 \otimes \cdots \otimes a_n$


\asterism


represented as an ``organized'' Multidimensional Array of Numerical
(?Scalar) Values

\emph{Order} (or \emph{Degree}) of a Tensor $x$ is the
Dimensionality of the Array needed to represent it or equivalently the
minimum number of Terms to represent $x$ as a Sum of Pure Tensors (the
Zero Element is Order $0$ by convention).

Order $0$ Tensor -- Scalar (\S\ref{sec:scalar})

Order $1$ Tensor -- Vector (\S\ref{sec:vector})

Order $2$ Tensor -- Linear Map (\S\ref{sec:linear_transformation}) ?
%FIXME correct?

Computing Tensor Order is $NP$-hard when $k = \rats$ and $NP$-complete
when $k$ is a Finite Field %FIXME


\asterism


Multilinear Maps


\asterism


a Type (\S\ref{sec:valence}) $(n,m)$ Tensor $T$ is defined as an
Element of the Tensor Product (\S\ref{sec:tensor_product}) of Vector
Spaces $V$ and Dual Spaces $V^*$:
\[
  T \in V_1 \otimes \cdots \otimes V_n
    \otimes V^*_1 \otimes \cdots \otimes V^*_m
\]

the Tensor Product is Initial with respect to Multilinear Mappings
from the Direct Product

Tensors (Abstract Algebra \S\ref{sec:tensor})

Tensor Products (Category Theory \S\ref{sec:tensor_product}):
\begin{itemize}
  \item Modules
  \item Vector Spaces
  \item Graded Vector Spaces
  \item $R$-algebras
  \item Sheaves of Modules
  \item Quadratic Forms
  \item Multilinear Form
  \item Topological Vector Spaces
\end{itemize}
are characterized by the Universal Property that they are the ``Freest''
(\S\ref{sec:free_object}) Bilinear Operators in each context
(FIXME: clarify)

Monoidal Category (\S\ref{sec:monoidal_category}): general context for
Tensor Products



% --------------------------------------------------------------------
\subsection{Module Tensor Product}\label{sec:module_tensor_product}
% --------------------------------------------------------------------

Balanced Product

\fist Tensor Product (\S\ref{sec:tensor_product})



% --------------------------------------------------------------------
\subsection{Valence}\label{sec:valence}
% --------------------------------------------------------------------

precise Covariant Transformation Law
(\S\ref{sec:covariant_transformation}) determines the \emph{Valence}
(or \emph{Type}) of a Tensor

Tensor Type: Pair of Natural Numbers $(n,m)$ where $n$ is the number
of Contravariant Indicies and $m$ is the number of Covariant Indices

\emph{Total Order} is the sum of $n$ and $m$



% --------------------------------------------------------------------
\subsection{Tensor Field}\label{sec:tensor_field}
% --------------------------------------------------------------------

Tensor assigned to each Point in a Space

cf. Scalar Field (\S\ref{sec:scalar_field})

cf. Vector Field (\S\ref{sec:vector_field})



% --------------------------------------------------------------------
\subsection{Tensor Algebra}\label{sec:tensor_algebra}
% --------------------------------------------------------------------

of a Vector Space



% --------------------------------------------------------------------
\subsection{Eigenconfiguration}\label{sec:eigenconfiguration}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Ricci Calculus}\label{sec:ricci_calculus}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Spinor}\label{sec:spinor}
% --------------------------------------------------------------------



% ====================================================================
\section{Stochastic Calculus}\label{sec:stochastic_calculus}
% ====================================================================

%FIXME: move to probability theory ???

cf. Stochastic $\pi$-calculus (\S\ref{sec:stochastic_pi_calculus})



% --------------------------------------------------------------------
\subsection{It\^o Calculus}\label{sec:ito_calculus}
% --------------------------------------------------------------------

\subsubsection{It\^o Integral}\label{sec:ito_integral}
