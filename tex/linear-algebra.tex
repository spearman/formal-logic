%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Linear Algebra}\label{part:linear_algebra}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Set Theory (Part \ref{part:set_theory}) is Linear Algebra over the
``Field with one Element''

cf. Vector Calculus (\S\ref{sec:vector_calculus})

\fist Linear Equations (\S\ref{sec:linear_equation}), Systems of Linear
Equations (\S\ref{sec:system_of_linear_equations})



% ====================================================================
\section{Module}\label{sec:module}
% ====================================================================

A \emph{Module} is a Unital Ring (\S\ref{sec:unital_ring}), $R$,
together with an Abelian Group (\S\ref{sec:abelian_group}), $(M, +)$,
and an Operation called \emph{Scalar Multiplication} which is either:
\[ R \times M \rightarrow M \]
for a \emph{Left $R$-module $M$}, $_R M$, or:
\[ M \times R \rightarrow M \]
for a \emph{Right $R$-module $M$}, $M_R$.

The Scalar Multiplication Operator is required that for all $r,s \in
R$ and $x,y \in M$ in a Left $R$-module $M$:
\begin{enumerate}
    \item $r(x + y) = rx + ry$
    \item $(r + s)x = rx + sx$
    \item $(rs)x = r(sx)$
    \item $1_Rx = x$
\end{enumerate}
or in a Right $R$-module $M$:
\begin{enumerate}
    \item $(x + y)r = xr + yr$
    \item $x(r + s) = xr + xs$
    \item $x(rs) = (sx)r$
    \item $x 1_R = x$
\end{enumerate}
where $1_R$ is the Multiplicative Identity for $R$. If the Ring is not
required to be Unital, then item (4) above can be ommitted, but can be
explicitly required by stating that we are talking about a
\emph{Unital Left/Right $R$-module $M$}.

\emph{Bimodule}

If $R$ is Commutative, then Left $R$-modules are the same as Right
$R$-modules and simply called \emph{$R$-modules}.

A Module Homomorphism is a \emph{Linear Map} (\S\ref{sec:linear_transformation})

Vector Space (\S\ref{sec:vector_space})

Multilinear Algebra (\S\ref{sec:multilinear_algebra}):
\emph{Multilinear Map} (\S\ref{sec:multilinear_map})



% --------------------------------------------------------------------
\subsection{Linear Transformation}\label{sec:linear_transformation}
% --------------------------------------------------------------------

A \emph{Linear Transformation} (or \emph{Linear Map}) is a Module Homomorphism
(\S\ref{sec:module}) $L : V \rightarrow W$ preserving Addition and Scalar
Multiplication:
\begin{align*}
  L(a\vec{v})              & = aL(\vec{v}) \\
  L(\vec{v}_1 + \vec{v}_2) & = L(\vec{v}_1 + \vec{v}_2)
\end{align*}

\fist cf. Linear Function (\S\ref{sec:linear_function}) -- a Polynomial
Function of Degree Zero or One

\fist cf. Locally Linear Transformations (\S\ref{sec:locally_linear})

Any Linear Map $A : V \rightarrow W$ from an $n$-dimensional Vector Space $V$
to an $m$-dimensional Vector Space $W$ can be represented by an $m \times n$
Matrix of Rank $r$ sending Column Vector $\vec{v} \in V$ to the Column Vector
$A\vec{v} \in W$. Each such Linear Transformation induces Four Fundamental
Subspaces (\S\ref{sec:fundamental_subspace}):
\begin{enumerate}
  \item $im(A) \subset W$ -- the \emph{Image} of $A$ is the \emph{Column Space}
    (\S\ref{sec:column_space}) of Dimension $r$
  \item $coim(A) \subset V$ -- the \emph{Coimage} of $A$ is the \emph{Row Space}
    (\S\ref{sec:row_space}) of Dimension $r$
  \item $ker(A) \subset V$ -- the \emph{Kernel} of $A$ is the \emph{Nullspace}
    (\S\ref{sec:nullspace}) of Dimension $n - r$ (Nullity) %FIXME clarify
  \item $coker(A) \subset W$ -- the \emph{Cokernel} of $A$ is the \emph{Left
    Nullspace} (\S\ref{sec:left_nullspace}) of Dimension $m - r$ (Corank)
    %FIXME clarify
\end{enumerate}
such that:
\begin{enumerate}
  \item $ker(A) = coim(A)^\bot$ -- the Nullspace is the Orthogonal Complement
    (\S\ref{sec:orthogonal_complement}) of the Row Space
  \item $coker(A) = im(A)^\bot$ -- the Left Nullspace is the Orthogonal
    Complement of the Column Space
\end{enumerate}

\url{https://www.youtube.com/watch?v=VmfTXVG9S0U&feature=youtu.be&t=1m39s}: in
a Linear Transformation of a Space, all of the ``grid lines'' of the Space
remain \emph{Parallel} and \emph{evenly spaced}

Linear Transformations are Morphisms in the Category of Modules over a given
Ring (\S\ref{sec:ring}).

All Linear Transformations are \emph{Affine Transformations}
(\S\ref{sec:affine_transformation}), but not every Affine Transformation is
Linear; Affine Transformations are not required to preserve the Zero Point in a
Linear Space (FiXME: clarify)

\fist General Linear Group (\S\ref{sec:general_linear_group}): Group of $n
\times n$ Invertible Matrices

\fist Special Linear Group (\S\ref{sec:special_linear_group}): Group of $n
\times n$ Matrices with Determinant $1$; Normal Subgroup of the General Linear
Group

A \emph{Linear Operator} (\S\ref{sec:linear_operator}) is an Endomorphic Linear
Map (i.e. the Domain and Codomain are the same Module).

\fist Linear Differential Operators (\S\ref{sec:linear_differential_operator})

A Linear Map between Topological Vector Spaces
(\S\ref{sec:topological_vector}) can be Continuous
(\S\ref{sec:continuous}) and if the Domain and Codomain are the same
it is a \emph{Continuous Linear Operator} (\S\ref{sec:continuous_linear}).

A \emph{Linear Functional} (or \emph{Linear Form} \S\ref{sec:linear_form}) is a
Linear Map from a Vector Space $V$ to its Field of Scalars $K$ (viewed as a
Vector Space over itself).

Homothety (\S\ref{sec:homothety}): a specific kind of Linear Transformation of
an Affine Space called a \emph{Dilation} or \emph{Scale Transformation}; in
Projective Geometry (\S\ref{sec:projective_geometry}) it is a Similarity
Transformation (\S\ref{sec:simliarity_transformation}) that leaves the Line at
Infinity Pointwise Invariant

(Fong16):

%FIXME: are linear relations different from linear maps in general?

\emph{Linear Relation} $L : U \rightsquigarrow V$ is a Subspace $L
\subseteq U \oplus V$

Linear Relations as Corelations in $\cat{Vect}$; Category of Vector
Spaces and Linear Relations $\cat{LinRel}$



\subsubsection{Fundamental Subspace}\label{sec:fundamental_subspace}

\fist Fundamental Theorem of Linear Algebra
(\S\ref{sec:fundamental_linear_algebra_theorem})



\paragraph{Column Space}\label{sec:column_space}\hfill

\emph{Image} or \emph{Range}

Rank (\S\ref{sec:rank})

the Left Nullspace (\S\ref{sec:left_nullspace}) of a Matrix $A$ is the Subspace
that is Orthogonal to the Column Space of $A$

\begin{align*}
  \vec{x} & \in \mathrm{null}(A^*) \Leftrightarrow         \\
  \vec{x} & \bot \text{all Rows of }A^* \Leftrightarrow    \\
  \vec{x} & \bot \text{all Columns of }A \Leftrightarrow   \\
  \vec{x} & \in \mathrm{image}(A)^\bot
\end{align*}



\paragraph{Row Space}\label{sec:row_space}\hfill

\emph{Coimage}

Rank (\S\ref{sec:rank})

the Nullspace (\S\ref{sec:nullspace}) of a Matrix $A$ is the Subspace that is
orthogonal to the Row Space of $A$

\begin{align*}
  \vec{x} & \in \mathrm{null}(A) \Leftrightarrow         \\
  \vec{x} & \bot \text{all Rows of }A \Leftrightarrow    \\
  \vec{x} & \bot \text{all Columns of }A^* \Leftrightarrow   \\
  \vec{x} & \in \mathrm{coimage}(A)^\bot
\end{align*}



\paragraph{Nullspace}\label{sec:nullspace}\hfill

The \emph{Nullspace} of a Linear Map $L : V \rightarrow W$ between Vector
Spaces $V$ and $W$ is the Set of all $\vec{v}\in{V}$ such that $L(\vec{v}) =
\vec{0}$ where $\vec{0}$ is the Zero Vector in $W$:
\[
  \text{ker}(L) = \{\vec{v}\in{V} \;|\; L(\vec{v}) = \vec{0}\}
\]

The Kernel of $L$ is a Linear Subspace of the Domain $V$.

Nullity (\S\ref{sec:nullity}): Dimension of the Null Space

\emph{Rank-Nullity Theorem}

for an $m \times n$ Matrix $A$, the Rank of $A$ plus the Nullity
of $A$ is equal to $n$



\paragraph{Left Nullspace}\label{sec:left_nullspace}\hfill

or \emph{Cokernel}

Corank (\S\ref{sec:rank})

the Left Nullspace of a Matrix $A$ is the Subspace that is Orthogonal to the
Column Space (\S\ref{sec:column_space}) of $A$

\begin{align*}
  \vec{x} & \in \mathrm{null}(A^*) \Leftrightarrow         \\
  \vec{x} & \bot \text{all Rows of }A^* \Leftrightarrow    \\
  \vec{x} & \bot \text{all Columns of }A \Leftrightarrow   \\
  \vec{x} & \in \mathrm{image}(A^*)^\bot
\end{align*}



\subsubsection{Linear Operator}\label{sec:linear_operator}

A \emph{Linear Operator} is an Endomorphic Linear Map, i.e. a Linear
Map from a Module to itself $V \rightarrow V$

there is a correspondence between $n\times{n}$ Square Matrices
(\S\ref{sec:square_matrix}) and Linear Operators on an $n$-dimensional Vector
Space

\begin{itemize}
  \item the Integral (\S\ref{sec:integral}) is a Linear Operator (FIXME:
    explain)
  \item every Integral Transform (\S\ref{sec:integral_transform}) is a Linear
    Operator, and if the Kernel is allowed to be a Generalized Function
    (\S\ref{sec:generalized_function}), then all Linear Operators are Integral
    Transforms (\emph{Schwartz Kernel Theorem})
  \item the Laplace Transform (\S\ref{sec:laplace_transform})
\end{itemize}



\paragraph{Bounded Linear Operator}\label{sec:bounded_linear_operator}

a Linear Transformation between two Normed Vector Spaces
(\S\ref{sec:normed_vectorspace}) is a Bounded Linear Operator if and
only if it is a Continuous Linear Operator
(\S\ref{sec:continuous_linear})

an Orthogonal Projection (\S\ref{sec:orthogonal_projection}) is a Bounded
Linear Operator

\begin{itemize}
  \item Hermitian Adjoint (Adjoint Operator \S\ref{sec:adjoint_operator})
\end{itemize}



\paragraph{Linear Projection}\label{sec:projection}\hfill

A \emph{Linear Projection} $P$ is an Idempotent Linear Operator, i.e. $P^2 = P$.

Singular Matrix (\S\ref{sec:singular_matrix}) FIXME

the Columns of $I - P$ Span $\mathrm{null}(P)$

$P(I - P) = 0$

$(I - P)$ is also a Projection

$P$ gives a ``Decomposition'' of a Vector $\vec{x}$ into Components in the
Range and Nullspace of $P$:
\[
  \vec{x} = P\vec{x} = (I - P)\vec{x}
\]

$P$ is an Orthogonal Projection (\S\ref{sec:orthogonal_projection}) if
$P\vec{x}$ and $(I-P)\vec{x}$ are Orthogonal



\subparagraph{Orthogonal Projection}\label{sec:orthogonal_projection}\hfill

in an Orthogonal Projection, the Direction of the Projection is Orthogonal to
the Image (Column Space) of the Projection

$P$ is an Orthogonal Projection if $P\vec{x}$ and $(I-P)\vec{x}$ are Orthogonal

Thm. \emph{A Matrix $P \in \comps^{n \times n}$ is an Orthogonal Projection if
  and only if $P^2 = P$ and $P = P^*$}

i.e. an Orthogonal Projection is exactly an Idempotent, Symmetric (Hermitian)
Linear Operator

for an $n$-dimensional Unit Vector $\hat{u}$, the $n \times n$ Orthogonal
Projection Matrix $P_u$ can be computed by the Outer Product:
\[
  P_u = \hat{u}\hat{u}^T
\]
(if $\hat{u}$ is Complex the Transpose is the Hermitian Transpose) which
Projects any given Vector onto the Line defined by $\hat{u}$

(wiki):

An \emph{Orthogonal Projection} on a Hilbert Space (\S\ref{sec:hilbert_space}),
i.e. a Complete Vector Space with an Inner Product,
$W$ is a Projection for which the Range (Column Space \S\ref{sec:column_space})
$U$ and the Nullspace (\S\ref{sec:nullspace}) $V$ are \emph{Orthogonal
  Subspaces} (\S\ref{sec:orthogonal_subspace}), i.e. every Vector in $U$ is
Orthogonal (\S\ref{sec:orthogonality}) to every Vector in $V$:
\[
  Px \bot y - Py
\]

A Projection is Orthogonal if and only if it is Self-adjoint
(\S\ref{sec:self_adjoint_operator}).

an Orthogonal Projection is a Bounded Linear Operator
(\S\ref{sec:bounded_linear_operator})


UC Math 352 \url{https://www.youtube.com/watch?v=3HS-BRbJOd0}:

\emph{Orthogonal Projection onto an arbitrary Basis}

for Set of Linearly Independent Vectors $\vec{a}_1, \ldots, \vec{a}_n$

Orthogonal Projection onto $\mathrm{span}\{\vec{a}_1, \ldots, \vec{a}_n\}$

Define $A = [\vec{a}_1 \cdots \vec{a}_n] \in \comps^{m \times n}, m \geq n$

($A$ is Full Rank because the Column Vectors are Linearly Independnt)

from the Reduced QR Decomposition (\S\ref{sec:qr_decomposition}):
\[
  A = \hat{Q}\hat{R}
\]
one can Define the Orthogonal Projection $P$:
\[
  P = \hat{Q}\hat{Q}^*
\]


\url{https://www.youtube.com/watch?v=ZWGIchXVbho}:

Orthogonal Projection onto the Column Space of $A$:
\[
  P = A(A^TA)^{-1}A^T
\]



\paragraph{Continuous Linear Operator}\label{sec:continuous_linear}\hfill

A \emph{Continuous Linear Operator} is a Continuous
(\S\ref{sec:continuous_map}) Linear Operator between Topological
Vector Spaces (\S\ref{sec:topological_vector})

a C$^*$-algebra (\S\ref{sec:cstar_algebra}) is a Complex Algebra $A$
of Continuous Linear Operators on a Complex Hilbert Space
(\S\ref{sec:hilbert_space}) with the additional Properties that $A$ is
Topologically Closed in the Norm Topology of Operators and Closed
under the Operation of taking Adjoints of Operators

(wiki):

a Linear Operator on a Normed Vector Space
(\S\ref{sec:normed_vectorspace}) is Continuous if and only if it is a
Bounded Linear Operator (\S\ref{sec:bounded_linear_operator}), e.g.
when the Domain is Finite-dimensional: every Linear Operator on a
Finite-dimensional Space is Continuous



\subsubsection{Linear Form}\label{sec:linear_form}

A \emph{Linear Form} (or \emph{Linear Functional} or \emph{Covector}) is a
Linear Map from a Vector Space to its Field of Scalars viewed as a Vector Space
over itself.

For a Vector Space $V$ over a Field $k$, a Linear Functional $f$ is a Function
from $V$ to $k$ that is \emph{Linear}:
\begin{itemize}
\item $f(\vec{v} + \vec{w}) = f(\vec{v}) + f(\vec{w})$ for all $\vec{v},
  \vec{w} \in V$
\item $f(a\vec{v} = af(\vec{v}$ for all $\vec{v} \in V, a \in k$
\end{itemize}

The action of a Linear Functional on a Vector is given by the Dot Product
(\S\ref{sec:inner_product}), or the Matrix Product with a Row Vector on the
Left and a Column Vector on the right.

The \emph{(Algebraic) Dual Space} (\S\ref{sec:dual_space}) of a Vector Space
$V$ is the Set of all Linear Forms on $V$ together with the Vector Space
structure of Pointwise Addition and Multiplication by Constants.

A \emph{Bilinear Form} (\S\ref{sec:bilinear_form}) is a Bilinear Map $V \times
V \rightarrow K$ (i.e. Linear in each argument separately).

Distribution (\S\ref{sec:distribution})



\subsubsection{Semilinear Map}\label{sec:semilinear_map}

(or \emph{Semilinear Transformation})

generalizes the class of Antilinear Maps (\S\ref{sec:antilinear_map})



\paragraph{Antilinear Map}\label{sec:antilinear_map}\hfill

or \emph{Conjugate-linear Map}

is a Mapping $f : V \rightarrow W$ between Complex Vector Spaces $V$ and $W$
when:
\[
  f (ax+by) = \bar{a}f(x) + \bar{b}f(y)
\]
for all Complex Numbers $a,b \in \comps$ with Complex Conjugates $\bar{a},
\bar{b}$ and all $x,y \in V$

may be equivalently described in terms of the Linear Map $\bar{f} : V
\rightarrow \bar{W}$ from $V$ to the Complex Conjugate Vector Space $\bar{W}$

A Sesquilinear Form (\S\ref{sec:sesquilinear_form}) is a Bilinear Form where
the Codomain $K$ is the Field of Complex Numbers and the mapping is Antilinear
in one argument.


Quantum mechanics: Time Reversal

Spinor Calculus

\fist Antiunitary Operator (\S\ref{sec:antiunitary_operator})



\subsubsection{Short Linear Map}\label{sec:short_linear}

$\cat{Hilb}$



\subsubsection{Eigenvector}\label{sec:eigenvector}

An \emph{Eigenvector} or \emph{Characteristic Vector} of a Linear
Transformation is a Non-zero Vector whose Direction does not change when the
Linear Transformation is applied to it:
\[
  A\vec{v} = \lambda\vec{v}
\]
where $\lambda$ is the \emph{Eigenvalue} corresponding to $\vec{v}$.

An \emph{Eigendecomposition} (\S\ref{sec:eigendecomposition}) of a Matrix is a
Factorization of the Matrix in terms of its Eigenvalues and Eigenvectors and
only Diagonlizable Matrices (\S\ref{sec:diagonalizable_matrix}) can be Factored
in this way.

\fist Linear Dynamical Systems (\S\ref{sec:linear_dynamical_system}):
$\dot{\vec{p}} = A\vec{p}$ always has a Fixed Point at the Origin with Phase
Portrait (\S\ref{sec:phase_plane}) around the Origin given by the Eigenvectors
and Eigenvalues of $A$



\paragraph{Eigenvalue}\label{sec:eigenvalue}\hfill

The \emph{Eigenvalue} (or \emph{Characteristic Value} or \emph{Characteristic
  Root}) of an Eigenvector $\vec{v}$ with respect to a Linear Operator $T$, is a
Scalar $\lambda$ such that:
\[
  T(\vec{v}) = \lambda\vec{v}
\]



% --------------------------------------------------------------------
\subsection{Locally Linear Transformation}\label{sec:locally_linear}
% --------------------------------------------------------------------

%FIXME: move this section ?

(FIXME: what is the criterea for a transformation betwen vector spaces to be
``locally linear'')

\fist cf. Local Linearization (\S\ref{sec:local_linearization})



\subsubsection{Jacobian}\label{sec:jacobian}

%FIXME: merge with jacobian matrix ?

Jacobian Matrix (\S\ref{sec:jacobian_matrix})

\fist the Jacobian Matrix for a System of DAEs (\S\ref{sec:system_of_daes}) is
a Singular (Non-invertible) Matrix (\S\ref{sec:singular_matrix})



% --------------------------------------------------------------------
\subsection{Additive Map}\label{sec:additive_map}
% --------------------------------------------------------------------

$f(x + y) = f(x) + f(y)$

Special case of Subadditive Function (\S\ref{sec:subadditive_function})

Norm (\S\ref{sec:norm})



% --------------------------------------------------------------------
\subsection{Free Module}\label{sec:free_module}
% --------------------------------------------------------------------

A \emph{Free Module} is a Freely Generated (\S\ref{sec:free_object})
Module over a given Basis (\S\ref{sec:basis}).

Free Vector Space



\subsubsection{Group Ring}\label{sec:group_ring}



% --------------------------------------------------------------------
\subsection{Bimodule}\label{sec:bimodule}
% --------------------------------------------------------------------

Abelian Group (\S\ref{sec:abelian_group}) that is both a Left and a
Right Module such that the Left and Right Multiplications are
compatible.

Rings $R$, $S$, $R$-$S$-bimodule is an Abelian Group $M$ such that:

\begin{enumerate}
\item $M$ is a Left $R$-module and a Right $S$-module
\item $\forall r \in R, s \in S, m \in M, (rm)s = r(ms)$
\end{enumerate}

An $R$-$R$-bimodule is known as an $R$-bimodule

Generalization of Algebra Homomorphism
(\S\ref{sec:algebra_homomorphism})

Categorical generalization: Profunctor (\S\ref{sec:profunctor})



% ====================================================================
\section{Vector Space}\label{sec:vector_space}
% ====================================================================

\emph{Vector} (\S\ref{sec:vector}); a \emph{Position Vector} is a Vector based
at the Origin; a \emph{Free Vector} is equivalent to a Position Vector of the
same Direction and Magnitude

\emph{Span}

\emph{Finite-dimensional Vector Space} -- has a Span; all
Finite-dimensional Vector Spaces are Nuclear Spaces
(\S\ref{sec:nuclear_space})

\emph{Infinite Dimensional Vector Space} -- does not have a Span

\emph{Linear Independence}

\emph{Basis} (\S\ref{sec:basis}) - Spans and is Linearly Independent

Field (\S\ref{sec:field})

A Cartesian Space (\S\ref{sec:cartesian_space}) $\reals^n$ is canonically a
Vector Space over the Field of Real Numbers.

every Vector Space is a \emph{Free Vector Space}: every Vector Space
looks like a Function $X \rightarrow R$ where $R$ is a Field and $X$
some Set of Basis Vectors, i.e. Vectors can be decomposed into a sum
of Scalars times the choice of Basis Vectors %FIXME

All Bases of a Vector Space $\mathbf{V}$ have the same number of
Elements equal to the \emph{Dimension} of $\mathbf{V}$,
$dim(\mathbf{V})$. The Dimension of a Vector Space is uniquely defined
because for any Vector Space, a Basis exists, and all Bases of a
Vector space have equal Cardinality (\S\ref{sec:cardinality}).

For a Finite Dimensional Vector Space a Subset of a Span defines a
Basis, and a Linearly Independent Subset can be extended to form a
Basis.

The number of Elements in a Spanning Subset of $\mathbf{V}$ is greater
than or equal to the Dimension of $\mathbf{V}$.

The number of Elements in a Linearly Independent Subset of
$\mathbf{V}$ is less than or equal to the Dimension of $\mathbf{V}$.

A Basis defines an Isomorphism of Vector Spaces:
\[
    \mathbf{V} \xrightarrow{f} F^n
\]

\emph{Tensor Product}, \emph{Outer Product} (\S\ref{sec:outer_product})

Scalar (0th-order Tensor \S\ref{sec:scalar}) is an Element of the
Field used to define a Vector Space

Vector (1st-order Tensor \S\ref{sec:vector}) is an Element of a Vector
Space

Functional Analysis: Sequence Space (\S\ref{sec:sequence_space})

a Euclidean Vector Space with the Group Operation of Vector Addition is an
example of a Non-compact (\S\ref{sec:compact_space}) Lie Group
(\S\ref{sec:lie_group})



% --------------------------------------------------------------------
\subsection{Linear Combination}\label{sec:linear_combination}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Fundamental Theorem of Linear Algebra}
\label{sec:fundamental_linear_algebra_theorem}
% --------------------------------------------------------------------

1993 - Strang - \emph{The Fundamental Theorem of Linear Algebra}

An $m \times n$ Matrix $A$ of Rank (\S\ref{sec:rank}) $r$ with Singular
Value Decomposition (\S\ref{sec:svd}):
\[
  A = U \Sigma V^*
\]
induces \emph{Four Fundamental Subspaces} (\S\ref{sec:fundamental_subspace}):
\begin{itemize}
  \item \emph{Column Space} (\S\ref{sec:column_space}) -- the \emph{Image},
    $im(A)$, of Dimension $r$ (Rank \S\ref{sec:rank})
  \item \emph{Row Space} (\S\ref{sec:row_space}) -- the \emph{Coimage},
    $coim(A)$, of Dimension $r$ (Rank)
  \item \emph{Nullspace} (\S\ref{sec:nullspace}) -- the \emph{Kernel},
    $ker(A)$, of Dimension $n - r$ (Nullity \S\ref{sec:nullity})
  \item \emph{Left Nullspace} (\S\ref{sec:left_nullspace}) -- the
    \emph{Cokernel}, $coker(A)$, of Dimension $m - r$ (Corank
    \S\ref{sec:corank})
\end{itemize}
such that:
\begin{enumerate}
  \item $ker(A) = coim(A)^\bot$ -- the Nullspace is the Orthogonal Complement
    (\S\ref{sec:orthogonal_complement}) of the Row Space
  \item $coker(A) = im(A)^\bot$ -- the Left Nullspace is the Orthogonal
    Complement of the Column Space
\end{enumerate}



% --------------------------------------------------------------------
\subsection{Linear Span}\label{sec:linear_span}
% --------------------------------------------------------------------

The \emph{Linear Span} (or \emph{Linear Hull} or \emph{Span}) of a Set of
Vectors in a Vector Space is the Intersection of all Linear Subspaces
(\S\ref{sec:linear_subspace}) containing that Set.

A \emph{Basis} (\S\ref{sec:basis}) of a Vector Space is a Linearly Independent
Spanning Set.

The \emph{Rank} (\S\ref{sec:rank}) of a Matrix is the Dimension of the Vector
Space Spanned by its Columns, i.e. the maximal number of Linearly Independent
Columns of the Matrix.

generalized to Matroids (\S\ref{sec:matroid}) and Modules (\S\ref{sec:module})
%FIXME explain

Closed Linear Span (\S\ref{sec:closed_linear_span})



\subsubsection{Basis}\label{sec:basis}

A Set of Vectors in a Vector Space is a \emph{Basis} if the Vectors are
\emph{Linearly Independent} and therefore every Vector in the Vector Space is a
Linear Combination of the Set, that is, a \emph{Linearly Independent Spanning
  Set} (\S\ref{sec:linear_span}).

\fist a \emph{Free Object} (\S\ref{sec:free_object}) is a direct generalization
to Categories of the notion of Basis in a Vector Space
(FIXME: clarify)



\paragraph{Orthogonal Basis}\label{sec:orthogonal_basis}\hfill

\emph{Gram-Schmidt Process}: method for Orthogonalizing a Set of Linearly
Independent Vectors in an Inner Product Space



\paragraph{Orthonormal Basis}\label{sec:orthonormal_basis}\hfill

a Basis for an Inner Product Space consisting of an Orthonormal
(\S\ref{sec:orthonormal}) Set of Vectors

\begin{itemize}
  \item an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix}) is an $n \times n$
    Matrix with Columns forming an Orthonormal Basis for Euclidean Space
    $\reals^n$
  \item the Sines and Cosines in the Fourier Series
    (\S\ref{sec:fourier_series}) form an Orthonormal Basis
\end{itemize}



\paragraph{Covariant Transformation}\label{sec:covariant_transformation}\hfill

Coordinate-free (\S\ref{sec:coordinate_free})

Covariant Transformation Law

change of Basis

precise Transformation Law determines the Valence
(\S\ref{sec:valence}) of a Tensor (\S\ref{sec:linear_tensor})



% --------------------------------------------------------------------
\subsection{Linear Subspace}\label{sec:linear_subspace}
% --------------------------------------------------------------------

A \emph{Linear Subspace} (or \emph{Vector Subspace}) is a Subset of a Vector
Space that is Closed under Addition and Scalar Multiplication. For Field $K$
and $V$ a Vector Space over $K$, a Subset $W \subset K$ is a \emph{Subspace} of
$V$ if:
\begin{enumerate}
  \item $\vec{0} \in W$ -- the Zero Vector is in $W$
  \item $\vec{u}, \vec{v} \in W \Rightarrow \vec{u} + \vec{v} \in W$
    -- $W$ is Closed under Vector Addition
  \item $c\vec{u} \in W \forall c \in K \vec{u} \in W$
    -- $W$ is Closed under Scalar Multiplication
\end{enumerate}

\fist Fundamental Subspaces (\S\ref{sec:fundamental_subspace})



\subsubsection{Orthogonal Subspace}\label{sec:orthogonal_subspace}

Two Linear Subspaces $A, B \subset V$ of an Inner Product Space $V$ are called
\emph{Orthogonal Subspaces} if each Vector in $A$ is Orthogonal to each Vector
in $B$.

The largest Subspace of $V$ that is Orthogonal to a given Subspace is its
Orthogonal Complement (\S\ref{sec:orthogonal_complement}).

An Orthogonal Projection (\S\ref{sec:orthogonal_projection}) on a Hilbert Space
(\S\ref{sec:hilbert_space}) $W$ is a Projection for which the Range (Column
Space \S\ref{sec:column_space}) $U$ and the Nullspace (\S\ref{sec:nullspace})
$V$ are Orthogonal Subspaces.



\paragraph{Orthogonal Complement}\label{sec:orthogonal_complement}\hfill

The \emph{Orthogonal Complement} of a Linear Subspace $W \subset V$ with a
Bilinear Form (\S\ref{sec:bilinear_form}) $B$ is the Set $W^\bot$ of all
Vectors in $V$ that are Orthogonal to every Vector in $W$ and is itself a
Linear Subspace of $V$. The Orthogonal Complement of $W$ is the largest
Subspace of $V$ that is Orthogonal to $W$.



% --------------------------------------------------------------------
\subsection{Scalar Field}\label{sec:scalar_field}
% --------------------------------------------------------------------

%FIXME: move scalar, vector, tensor fields to vector calculus?

Gradient (\S\ref{sec:gradient}): the Vector Derivative of a Scalar Field

cf. Vector Field (\S\ref{sec:vector_field})

cf. Tensor Field (\S\ref{sec:tensor_field})

Line Integral of Parametric Curves (\S\ref{sec:parametric_curve}) in a Scalar
Field; unlike Line Integrals of Curves in Vector Fields, the Line Integral in a
Scalar Field is \emph{independent} of Path direction

when a Vector Field is the \emph{Gradient Field} (\S\ref{sec:gradient}) of some
Scalar Field, it is a \emph{Conservative} (\emph{Path Independent}
\S\ref{sec:conservative_vector_field}) Vector Field and the Line Integral of
any Path between two Points is equivalent and the Line Integral of any Closed
Path from a Point to itself is Zero



\subsubsection{Scalar Function}\label{sec:scalar_function}



% --------------------------------------------------------------------
\subsection{Vector Field}\label{sec:vector_field}
% --------------------------------------------------------------------

a Vector Field on a Manifold $M$ is a Smooth Map $V : M \rightarrow T M$ where
$TM$ is the Tangent Bundle (\S\ref{sec:tangent_bundle}) of $M$, i.e. a Cross
Section (\S\ref{sec:cross_section}) of $TM$

the Set of all Vector Fields on $M$ is denoted $\Gamma(TM)$ and has the
structure of a Module over the Commutative Algebra of Smooth Functions on $M$,
$C^\infty(M)$

\emph{Canonical Vector Field} (Liouville Vector Field
\S\ref{sec:liouville_vector_field}) on $TM$:
\[
  V : TM \rightarrow TTM
\]
as the Diagonal Map on the Tangent Space at each Point

FIXME: does a Vector Field require that the Domain and Range have the same
Dimension ???

\begin{equation*}
  \vec{v}(x,y,z) = \begin{bmatrix}
    p(x,y,z) \\
    q(x,y,z) \\
    r(x,y,z)
  \end{bmatrix}
\end{equation*}

Partial Derivative (\S\ref{sec:partial_derivative}) ... TODO

Vector Field Line Integral (\S\ref{sec:line_integral}) of a Parametric Curve
(\S\ref{sec:parametric_curve}) is a Sum of the Dot Products of the Derivative
of the Path Function $C = \vec{r}(t)$ with the Vectors in the Vector Field
$\vec{f}(x,y)$:
\[
  \int_C \vec{f} \bullet d\vec{r}
\]
represents the amount of ``Work'' done on a Path in the Field (Physics)
that unlike Line Integrals of Curves in a Scalar Field
(\S\ref{sec:scalar_field}), the Line Integral in a Vector Field is
\emph{dependent} on Path direction (it is Negative in the opposite direction)

a Vector Field is called \emph{Path Independent} (or \emph{Conservative}
\S\ref{sec:conservative_vector_field}) if it is equal to the Gradient Field
(\S\ref{sec:gradient}) of a Scalar Field; if so the Line Integral of all Paths
between Points are equal and the Line Integral from a Point to itself (Closed
Path) is Zero

wiki:

Divergence (\S\ref{sec:divergence}) of a Vector Field: Scalar-valued Function
(\S\ref{sec:scalar_function}) associating a Scalar Value with each Point in
the Vector Field

Curl (\S\ref{sec:curl}) of a Vector Field: Vector Function
(\S\ref{sec:vector_function}) associating each Point in the Vector field with
the proportional ``on-axis'' Torque to which a ``tiny pinwheel'' would be
subjected if it were centered at the Point

a Vector Field attaches to every Point of a Manifold (\S\ref{sec:manifold}) a
Vector from the Tangent Space (\S\ref{sec:tangent_space}) at that Point in a
\emph{Smooth} (\S\ref{sec:smooth_function}) manner and such a Vector Field
defines a generalized Ordinary Differential Equation (ODE \S\ref{sec:ode}) on a
Manifold where a Solution to such an Equation is a Diefferentiable Curve on the
Manifold with Derivative at any Point equal to the Tangent Vector attached to
that Point by the Vector Field

cf. Scalar Field (\S\ref{sec:scalar_field})

cf. Tensor Field (\S\ref{sec:tensor_field})

Autonomous Vector Field (???)



\subsubsection{Vector Flow}\label{sec:vector_flow}

Flow (\S\ref{sec:flow})

cf. Gradient Flow (\S\ref{sec:gradient_flow})

cf. Geometric Flow (\S\ref{sec:geometric_flow})



\paragraph{Flux}\label{sec:flux}\hfill

Flux as Flow Rate per Arclength \fist 2D Divergence Theorem (Green's Theorem
\S\ref{sec:greens_theorem})

Flux as Flow Rate per Unit Area \fist Surface Integral
(\S\ref{sec:surface_integral})

Flux Densities \fist (\S\ref{sec:volume_integral})

Stokes' Theorem (\S\ref{sec:stokes_theorem}), 3D Divergence Theorem



\subsubsection{Conservative Vector Field}\label{sec:conservative_vector_field}

a Vector Field is called \emph{Conservative} (or \emph{Path Independent}) if
the Line Integral (\S\ref{sec:line_integral}) of all Paths between Points are
equal and the Line Integral of a Closed Path (from a Point to itself) is Zero

this is true when the Vector Field is the \emph{Gradient Field}
(\S\ref{sec:gradient}) of some Scalar Field

\fist Green's Theorem (\S\ref{sec:greens_theorem})



% --------------------------------------------------------------------
\subsection{Covector Field}\label{sec:covector_field}
% --------------------------------------------------------------------

Cross Section of the Cotangent Bundle (FIxME: xref)



% --------------------------------------------------------------------
\subsection{Normed Vector Space}\label{sec:normed_vectorspace}
% --------------------------------------------------------------------

A \emph{Normed Vector Space} is a Vector Space $V$ over the Real or Complex
Numbers on which a \emph{Norm} (\S\ref{sec:norm}) $\|\cdot\| : V \rightarrow
\reals^{0\leq}$ which is a Real-valued Function defined on the Vector Space
formalizing the notion of \emph{Distance} between Vectors.

Any Normed Vector Space is a Metric Space (\S\ref{sec:metric_space}) by
defining the Metric (Distance Function \S\ref{sec:metric}) $d(x,y) = \|y-x\|$.

A Complete (\S\ref{sec:complete_metric_space}) Normed Vector Space is a Banach
Space (\S\ref{sec:banach_space}).

Any Inner Product Space (\S\ref{sec:innerproduct_space}) with Inner Product
$\langle{\cdot,\cdot}\rangle$ induces a Norm by:
\[
  \|\vec{v}\| = \sqrt{\langle{\vec{v},\vec{v}}\rangle}
\]
A Complete Inner Product Space is a Hilbert Space (\S\ref{sec:hilbert_space}).

Bounded Linear Operator (\S\ref{sec:bounded_linear_operator})

Functional Analysis (\S\ref{sec:functional_analysis})



\subsubsection{Norm}\label{sec:norm}

The \emph{Norm} $\|\cdot\| : V \rightarrow \reals$ of a Normed Vector Space is
a Real-valued Function defined on the Vector Space with the properties:
\begin{enumerate}
  \item $0 \leq \|\vec{v}\|$ and
    $\|\vec{v}\| = 0 \Leftrightarrow \vec{v} = \vec{0}$
  \item $\|c\vec{v}\| = |c|\|\vec{v}\|$
  \item $\|\vec{u} + \vec{v}\| \leq \|\vec{u}\| + \|\vec{v}\|$ (\emph{Triangle
    Inequality})
\end{enumerate}

\fist Matrix Norm (\S\ref{sec:matrix_norm})

the Vector Derivative (\S\ref{sec:vector_derivative}) represents a change of
\emph{Length} of the Vector

\emph{Seminorm}

\emph{Quasinorm}

Subadditive Function (\S\ref{sec:subadditive_function})



\paragraph{Triangle Inequality}\label{sec:triangle_inequality}\hfill

Real Line (\S\ref{sec:real_line}) with Absolute Value as Norm:\\
$|x + y| \leq |x| + |y|$



\paragraph{$p$-norm}\label{sec:}\hfill

\fist Metric (Distance Function \S\ref{sec:metric})

\[
  \|\vec{x}\|_p = \Big(\sum_{i=1}^m|x_i|^p\Big)^{\frac{1}{p}}
\]

$1$-norm -- Taxicab Norm; all components are ``weighted'' equally

$2$-norm -- \emph{Euclidean Norm}; large elements have ``more weighting''

$\infty$-norm -- Max Norm; only the largest element contributes to the length



\subsubsection{Inner Product Space}\label{sec:innerproduct_space}

An \emph{Inner Product Space} $V$ is a Normed Vector Space with Norm
(\S\ref{sec:norm}) $\|\cdot\|$ induced by the \emph{Inner Product}
(\S\ref{sec:inner_product}) $\langle{\cdot,cdot}\rangle : V \times V
\rightarrow \reals$ which is a Sesquilinear Form
(\S\ref{sec:sesquilinear_form}).

An Inner Product on a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}).

\fist Orthogonality (\S\ref{sec:orthogonality})

generalizes case of Euclidean Space (\S\ref{sec:euclidean_space}) to
Vector Spaces of any, possibly Infinite, Dimension

Functional Analysis (\S\ref{sec:functional_analysis})



\paragraph{Inner Product}\label{sec:inner_product}\hfill

associates each pair of Vectors in a Vector Space with a Scalar known
as the \emph{Inner Product} of the Vectors

two Vectors in an Inner Product Space are \emph{Orthogonal} when the Inner
Product is Zero, denoted $\vec{u} \bot \vec{y}$

in Euclidean Spaces the Inner Product is the Dot Product (aka Scalar Product)

An Inner Product induces a Norm (\S\ref{sec:norm}) $\|\cdot\|$ by:
\[
  \|\vec{v}\| = \sqrt\langle{\vec{v},\vec{v}}\rangle
\]

an Inner Product on a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}) and a Positive-definite Hermitian Form
(\S\ref{sec:hermitian_form})

A \emph{Hilbert Space} (\S\ref{sec:hilbert_space}) is an Inner Product Space
that is also a Complete Metric Space (\S\ref{sec:complete_metric_space}).

\fist Sliding Inner-product (Cross-correlation \S\ref{sec:cross_correlation})

\fist Gram Matrix (\S\ref{sec:gram_matrix}): Hermitian Matrix of Inner
Products

\emph{Cauchy-Schwarz Inequality}:
\[
  |\langle{\vec{u},\vec{v}}\rangle|^2 \leq
    \langle{\vec{u},\vec{u}}\rangle \cdot \langle{\vec{v},\vec{v}}\rangle
\]
or equivalently:
\[
  |\langle{\vec{u},\vec{v}}\rangle| \leq \|\vec{u}\| \|\vec{v}\|
\]



\paragraph{Orthonormality}\label{sec:orthonormality}\hfill

two Vectors in an Inner Product Space are \emph{Orthonormal} if they are
Orthogonal and Unit Vectors

\fist Orthogonality (\S\ref{sec:orthogonality})

\fist Orthonormal Basis (\S\ref{sec:orthonormal_basis})

an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix}) is a Real-valued Square
Matrix with Columns and Rows that are Orthonormal Vectors



\paragraph{Unitary Space}\label{sec:unitary_space}\hfill

Inner Product Space over the Field of Complex Numbers



\paragraph{Hilbert Space}\label{sec:hilbert_space}\hfill

%FIXME: move to analysis ?

A \emph{Hilbert Space} is an Inner Product Space (\S\ref{sec:inner_product})
that is also a Complete Metric Space (\S\ref{sec:complete_metric_space}).

Functional Analysis (\S\ref{sec:functional_analysis}): can be defined
as a Banach Space (\S\ref{sec:banach_space})

Topological Vector Space (\S\ref{sec:topological_vector})

Hermitian Adjoint (Adjoint Operator \S\ref{sec:adjoint_operator})

$\cat{Hilb}$

$\cat{FdHilb}$

An Orthogonal Projection (\S\ref{sec:orthogonal_projection}) on a Hilbert Space
$W$ is a Projection for which the Range (Column Space \S\ref{sec:column_space})
$U$ and the Nullspace (\S\ref{sec:nullspace}) $V$ are Orthogonal Subspaces
(\S\ref{sec:orthogonal_subspace}).



\subparagraph{Hilbert Tensor Product}\label{sec:hilbert_tensor}\hfill

Topological Tensor Product (\S\ref{sec:topological_tensor})



\subparagraph{Antiunitary Operator}\label{sec:antiunitary_operator}\hfill

a Bijective Antilinear Map (\S\ref{sec:antlinear_map}) $U$ between Complex
Hilbert Spaces $H_1$ and $H_2$:
\[
  U : H_1 \rightarrow H_2
\]
such that:
\[
  \langle{Ux,Uy}\rangle = \overline{\langle{x,y}\rangle}
\]

Quantum Theory: Time-reversal Symmetry (Wigner's Theorem, Ray Space) %FIXME



\textbf{Hilbert-Schmidt Mapping}

\emph{Weakly Hilbert-Schmidt Mapping}:
\[
  L : H_1 \otimes H_2 \rightarrow H
\]
is a Bilinear Map for which a $d$ exists such that
$\sum_{i,j=1}^\infty | \langle L(e_i,f_j), u \rangle |^2 \leq d^2
||u||^2$ for all $u \in K$ and one (implies all) Orthonormal Basis
$e_1, e_2, \ldots$ of $H_1$ and $f_1, f_2, \ldots$ of $H_2$

% FIXME

1970 - Blute, Panangaden:

the Ideal of Hilbert-Schmidt Maps contained in the Category of Hilbert
Spaces is an example of a \emph{Nuclear Ideal}: an Ideal contained in
an ambient Monoidal Dagger Category with the structure of a Compact
Closed Category except it is lacking Identities



% --------------------------------------------------------------------
\subsection{Scalar Product Space}\label{sec:scalar_product_space}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Hermitian Product Space}\label{sec:hermitian_product_space}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Outer Product}\label{sec:outer_product}
% --------------------------------------------------------------------

Tensor Product (\S\ref{sec:tensor_product}) of two Vectors

\fist Orthogonal Projection (\S\ref{sec:orthogonal_projection})



% --------------------------------------------------------------------
\subsection{Real Vector Space}\label{sec:real_vector_space}
% --------------------------------------------------------------------

Vector Space over Scalar Field $F = \reals$

\fist \emph{Real Coordinate Space} (\S\ref{sec:real_coordinate_space})
$\reals^n$ -- prototypical Real Vector Space; Models Euclidean Space
(\S\ref{sec:euclidean_space}) with Cartesian Coordinates
(\S\ref{sec:cartesian_coordinates})



% --------------------------------------------------------------------
\subsection{Complex Vector Space}\label{sec:complex_vector_space}
% --------------------------------------------------------------------

Vector Space over Scalar Field $F = \comps$



% --------------------------------------------------------------------
\subsection{Symplectic Vector Space}\label{sec:symplectic_vectorspace}
% --------------------------------------------------------------------

Symplectic Geometry (\S\ref{sec:symplectic_geometry})

``Black-box Functor'': Category of Circuits $\rightarrow$ Category of
Symplectic Vector Spaces % FIXME Baez 15' Passive Linear Networks



% --------------------------------------------------------------------
\subsection{Finite-dimensional Vector Space}
\label{sec:finite_dimensional_vectorspace}
% --------------------------------------------------------------------

Finite-dimensional Vector spaces of Equal Dimension are Isomorphic
%FIXME



% --------------------------------------------------------------------
\subsection{Graded Vector Space}\label{sec:graded_vectorspace}
% --------------------------------------------------------------------

Graded Algebra (\S\ref{sec:graded_algebra})



% --------------------------------------------------------------------
\subsection{Dual Space}\label{sec:dual_space}
% --------------------------------------------------------------------

the \emph{Dual Space} (or \emph{Algebraic Dual Space}) of any Vector Space $V$
is the set of all Linear Functionals (Linear Forms \S\ref{sec:linear_form}) of
$V$ together with the Vector Space structure of Pointwise Addition and
Multiplication by Scalars

for a Topological Vector Space (\S\ref{sec:topological_vector_space}), there is
a Subspace of the Dual Space corresponding to Continuous Linear Functionals
called the \emph{Continuous Dual Space} (\S\ref{sec:continuous_dual_space})

$V \cong V^*$

$V \cong V^**$ ``Naturally'' (\S\ref{sec:natural_transformation})

Contravariant Representable Functor
(\S\ref{sec:representable_functor}):
\[
  (-)^* = Vect(-,\mathbb{R}) :
    \mathbf{Vect}^op \rightarrow \mathbf{Vect}
\]

\[
  A^* = \pow(A) \cong \mathbf{Set}(A,2)
\]\cite{awodey06}

(FIXME: does this actually relate to linear logic ???):

given a Vector Space $V$ over a Field $R$, the Dual Space $V^*$ is just the Set
of Linear Functions into $V \mapsto R$
--\url{https://cstheory.stackexchange.com/questions/39440/algebraic-account-of-gaussian-elimination}

the Dual Bundle (\S\ref{sec:dual_bundle}) of a Vector Bundle
(\S\ref{sec:vector_bundle}) is a Vector Bundle whose Fibers are the Dual Spaces
of the Fibers of the original Vector Bundle



% --------------------------------------------------------------------
\subsection{Vector Bundle}\label{sec:vector_bundle}
% --------------------------------------------------------------------

FIXME: move to topology ?

A \emph{Vector Bundle} is a Fiber Bundle (\S\ref{sec:fiber_bundle}) with Fibers
that are Vector Spaces.

The prototypical example of a Vector Bundle is the Tangent Bundle
(\S\ref{sec:tangent_bundle}) of Tangent Spaces (\S\ref{sec:tangent_space}) of a
Differentiable Manifold.

a Topological construction making precise the idea of a Family of Vector Space
parameterized by another Space $X$ (e.g. a Topological Space, Manifold, or
Algebraic Veriety) associating for every point $x \in X$ a Vector Space $V(x)$
such that all the Vector Spaces ``fit together'' to form another Space of the
same kind as $X$, called the \emph{Vector Bundle over $X$}

the Dual Bundle (\S\ref{sec:dual_bundle}) of the Tangent Bundle is the
a Cotangent Bundle (\S\ref{sec:cotangent_bundle}) of
Cotangent Spaces (\S\ref{sec:cotangent_space})



\subsubsection{Sub-bundle}\label{sec:subbundle}

\fist a Subbundle of the Tangent Bundle (\S\ref{sec:tangent_bundle}) of a
Smooth manifold is called a \emph{Distribution}
(\S\ref{sec:tangent_bundle_distribution}) of Tangent Vectors
(\S\ref{sec:tangent_space})

%FIXME: different frobenius theorem ?

\emph{Frobenius' Theorem} (\S\ref{sec:frobenius_theorem}): the Subbundle of the
Tangent Bundle of a Manifold is Integrable (or Involutive) if and only if it
arises from a \emph{Regular Foliation} (\S\ref{sec:foliation})



\subsubsection{Dual Bundle}\label{sec:dual_bundle}

the \emph{Dual Bundle} of a Vector Bundle is a Vector Bundle whose Fibers are
Dual Spaces (\S\ref{sec:dual_space}) of the Fibers of the Vector Bundle

the Dual Bundle of the Tangent Bundle (\S\ref{sec:tangent_bundle}) of a the
Tangent Spaces of Differentiable Manifold is the a Cotangent Bundle
(\S\ref{sec:cotangent_bundle}) of the Cotangent Spaces of the Manifold



\subsubsection{Complex Vector Bundle}\label{sec:complex_vector_bundle}



% ====================================================================
\section{$R$-algebra}\label{sec:r_algebra}
% ====================================================================

Algebra over a Commutative Unital Ring

Module (\S\ref{sec:module}) with a \emph{Bilinear Product}
(\S\ref{sec:bilinear_product})



% --------------------------------------------------------------------
\subsection{Bilinear Product}\label{sec:bilinear_product}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Algebra Homomorphism}\label{sec:algebra_homomorphism}
% --------------------------------------------------------------------

Generalized by Bimodules (\S\ref{sec:bimodule})



% --------------------------------------------------------------------
\subsection{Non-associative $R$-algebra}
\label{sec:nonassociative_r_algebra}
% --------------------------------------------------------------------

For Commutative Unital Ring $R$, $R$-module $V$ with Bilinear Product
(\S\ref{sec:bilinear_product}) $V \otimes V \rightarrow V$



% --------------------------------------------------------------------
\subsection{Associative $R$-algebra}\label{sec:associative_r_algebra}
% --------------------------------------------------------------------

\emph{Associative Unital $R$-algebra}, $R$-module $V$ with Bilinear
Product (\S\ref{sec:bilinear_product}) $p : V \otimes V \rightarrow V$
Linear Map $i : R \rightarrow V$ satisfying Associative and Unit Laws



% ====================================================================
\section{$K$-algebra}\label{sec:k_algebra}
% ====================================================================

\emph{Algebra over a Field}

Vector Space (\S\ref{sec:vector_space}) with a \emph{Bilinear Product}
(\S\ref{sec:bilinear_product})

i.e. a Vector Space that has a ``sensible way'' to defined Vector
Multiplication; e.g. $\comps$ (\S\ref{sec:complex_number}) is an
$\reals$-algebra

Coalgebra (\S\ref{sec:coalgebra})

$F$-algebra (\S\ref{sec:f_algebra})

Every Finite Dimensional Associative Division Algebra over $\reals$ is
Isomorphic to either $\reals$, $\comps$, or $\quats$ (Theorem of
Frobenius) %FIXME



% --------------------------------------------------------------------
\subsection{Unital Algebra}\label{sec:unital_algebra}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Zero Algebra}\label{sec:zero_algebra}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Associative Algebra}\label{sec:associative_algebra}
% --------------------------------------------------------------------

Coalgebra (\S\ref{sec:coalgebra}): Dual to a Unital Associative
Algebra

\fist Associative Division Algebras (\S\ref{sec:associative_division_algebra})



\subsubsection{Frobenius Algebra}\label{sec:frobenius_algebra}

Representation Theory (\S\ref{sec:representation_theory})

Module Theory (\S\ref{sec:module})

Frobenius Algebras can be defined in any Monoidal Category (or
Polycategory \S\ref{sec:polycategory}) and are sometimes called
\emph{Frobenius Monoids} (\S\ref{sec:frobenius_monoid})

Star-autonomous Categories (\S\ref{sec:star_autonomous}) are Pseudo-Frobenius
Algebras
--\url{https://golem.ph.utexas.edu/category/2017/11/starautonomous_categories_are.html}



% --------------------------------------------------------------------
\subsection{Non-associative Algebra}
\label{sec:nonassociative_algebra}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Division Algebra}\label{sec:division_algebra}
% --------------------------------------------------------------------

cf. Fields (\S\ref{sec:field}) which are Commutative



\subsubsection{Associative Division Algebra}
\label{sec:associative_division_algebra}

\paragraph{Frobenius Theorem}\label{sec:frobenius_theorem}\hfill

every Finite-dimensional Associative Division Algebra over the Real Numbers is
Isomorphic to one of:
\begin{enumerate}
  \item $\reals$ -- Commutative
  \item $\comps$ -- Commutative
  \item $\quats$ -- Non-commutative
\end{enumerate}

the only Division Algebra over $\comps$ is $\comps$ itself



\subsubsection{Composition Algebra}\label{sec:composition_algebra}

or \emph{Normed Division Algebra}



\paragraph{Hurwitz's Theorem}\label{sec:hurwitzs_theorem}\hfill

the only Normed Division Algebras over the Real Numbers are:
\begin{enumerate}
  \item $\reals$
  \item $\comps$
  \item $\quats$
  \item $\octs$
\end{enumerate}



% --------------------------------------------------------------------
\subsection{Coalgebra}\label{sec:coalgebra}
% --------------------------------------------------------------------

Dual to Unital Associative Algebra (\S\ref{sec:associative_algebra})

Cf. $F$-coalgebra (\S\ref{sec:f_coalgebra})



% --------------------------------------------------------------------
\subsection{Bialgebra}\label{sec:bialgebra}
% --------------------------------------------------------------------

Unital Associative Algebra and Coalgebra Vector Space over Field $K$



\subsubsection{Quasi-bialgebra}\label{sec:quasi_bialgebra}

\subsubsection{Hopf Algebra}\label{sec:hopf_algebra}

simultaneously a Unital Associative Algebra and a Counital
Coassociative Coalgebra with compatibility of these Structures making
it a Bialgebra, also equipped with an Anti-automorphism satisfying a
certain Property %FIXME what property?

Algebraic Topology



% ====================================================================
\section{Graded Algebra}\label{sec:graded_algebra}
% ====================================================================

Graded Vector Space (\S\ref{sec:graded_vectorspace})



% ====================================================================
\section{Exterior Algebra}\label{sec:exterior_algebra}
% ====================================================================

%FIXME multilinear algebra?

Grassmann Algebra (\S\ref{sec:grassmann_algebra})

\fist cf. Exterior Derivative (\S\ref{sec:exterior_derivative}) %FIXME

``Oriented Density'' %FIXME



% --------------------------------------------------------------------
\subsection{Bivector}\label{sec:bivector}
% --------------------------------------------------------------------

$2$-vector

Isomorphic to Skew-symmetric Matrices (\S\ref{sec:skew_symmetric})

Psuedovectors (Axial Vectors \ref{sec:pseudovector}) are equivalent to
Three-dimensional Bivectors



% --------------------------------------------------------------------
\subsection{Exterior Product}\label{sec:exterior_product}
% --------------------------------------------------------------------

(\emph{Wedge Product})

$\wedge$

\fist Exterior Derivative (\S\ref{sec:exterior_derivative})



% ====================================================================
\section{Matrix Theory}\label{sec:matrix_theory}
% ====================================================================

%FIXME: merge with matrix calculus ?

A \emph{Matrix} represents a \emph{Linear Map} (Linear
Transformation\S\ref{sec:linear_transformation}) between two Vector Spaces
(\S\ref{sec:vector_space}). Any Linear Map between Finite-dimensional Vector
Spaces $A : V \rightarrow W$ where $V$ has Dimension $n$ and $W$ has Dimension
$m$ can be represented as an $m \times n$ Matrix sending the Column Vector
$\vec{v} \in V$ to the Column Vector $A\vec{v} \in W$. Another way of viewing
this is that each of the components in the Vector $\vec{v}$ defines a
coefficient for the $n$th Column of $A$, and each of these scaled Column
Vectors are added together to produce the $n \times 1$ Vector in $W$, i.e.
Multiplication by $\vec{v}$ determines a \emph{Linear Combination} of the
Columns of $A$.

\fist Matrix (Second-order Tensor \S\ref{sec:matrix})

\fist Matrix Calculus (\S\ref{sec:matrix_calculus})

\fist Matrix Difference Equation (\S\ref{sec:matrix_difference_equation})

\fist System of Linear Equations (\S\ref{sec:linear_system})

\fist Fundamental Subspaces (\S\ref{sec:fundamental_subspace})

\emph{Column Vector} -- an $m \times 1$ Matrix

\emph{Row Vector} -- an $1 \times m$ Matrix

The Transpose of a Column Vector is a Row Vector, and the Transpose of a Row
Vector is a Column Vector.

The Set of all Row Vectors forms a Vector Space called the \emph{Row Space}
(\S\ref{sec:row_space}) and the Set of all Column Vectors forms a Vector Space
called the \emph{Column Space} (\S\ref{sec:column_space}).

FIXME: how do the 'row space' and 'column space' defined here relate to the
'row space' and 'column space' of a matrix ?

The Column Space can be viewed as the Dual Space (\S\ref{sec:dual_space}) to
the Row Space since any Linear Functional (Linear Form \S\ref{sec:linear_form})
on the Column Space can be represented uniquely as an Inner Product with a
specific Row Vector.

When Column and Row Vectors viewed as $m \times 1$ and $1 \times n$ Matrices,
respectively, a Row Vector times a Column Vector is the Dot Product
(Inner Product \S\ref{sec:inner_product}), resulting in a single Scalar value,
and a Column Vector times a Row Vector is a Dyadic Product
(\S\ref{sec:dyadic_product}), which results in an $m \times n$ Matrix

\asterism

For the One-dimensional Vector Space over the Reals $\reals$, a 1x1 Matrix is
just a Real Number. The kinds of Linear Transformations are:
\begin{itemize}
  \item Identity -- $A = 1$
  \item Zero -- $A = 0$
  \item Reflection -- $A = -1$
  \item Scaling -- $A > 0$
  \item Scaling + Reflection -- $A < 0$
\end{itemize}
all Non-zero $A$ are Invertible

In $\reals^2$, a 2x2 Matrix may additionally represent:
\begin{itemize}
  \item Shear -- Horizontal, Vertical; every Shear Matrix has an Inverse which
    is a Shear Matrix with the Shear Element negated
\end{itemize}



% --------------------------------------------------------------------
\subsection{Rank}\label{sec:rank}
% --------------------------------------------------------------------

The \emph{Rank} of a Matrix $A$ is the Dimension of the Vector Space
``generated'' or \emph{Spanned} (\S\ref{sec:linear_span}) by its Columns, that
is, equal to the maximal number of Linearly Independent Columns of $A$ and is
identical to the Dimension of the Space Spanned by its Rows.

\fist Rank of a Matrix can be computed by Gaussian Elimination
(\S\ref{sec:gaussian_elimination})

the Maximum Rank of an $m \times n$ Matrix is $\mathrm{min}(m,n)$

the Column Rank and the Row Rank are always equal

Dimension of the Column Space (\S\ref{sec:column_space}) and the Row Space
(\S\ref{sec:row_space})

a Matrix is \emph{Full Rank} if its Rank equals the largest possible for a
Matrix of the same Dimensions, which is the lesser of the number of Rows or
Columns; for a Square Matrix this is equivalent to the statement that the
Matrix is \emph{Invertible} (\S\ref{sec:invertible})

if a Matrix $A$ is Full Rank, then $A^*A$ is Invertible

every Full Rank $m \times n$ Matrix with $m \geq n$ has a unique QR
Factorization (\S\ref{sec:qr_decomposition}) with $r_{jj} > 0$

\emph{Rank-Nullity Theorem}

for an $m \times n$ Matrix $A$, the Rank of $A$ plus the Nullity of $A$ is
equal to $n$



% --------------------------------------------------------------------
\subsection{Nullity}\label{sec:nullity}
% --------------------------------------------------------------------

Dimension of the Nullspace (\S\ref{sec:nullspace})

\emph{Rank-Nullity Theorem}

for an $m \times n$ Matrix $A$, the Rank of $A$ plus the Nullity of $A$ is
equal to $n$



% --------------------------------------------------------------------
\subsection{Corank}\label{sec:corank}
% --------------------------------------------------------------------

Dimension of the Left Nullspace (\S\ref{sec:left_nullspace})



% --------------------------------------------------------------------
\subsection{Matrix Product}\label{sec:matrix_product}
% --------------------------------------------------------------------

\fist Tensor Product (\S\ref{sec:module_tensor_product})

The Multiplication of an $m \times n$ Matrix $A$ with entries $a_{xy}$
on the left by an $n \times p$ Matrix $B$ with entries $b_{wv}$ on the
right results in an $m \times p$ Matrix $C$ with the entry $c_{ij}$
defined as:
\[
  \sum_{k=1}^n a_{ik} b_{kj}
\]
The columns of the resulting Matrix $C$ are the result of Multiplying
the corresponding Column of $B$ on the left by the Matrix $A$. And the
rows of the resulting Matrix $C$ are the result of Multiplying the
corresponding row of $A$ on the right by the Matrix $B$.



% --------------------------------------------------------------------
\subsection{Matrix Square Root}\label{sec:matrix_square_root}
% --------------------------------------------------------------------

$BB = A$



% --------------------------------------------------------------------
\subsection{Transpose}\label{sec:transpose}
% --------------------------------------------------------------------

Conjugate Transpose (\S\ref{sec:conjugate_transpose}) for Complex-valued
Matrices

if a Matrix $A$ is Full Rank, then $A^TA$ is Invertible



% --------------------------------------------------------------------
\subsection{Reduced Row Echelon Form}\label{sec:reduced_row_echelon}
% --------------------------------------------------------------------

Coefficient Matrix (System of Linear Equations \S\ref{sec:coefficient_matrix})



% --------------------------------------------------------------------
\subsection{Matrix Norm}\label{sec:matrix_norm}
% --------------------------------------------------------------------

Vector Norm (\S\ref{sec:norm}) in a Vector Space in which Elements are Matrices
of a given Dimension

is a measure of how much a Matrix $A$ can \emph{Scale} Vectors

\emph{Induced Matrix Norm}

Vector Norm $\|\cdot\|_p$ and Matrix $A$, Induced Matrix Norm $\|A\|_p$, is the
length of the largest Vector that can result from multiplying $A$ by a Unit
Vector:
\[
  \|A\| = \mathrm{sup}_{\vec{x}} \frac{\|A\vec{x}\|}{\|\vec{x}\|}
\]
and therefore:
\[
  \|A\| \geq \frac{\|A\vec{x}\|}{\|\vec{x}\|}
\]
and:
\[
  \|A\vec{x}\| \leq \|A\| \|\vec{x}\|
\]

Induced $p$-norms (\S\ref{sec:p_norm})

the Induced $1$-norm (Taxicab Norm) of a Matrix is equal to the maximum Column
Sum (Sum of Absolute Value of individual Components) of $A$

the Induced $\infty$-norm (Max Norm) is the maximum Row Sum of $A$


\emph{Frobenius Norm} -- Square Root of the Sum of the Squares of all the
entries; equal to the Square Root of the Trace of $A^*A$


the Frobenius Norm and the Euclidean Norm ($2$-norm) are both invariant under
Unitary Multiplication, i.e. Left or Right Multiplying a Matrix $A$ by an
Unitary Matrix $Q$ results in a Matrix with the same Norm as the original
Matrix $A$

the Absolute Condition Number (\S\ref{sec:absolute_condition_number}),
$\hat{\kappa}$, of a Differentiable Function $f(x)$ is equal to the Matrix Norm
of its Jacobian Matrix (\S\ref{sec:jacobian_matrix}) $J$:
\[
  \hat{\kappa} = \|J(x)\|
\]

the Relative Condition Number (\S\ref{sec:relative_condition_number}),
$\kappa$, of a Differentiable Function $f(x)$ is equal to:
\[
  \kappa = \frac{\|J(x)\| \|x\|}{\|f(x)\|}
\]

the worst case $\kappa$ for the Matrix/Vector Multiplication Function defined
by a Matrix $A$ is the Sharp Inequality (i.e. is Equal for \emph{some}
$\vec{x}$):
\[
  \kappa \leq \|A\|\|A^{-1}\|
\]
where $\|A\|\|A^{-1}\|$ is called the Condition Number $\kappa(A)$ of Matrix
$A$



% --------------------------------------------------------------------
\subsection{Matrix Decomposition}\label{sec:matrix_decomposition}
% --------------------------------------------------------------------

Factorization of a Matrix into a Product of Matrices



\subsubsection{Eigendecomposition}\label{sec:eigendecomposition}

or \emph{Spectral Decomposition}

Factorization of a Matrix where the Matrix is represented in terms of its
Eigenvalues and Eigenvectors (\S\ref{sec:eigenvector})

only Diagonalizable Matrices (\S\ref{sec:diagonlizable_matrix}) can be
Factorized in this way



\subsubsection{LU Decomposition}\label{sec:lu_decomposition}

``Matrix form'' of Gaussian Elimination (Row Reduction
\S\ref{sec:gaussian_elimination})

cf. QR Factorization (\S\ref{sec:qr_decomposition})



\subsubsection{QR Decomposition}\label{sec:qr_decomposition}

(or \emph{QR Factorization})

\emph{Full QR Decomposition}: Decomposition of a Matrix $A \in \comps^{m \times
  n}, m \geq n$ into a Product $A = QR$ of an Orthogonal (Unitary) Matrix
(\S\ref{sec:orthogonal_matrix}) $Q$ (i.e. $Q^TQ = I$) and an Upper (Right)
Triangular Matrix (\S\ref{sec:upper_triangular}) $R$

this Factorization exists for all $A \in \comps^{m \times n}, m \geq n$, and is
\emph{unique} if $A$ is Full Rank (\S\ref{sec:rank}) when the Diagonal Elements
of $R$ are required to be Positive

Numerically Stable way to solve Least Squares Problem
(\S\ref{sec:linear_least_squares}); Overdetermined Systems
(\S\ref{sec:overdetermined_solution})

solving Eigenvalue Problems (QR Algorithm)


\url{https://www.youtube.com/watch?v=3HS-BRbJOd0}:

\emph{Reduced QR Factorization}

a Full Rank $m \times n, m \geq n$ Matrix $A$ with Columns:
\[
  A = [ \vec{a}_1, \ldots, \vec{a}_n ]
\]
can have its Column Space decomposed into nested Subspaces by taking the Spans
(\S\ref{sec:linear_span}) of the First $1$ to $n$ Columns:
\[
  \mathrm{span} \{ \vec{a}_1 \} \subseteq
  \mathrm{span} \{ \vec{a}_1, \vec{a}_2 \}
  \subseteq \cdots \subseteq
  \mathrm{span} \{ \vec{a}_1, \ldots, \vec{a}_n \}
  = \mathrm{span} A
\]

find Orthonormal Bases $\{ \hat{q}_1 \}, \{ \hat{q}_1, \hat{q}_2 \}, \ldots \{
\hat{q}_1, \ldots, \hat{q}_n\}$ for each of these Column Spaces, i.e.:
\[
  \mathrm{span} \{ \hat{q}_1, \hat{q}_2, \ldots, \hat{q}_j \} =
  \mathrm{span} \{ \vec{a}_1, \vec{a}_2, \ldots, \vec{a}_j \}
\]
which can be expressed as the Multiplication of the $m \times n$ Matrix
$\hat{Q} = [ \hat{q}_1 \cdots \hat{q}_n ]$ by an $n \times n$ Upper Triangular
Matrix, $A = \hat{Q}\hat{R}$:
\[
  [\vec{a}_1, \vec{a}_2, \ldots, \vec{a}_n] =
  [\hat{q}_1, \hat{q}_2, \ldots, \hat{q}_n]
  \begin{bmatrix}
    r_{11} & r_{12} & \cdots & r_{1n} \\
    0      & r_{22} & \cdots & r_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    0      & \cdots & 0      & r_{nn} \\
  \end{bmatrix}
\]
yielding the Equations:
\begin{align*}
  \vec{a}_1 & = r_{11}\hat{q}_1 \\
  \vec{a}_2 & = r_{12}\hat{q}_1 + r_{22}\hat{q}_2 \\
            & \vdots \\
  \vec{a}_n & = r_{1n}\hat{q}_1 + r_{2n}\hat{q}_2 + \cdots + r_{nn}\hat{q}_n \\
\end{align*}


\emph{Full QR Factorization}

\[
  A = QR = [\hat{Q}Q_N]\begin{bmatrix} \hat{R} \\ 0 \end{bmatrix}
\]
$Q$ is an Orthogonal (or Unitary) $m \times m$ Matrix formed by extending
$\hat{Q}$ by $m - n$ additional Orthonormal Columns and $R$ is an $m \times n$
Upper Triangular Matrix formed by extending $\hat{R}$ by $m - n$ additional
Rows of $1 \times m$ Zero Vectors

the Columns of $Q_N$ form an Orthonormal Basis for the Left Nullspace of $A$,
viz. $\mathrm{null}(A^*)$


\emph{Existence and Uniqueness}

Thm. \emph{Every Matrix $A \in \comps^{m \times n}, m \geq n$ has a Full (and
  Reduced) QR Factorization}

Thm. \emph{Every Matrix $A \in \comps^{m \times n}, m \geq n$ of Full Rank has
  a unique Reduced QR Factorization $A = \hat{Q}\hat{R}$ with $r_{jj} \geq 0$}


\emph{Orthogonal Projections} (\S\ref{sec:orthogonal_projection})

one may define an Orthogonal Projection $P$ onto the Column Space of a Full Rank
$m \times n, m \geq n$ Matrix $A$ using the $\hat{Q}$ Matrix from the Reduced
QR Factorization:
\[
  P = \hat{Q}\hat{Q}^*
\]


\emph{Algorithm}

cf. the process of finding Elementary Operations
(\S\ref{sec:elementary_operations}) that produce an LU Factorization
(\S\ref{sec:lu_factorization}) \fist Gaussian Elimination
(\S\ref{sec:gaussian_elimination})

computing $R$:
\[
  \cdots Q_2 Q_1 A = R
\]

using Elementary Orthogonal Matrices (\S\ref{sec:elementary_orthogonal_matrix}):
\begin{itemize}
  \item Givens Matrices (\emph{Plane Rotations} \S\ref{sec:plane_rotation})
  \item Householder Matrices (\emph{Elementary Reflector}
    \S\ref{sec:elementary_reflector})
\end{itemize}

using Givens Rotations, Multiplying by $G_{ij}$ can be used to make an entry
$A[ji]$ Zero; combining a number of such Rotations can be used to yield $R$:
\begin{align*}
  \cdots G_{ij}G_{kl}A & = R \\
  A & = (\cdots G_{ij}G_{kl})^{-1}R \\
  A & = G_{kl}^T G_{ij}^T \cdots R = QR
\end{align*}



\subsubsection{Polar Decomposition}\label{sec:polar_decomposition}

The \emph{Polar Decomposition} of a Square Matrix $A$ is a Matrix Decomposition
of the form:
\[
  A = U P
\]
where $U$ is a Unitary Matrix (\S\ref{sec:unitary_matrix}) and $P$ is a
Positive-semidefinite (\S\ref{sec:positive_definite}) Hermitian
(\S\ref{sec:hermitian_matrix}) Matrix



\subsubsection{Singular-value Decomposition}\label{sec:svd}

(SVD)

Factoriztion of a Real or Complex Matrix generalizing the Eigendecomposition
(\S\ref{sec:eigendecomposition}) of a Positive Semidefinite
(\S\ref{sec:positive_definite}) Normal (\S\ref{sec:normal_matrix}) Matrix (e.g.
a Symmetric Matrix with Positive Eigenvalues) to any $m \times n$ Matrix via an
extension of the Polar Decomposition (\S\ref{sec:polar_decomposition})

The Singular-value Decomposition of an $m \times n$ Real or Complex Matrix $M$
is a Factorization of the form:
\[
  U \Sigma V^*
\]
where $U$ is an $m \times m$ Real or Complex Unitary Matrix
(\S\ref{sec:unitary_matrix}) of \emph{Left-singular Vectors}, $\Sigma$ is an $m
\times n$ Rectangular Diagonal Matrix (\S\ref{sec:diagonal_matrix}) with
Non-negative Real Numbers known as the \emph{Singular Values} of $M$ on the
Diagonal, and $V$ is an $n \times n$ Real or Complex Unitary Matrix of
\emph{Right-singular Vectors}.



% --------------------------------------------------------------------
\subsection{Diagonal Matrix}\label{sec:diagonal_matrix}
% --------------------------------------------------------------------

A Matrix with $0$ values in all non-diagonal entries

a Diagonalizable Matrix is a Square Matrix $A$ for which there exists an
Invertible Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix

\begin{itemize}
  \item Identity Matrices (\S\ref{sec:identity_matrix})
\end{itemize}



% --------------------------------------------------------------------
\subsection{Square Matrix}\label{sec:square_matrix}
% --------------------------------------------------------------------

there is a correspondence between $n\times{n}$ Square Matrices and Linear
Operators (\S\ref{sec:linear_operator}) on an $n$-dimensional Vector Space

a Square Matrix is Invertible (\S\ref{sec:invertible_matrix}) if and only if
its Determinant (\S\ref{sec:determinant}) is $0$

the Product of all Non-zero Eigenvalues (\S\ref{sec:eigenvalue}) of a Square
Matrix is the Pseudo-determinant (\S\ref{sec:pseudo_determinant}) and coincides
with the Determinant when the Matrix is Invertible

a Square Jacobian Matrix (\S\ref{sec:jacobian_matrix}) and its Determinant
(\S\ref{sec:determinant}) are both called ``the Jacobian''



\subsubsection{Determinant}\label{sec:determinant}

A Square Matrix is Invertible (\S\ref{sec:invertible_matrix}) if and only if
its Determinant is non-zero, or equivalently if its Rank (\S\ref{sec:rank})
equals the size of the Matrix, and if it is Invertible the Determinant of the
Inverse Matrix is given by $det(A^{-1}) = \frac{1}{det(A)}$.

\fist the Determinant of a Matrix can be computed by Gaussian Elimination
(\S\ref{sec:gaussian_elimination})

The Special Linear Group $SL(n,F)$ of Degree $n$ over a Field $F$ is the Set of
$n \times n$ Matrices with Determinant $1$ with the Group Operations of
ordinary Matrix Multiplication and Matrix Inversion.

The Determinant of a Square Jacobian Matrix and the Matrix itself are both
called ``the Jacobian''.



\paragraph{Pseudo-determinant}\label{sec:pseudo_determinant}\hfill

Product of all Non-zero Eigenvalues (\S\ref{sec:eigenvalue}) of a Square Matrix

coincides with the regular Determinant when the Matrix is Non-singular
(Invertible \S\ref{sec:invertible_matrix})



\subsubsection{Identity Matrix}\label{sec:identity_matrix}

A Square Diagonal Matrix (\S\ref{sec:diagonal_matrix}) with $1$ on the main
diagonal



\subsubsection{Permutation Matrix}\label{sec:permutation_matrix}

A \emph{Permutation Matrix} is a Square Binary Matrix with exactly one
$1$ in each Row and each Column and $0$s elsewhere.




\subsubsection{Invertible Matrix}\label{sec:invertible_matrix}

(or \emph{Non-singular} or \emph{Non-degenerate})

Non-zero Determinant

\fist the Inverse of an Invertible Matrix can be computed through Gaussian
Elimination (\S\ref{sec:gaussian_elimination})

in general, a Square Matrix over a Commutative Ring is Invertible if and only
if its Determinant is a Unit in that Ring

\fist General Linear Group (\S\ref{sec:general_linear_group}): Group of $n
\times n$ Invertible Matrices

\fist Special Linear Group (\S\ref{sec:special_linear_group}): Group of $n
\times n$ Matrices with Determinant $1$; Normal Subgroup of the General Linear
Group

the Pseudo-determinant (\S\ref{sec:pseudo_determinant}) of a Square Matrix
coincides with the Determinant when the Matrix is Invertible

a Square Matrix $A$ is Diagonalizable (\S\ref{sec:diagonalizable}) if there
exists an Invertible Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix
(\S\ref{sec:diagonal_matrix})

for a Square Matrix, the Matrix is Full Rank (\S\ref{sec:rank}) is
equivalent to being Invertible

if a Matrix $A$ is Full Rank, then $A^*A$ is Invertible



\paragraph{Involutory Matrix} (\S\ref{sec:involutory_matrix})\hfill

$A^2 = I$

a Matrix that is its own Inverse



\subsubsection{Orthogonal Matrix}\label{sec:orthogonal_matrix}

An \emph{Orthogonal Matrix} $Q$ is an $n \times n $ Real-valued Square Matrix
with Columns and Rows that are \emph{Orthonormal Vectors}
(\S\ref{sec:orthonormality}), i.e. forms an Orthonormal Basis
(\S\ref{sec:orthonormal_basis}) for Euclidean Space $\reals^n$, resulting in
the identity:
\[
  Q^TQ = QQ^T = I
\]
i.e. a Matrix $Q$ is Orthogonal if its Transpose is equal to its Inverse:
\[
  Q^T = Q^{-1}
\]

the Set of $n \times n$ Orthogonal Matrices forms the Orthogonal Group
$O(n)$ (\S\ref{sec:orthogonal_group})

an Orthogonal Matrix has Determinant of either $1$ or $-1$; the Orthogonal
Matrices with Determinant $+1$ are \emph{Rotation Matrices}
(\S\ref{sec:rotation_matrix}), forming the Subgroup $SO(n)$ of Special
Orthogonal Matrices (\S\ref{sec:special_orthogonal_group})

\fist an Unitary Matrix (\S\ref{sec:unitary_matrix}) is the Complex analogue of
an Orthogonal Matrix

QR Decomposition (\S\ref{sec:qr_decomposition}): Decomposition of a Matrix $A$
into a Product $A = QR$ of an Orthogonal Matrix $Q$ and an Upper Triangular
Matrix (\S\ref{sec:upper_triangular}) $R$ \fist Elementary Orthogonal Matrices
(\S\ref{sec:elementary_orthogonal_matrix})

cf. LU Decomposition (\S\ref{sec:lu_decomposition}) of a Matrix into a sequence
of Elementary Operations (\S\ref{sec:elementary_operations}), which can be
represented as Matrix Multiplications



\paragraph{Rotation Matrix}\label{sec:rotation_matrix}\hfill

Orthogonal Matrix with Determinant $1$

Special Orthogonal Group $SO(n)$ (\S\ref{sec:special_orthogonal_group})



\paragraph{Elementary Orthogonal Matrix}
\label{sec:elementary_orthogonal_matrix}\hfill

Elementary Matrix (\S\ref{sec:elementary_matrix})

QR Factorization (\S\ref{sec:qr_decomposition})



\subparagraph{Plane Rotation}\label{sec:plane_rotation}\hfill

or \emph{Givens Rotation}

\[
  G_{ij}(\theta) = \begin{bmatrix}
    1 &        &   &            &   &        &   &             &   &        &   \\
      & \ddots &   &            &   &        &   &             &   &        &   \\
      &        & 1 &            &   &        &   &             &   &        &   \\
      &        &   & \cos\theta &   &        &   & -\sin\theta &   &        &   \\
      &        &   &            & 1 &        &   &             &   &        &   \\
      &        &   &            &   & \ddots &   &             &   &        &   \\
      &        &   &            &   &        & 1 &             &   &        &   \\
      &        &   & \sin\theta &   &        &   & \cos\theta  &   &        &   \\
      &        &   &            &   &        &   &             & 1 &        &   \\
      &        &   &            &   &        &   &             &   & \ddots &   \\
      &        &   &            &   &        &   &             &   &        & 1 \\
  \end{bmatrix}
\]
where the ``square'' formed by the $\cos$ and $\sin$ terms are in the $i$th and
$j$th Rows and Columns:
\begin{itemize}
  \item $G_{ij}[ii] = \cos\theta$
  \item $G_{ij}[ij] = -\sin\theta$
  \item $G_{ij}[ji] = \sin\theta$
  \item $G_{ij}[jj] = \cos\theta$
\end{itemize}

\fist may be used in computing QR Factorization (\S\ref{sec:qr_decomposition})



\subparagraph{Elementary Reflector}\label{sec:elementary_reflector}\hfill

or \emph{Householder Transformation}

\emph{Reflection} about a Plane containing the Origin

Plane Normal $\hat{w}$

\begin{align*}
  H\vec{x} & = \vec{x} - 2(\hat{w}^T\vec{x})\hat{w} \\
  H\vec{x} & = (I - 2\hat{w}\hat{w}^T)\vec{x}       \\
         H & = I - 2\hat{w}\hat{w}^T
\end{align*}

\fist may be used in computing QR Factorization (\S\ref{sec:qr_decomposition})



\subsubsection{Singular Matrix}\label{sec:singular_matrix}

Non-invertible Matrix

a Square Matrix is Singular if and only if its Determinant
(\S\ref{sec:determinant}) is $0$

\fist in a System of DAEs (\S\ref{sec:system_of_daes}), the Jacobian Matrix
(\S\ref{sec:jacobian_matrix}) is a Singular (Non-invertible) Matrix

examples:
\begin{itemize}
  \item Projections (\S\ref{sec:projection}) -- $A^2 = A$
\end{itemize}



\subsubsection{Diagonalizable Matrix}\label{sec:diagonalizable_matrix}

A Square Matrix $A$ is \emph{Diagonalizable} if there exists an Invertible
Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix
(\S\ref{sec:diagonal_matrix}).

A Square Matrix that is \emph{not} Diagonalizable is called \emph{Defective}.

Only Diagonlizable Matrices can be Eigendecomposed
(\S\ref{sec:eigendecomposition}).



\subsubsection{Symmetric Matrix}\label{sec:symmetric_matrix}

A \emph{Symmetric Matrix} $A$ is a Square Matrix that is equal to its
Transpose:
\[
  A = A^T
\]

the Complex extension of a Symmetric Matrix is a \emph{Hermitian Matrix}
(\S\ref{sec:hermitian_matrix})

Positive-definite Matrix (\S\ref{sec:positive_definite})

an Orthogonal Projection (\S\ref{sec:orthogonal_projection}) is exactly an
Idempotent Symmetric Linear Operator



\subsubsection{Skew-symmetric Matrix}\label{sec:skew_symmetric}

Bivectors (\S\ref{sec:bivector}) are Isomorphic to Skew-symmetric Matrices



\subsubsection{Hamiltonian Matrix}\label{sec:hamiltonian_matrix}



% --------------------------------------------------------------------
\subsection{Triangular Matrix}\label{sec:triangular_matrix}
% --------------------------------------------------------------------

the Group of Invertible Upper (or Lower) Triangular Matrices is a Borel
Subgroup of the General Linear Group $GL_n$ of Invertible Matrices



\subsubsection{Upper Triangular Matrix}\label{sec:upper_triangular}

QR Decomposition (\S\ref{sec:qr_decomposition}): Decomposition of a Matrix $A$
into a Product $A = QR$ of an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix})
$Q$ and an Upper Triangular Matrix $R$



% --------------------------------------------------------------------
\subsection{Elementary Matrix}\label{sec:elementary_matrix}
% --------------------------------------------------------------------

a Matrix that differs from the Identity Matrix by a single Elementary Row
Operation (\S\ref{sec:elementary_operation})

Elementary Matrices generate the General Linear Group $GL_n(R)$ of Invertible
Matrices

the LU Decomposition (\S\ref{sec:lu_decomposition}) of a Matrix is the
Decomposition into Elementary Operations represented as Matrices \fist Gaussian
Elimination (\S\ref{sec:gaussian_elimination})

Elementary Orthogonal Matrix (\S\ref{sec:elementary_orthogonal_matrix}): can be
used in QR Decomposition (\S\ref{sec:qr_decomposition})



\subsubsection{Elementary Operation}\label{sec:elementary_operation}

\emph{Elementary Row Operation}

\emph{Elementary Column Operation}



\subsubsection{Gaussian Elimination}\label{sec:gaussian_elimination}

\emph{Row Reduction} or \emph{Gauss-Jordan Elimination}

Algorithm for solving Systems of Linear Equations
(\S\ref{sec:system_of_linear_equations})

can be used to find the Rank (\S\ref{sec:rank}) of a Matrix, calculate the
Determinant (\S\ref{sec:determinant}) of a Matrix, and to calculate the Inverse
of an Invertible Square Matrix (\S\ref{sec:invertible_matrix})

the Elementary Operations can be represented as Matrices, giving the \emph{LU
  Factorization} (\S\ref{sec:lu_decomposition}) of a Matrix

cf. QR Factorization (\S\ref{sec:qr_decomposition}) as a Decomposition into
Orthogonal Matrices (\S\ref{sec:orthogonal_matrix})



\paragraph{Echelon Form}\label{sec:echelon_form}\hfill

\emph{Row Echelon Form}

\emph{Column Echelon Form}



\subparagraph{Reduced Echelon Form}\label{sec:reduced_echelon}\hfill

or \emph{Canonical Form}



% --------------------------------------------------------------------
\subsection{Diagonally Dominant Matrix}\label{sec:diagonally_dominant}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Irreducible Matrix}\label{sec:irreducible_matrix}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Complex Matrix}\label{sec:complex_matrix}
% --------------------------------------------------------------------

\subsubsection{Conjugate Transpose}\label{sec:conjugate_transpose}

The \emph{Conjugate Transpose} (or \emph{Hermitian Transpose}) of an $m \times
n$ Complex Matrix $A$ is the $n \times m$ Matrix $A^*$ resulting from the
Transpose (\S\ref{sec:transpose}) of $A$ followed by taking the Complex
Conjugate (\S\ref{sec:complex_conjugate}) of each entry.

if a Matrix $A$ is Full Rank, then $A^*A$ is Invertible



\subsubsection{Normal Matrix}\label{sec:normal_matrix}

\subsubsection{Unitary Matrix}\label{sec:unitary_matrix}

Complex analogue of an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix})

A \emph{Unitary Matrix} $U$ has the Property that:
\[
  U^*U = UU^* = I
\]
where $U^* = \overline{U^T}$ is the Conjugate Transpose
(\S\ref{sec:conjugate_transpose}) of $U$

\fist Special Unitary Group (\S\ref{sec:special_unitary})
$\mathrm{SU}(n)$: the Lie Group of $n \times n$ Unitary Matrices with
Determinant $1$

the Frobenius Norm and the Euclidean Norm ($2$-norm \S\ref{sec:matrix_norm}))
are both invariant under Unitary Multiplication, i.e. Left or Right Multiplying
a Matrix $A$ by an Unitary Matrix $Q$ results in a Matrix with the same Norm as
the original Matrix $A$

only a Unitary Matrix can be used as a Quantum Gate in Quantum Computation

Polar Decomposition (\S\ref{sec:polar_decomposition})



\subsubsection{Hermitian Matrix}\label{sec:hermitian_matrix}

or \emph{Self-adjoint Matrix}

a Complex Square Matrix that is equal to its own Conjugate Transpose

Complex extension of Real Symmetric Matrices (\S\ref{sec:symmetric_matrix})

Positive-definite Matrix (\S\ref{sec:positive_definite})

Polar Decomposition (\S\ref{sec:polar_decomposition})

an Orthogonal Projection (\S\ref{sec:orthogonal_projection}) is exactly an
Idempotent Hermitian Linear Operator



\paragraph{Gramian Matrix}\label{sec:gramian_matrix}\hfill

Hermitian Matrix of Inner Products

Set of Vectors $\vec{v}_1,\ldots,\vec{v}_n$ in an Inner Product Space

Gramian Matrix $G$ with entries:
\[
  G_{ij} = \langle{\vec{v}_i,\vec{v}_j}\rangle
\]

a Set of Vectors is Linearly Independent if and only if the Determinant is
Non-zero

a Matrix is Positive Semidefinite (\S\ref{sec:positive_semidefinite}) if it is
the Gramian Matrix of some Vectors



\subsubsection{Skew Hermitian Matrix}\label{sec:skew_hermitian}

or \emph{Antihermitian}



% --------------------------------------------------------------------
\subsection{Positive-definite Matrix}\label{sec:positive_definite}
% --------------------------------------------------------------------

a Property of Symmetric (\S\ref{sec:symmetric_matrix}) and Hermitian
(\S\ref{sec:hermitian_matrix}) Matrices

$n \times n$ Real Matrix $M$ is Positive Definite if the Scalar $z^TMz$ is
\emph{Strictly Positive} ($>0$) for every Non-zero Column Vector $z$



\subsubsection{Positive-semidefinite Matrix}\label{sec:positive_semidefinite}

$M \succeq 0$

Zeros are allowed

a Matrix is Positive Semidefinite if it is the Gramian Matrix
(\S\ref{sec:gramian_matrix}) of some Vectors



% --------------------------------------------------------------------
\subsection{Jacobian Matrix}\label{sec:jacobian_matrix}
% --------------------------------------------------------------------

%FIXME: move to multivariable calculus or merge with jacobian section ?

Matrix of all First-order Partial Derivatives (\S\ref{sec:partial_derivative})
of a Vector-valued Function (\S\ref{sec:vector_function})

a Square Jacobian Matrix and its Determinant (\S\ref{sec:determinant}) are both
called ``the Jacobian''

Jacobian (\S\ref{sec:jacobian}),
Locally Linear Functions (\S\ref{sec:locally_linear})

\fist Local Linearization (\S\ref{sec:local_linearization})

\fist the Jacobian Matrix for a System of DAEs (\S\ref{sec:system_of_daes}) is
a Singular (Non-invertible) Matrix (\S\ref{sec:singular_matrix})

the Absolute Condition Number (\S\ref{sec:absolute_condition_number}),
$\hat{k}$, of a Differentiable Function $f(x)$ is equal to the Matrix Norm
(\S\ref{sec:matrix_norm}) of its Jacobian Matrix $J$:
\[
  \hat{k} = \|J(x)\|
\]



\subsubsection{Jacobian Determinant}\label{sec:jacobian_determinant}



% --------------------------------------------------------------------
\subsection{Hessian Matrix}\label{sec:hessian_matrix}
% --------------------------------------------------------------------

%FIXME: move to multivariable calculus ?

Matrix of all Second-order Partial Derivatives (\S\ref{sec:partial_derivative})
of a Scalar-valued Function or Scalar Field

\fist Quadratic Approximation (\S\ref{sec:quadratic_approximation})



% ====================================================================
\section{Differential Algebra}\label{sec:differential_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Derivation}\label{sec:derivation}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Differential Ring}\label{sec:differential_ring}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Differential Field}\label{sec:differential_field}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Differential Galois Theory}\label{sec:differential_galois}
% --------------------------------------------------------------------

Galois Theory (\S\ref{sec:galois_theory})

Extensions of Differential Fields

Galois Groups (\S\ref{sec:galois_group}) of Differential Equations



% ====================================================================
\section{Multilinear Algebra}\label{sec:multilinear_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Multilinear Map}\label{sec:multilinear_map}
% --------------------------------------------------------------------

\subsubsection{Bilinear Map}\label{sec:bilinear_map}

Module (\S\ref{sec:module}), Vector Space (\S\ref{sec:vector_space})

$R$-Algebra (\S\ref{sec:r_algebra})

$K$-Algebra (\S\ref{sec:k_algebra})

Bilinear Product (\S\ref{sec:bilinear_product})



% --------------------------------------------------------------------
\subsection{Multilinear Form}\label{sec:multilinear_form}
% --------------------------------------------------------------------

\subsubsection{Bilinear Form}\label{sec:bilinear_form}

generalization of Dot Product (\S\ref{sec:inner_product}) of Euclidean Space

Sesquilinear Form (\S\ref{sec:sesquilinear_form}) generalization of Bilinear
Forms to cases where $K$ is the Field of Complex Numbers and the mapping is
Antilinear (Conjugate-linear \S\ref{sec:antilinear}) in one argument.

An Inner Product (\S\ref{sec:inner_product}) is a Sesquilinear Form and an
Inner Product over a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}).



\paragraph{Orthogonality}\label{sec:orthogonality}\hfill

Two Vectors $\vec{u}, \vec{v}$ in a Vector Space with Bilinear Form $B$
are are \emph{Orthogonal} when $B(u,v) = 0$, denoted:
\[
  \vec{u} \bot \vec{v}
\]
The Zero Vector, $\vec{0}$, is Orthogonal to every Vector (including itself).

\fist Inner Product (\S\ref{sec:inner_product})

\fist Orthonormality (\S\ref{sec:orthonormality})

Two Linear Subspaces (\S\ref{sec:linear_subspace}) $A, B \subset V$ of an Inner
Product Space $V$ are called \emph{Orthogonal Subspaces}
(\S\ref{sec:orthogonal_subspace}) if each Vector in $A$ is Orthogonal to each
Vector in $B$.

a Set of Vectors is Orthogonal if every Pair of Vectors in it is Orthogonal; a
Set is Orthonormal if it is Orthogonal and every Vector in it is a Unit Vector



\paragraph{Symmetric Bilinear Form}\label{sec:symmetric_bilinear}\hfill

Symmetric Bilinear Forms over a Vector Space correspond one-to-one
with Quadratic Forms (\S\ref{sec:quadratic_form}) over the Vector
Space

An Inner Product (\S\ref{sec:inner_product}) on a Real Vector Space is
a Positive-definite (\S\ref{sec:definite_quadratic}) Symmetric
Bilinear Form.



\paragraph{Degenerate Bilinear Form}
\label{sec:degenerate_bilinear_form}\hfill

Pseudo-Riemannian Manifold (\S\ref{sec:pseudo_riemannian})



\subsubsection{Quadratic Form}\label{sec:quadratic_form}

%FIXME: move to polynomials ???

Homogenous Polynomial (\S\ref{sec:homogenous_polynomial})

Quadratic Forms over a Vector Space correspond one-to-one with
Symmetric Bilinear Forms (\S\ref{sec:symmetric_bilinear}) over
the Vector Space

Matrix form $\vec{x}^T M \vec{x}$:
\[
  [x y]
  \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix}
  = ax^2 + 2bxy + cy^2
\]



\paragraph{Definite Quadratic Form}\label{sec:definite_quadratic}\hfill

a Quadratic Form over some Real Vector Space that has the same Sign
for every Non-zero Vector

\emph{Positive Definite}

\emph{Negative Definite}

\emph{Semidefinite}

an Inner Product (\S\ref{sec:inner_product}) on a Real Vector Space is
a Positive-definite Symmetric Bilinear
(\S\ref{sec:symmetric_bilinear}) Form



\paragraph{Quadratic Space}\label{sec:quadratic_space}\hfill



\subsubsection{Sesquilinear Form}\label{sec:sesquilinear_form}

generalization of Bilinear Forms (\S\ref{sec:bilinear_form}) to cases where $K$
is the Field of Complex Numbers and the mapping is Antilinear (Conjugate-linear
\S\ref{sec:antilinear}) in one argument

Inner Products (\S\ref{sec:inner_product}) are Sesquilinear Forms



\paragraph{Hermitian Form}\label{sec:hermitian_form}\hfill

an Inner Product (\S\ref{sec:inner_product}) is a Positive-definite
(\S\ref{sec:definite_quadratic}) Hermitian Form



% --------------------------------------------------------------------
\subsection{Dyadic Algebra}\label{sec:dyadic_algebra}
% --------------------------------------------------------------------

\subsubsection{Dyadic Product}\label{sec:dyadic_product}

Product of a Column Vector multiplied by a Row Vector produces a Second-order
Tensor called a \emph{Dyadic}



% ====================================================================
\section{Numerical Linear Algebra}\label{sec:numerical_linear_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Iterative Method}\label{sec:iterative_method}
% --------------------------------------------------------------------

%FIXME: move to optimization ???

cf. \emph{Direct Methods} attempt to solve a problem by a Finite Sequence of
Operations delivering the exact Solution

Attractive Fixed Points (Dynamical Systems \S\ref{sec:dynamical_systems})

\fist Ellipsoid Method (\S\ref{sec:ellipsoid_method}); when specialized for
solving feasible Linear Optimization (\S\ref{sec:linear_programming}) Problems
with Rational Data, the Ellipsoid Metohd finds an Optimal Solution in a Finite
number of steps

\fist Sequential Quadratic Programming
(\S\ref{sec:sequential_quadratic_programming}): Iterative Method for
Constrained Non-linear Optimization (\S\ref{sec:nonlinear_programming})



\subsubsection{Stationary Iterative Method}
\label{sec:stationary_iterative}

or \emph{Relaxation Methods}

\fist not to be confused with Relaxation (Approximation \S\ref{sec:relaxation})

used to solve the Linear Equations resulting from a Discretization of the
Differential Equation, e.g. by Finite Differences



\paragraph{Jacobi Method}\label{sec:jacobi_method}\hfill

Diagonally Dominant (\S\ref{sec:diagonally_dominant}) System of Linear
Equations (\S\ref{sec:system_of_linear_equations})



\subparagraph{Projected Jacobi Method}
\label{sec:projected_jacobi_method}



\paragraph{Gauss-Seidel Method}\label{sec:gauss_seidel}\hfill

(or \emph{Liebmann Method} or \emph{Method of Successive
  Displacement})

Iterative Method for solving a Linear System of Equations
(\S\ref{sec:system_of_linear_equations})

Convergence only guaranteed if the Matrix is either Diagonally
Dominant (\S\ref{sec:diagonally_dominant}) or Symmetric
(\S\ref{sec:symmetric_matrix}) and Positive Definite
(\S\ref{sec:positive_definite})

Gauss-Seidel is the same as Successive Over-Relaxation
(\S\ref{sec:sucessive_over_relaxation}) with $\omega = 1$



\subparagraph{Projected Gauss-Seidel Method}\hfill
\label{sec:projected_gauss_seidel}

Gauss-Seidel applied to Linear Complementarity Problem (LCP
\S\ref{sec:linear_complementarity})

PGS



\subparagraph{Non-linear Gauss-Seidel Method}
\label{sec:nonlinear_gauss_seidel}

NGS



\paragraph{Successive Over-Relaxation Method}
\label{sec:successive_over_relaxation}\hfill

SOR

Gauss-Seidel (\S\ref{sec:gauss_seidel}) is the same as SOR with $\omega = 1$



\subsubsection{Krylov Subspace Method}\label{sec:krylov_subspace_method}



% ====================================================================
\section{Super Linear Algebra}\label{sec:super_linear_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Superalgebra}\label{sec:superalgebra}
% --------------------------------------------------------------------

$Z_2$-graded Algebra



\subsubsection{Clifford Algebra}\label{sec:clifford_algebra}

\url{https://golem.ph.utexas.edu/category/2014/07/the_tenfold_way.html} -- kinds
of matter corresponding to Real Clifford Algebras:
\begin{itemize}
\item $Cl_0$ (equivalent to $\reals$)
\item $Cl_1$
\item $Cl_2$
\item $Cl_3$
\item $Cl_4$ (equivalent to $\quats$)
\item $Cl_5$
\item $Cl_6$
\item $Cl_7$
\end{itemize}
and Complex Clifford Algebras:
\begin{itemize}
\item $\comps{l}_0$ (equivalent to $\comps$)
\item $\comps{l}_1$ (Superalgebra created by adding an Odd Square Root of $-1$
  to the purely Even Algebra $\comps$)
\end{itemize}



\paragraph{Dual Quaternion}\label{sec:dual_quaternion}\hfill

\fist Quaternions (\S\ref{sec:quaternion})

Rigid-body Displacements

Rigid Motions in 3D Space can be represented by Dual Quaternions of Unit Length

Parallel Manipulators
