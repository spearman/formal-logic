%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Linear Algebra}\label{part:linear_algebra}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fist cf. Vector Calculus (\S\ref{sec:vector_calculus})

\fist Set Theory (Part \ref{part:set_theory}) as Linear Algebra over the
``Field with one Element'' \fist $F_1$-geometry (\S\ref{sec:f1_geometry})

\fist Linear Equations (\S\ref{sec:linear_equation}), Systems of Linear
Equations (\S\ref{sec:linear_equation_system})

\fist Max-linear Algebra (Tropical Geometry \S\ref{sec:tropical_geometry})

\begin{itemize}
  \item $R$-modules (Module over a Commutative Ring \S\ref{sec:module}) -- an
    Abelian Group (\S\ref{sec:commutative_group}):
    \[
      (M,+)
    \]
    with Scalar Multiplication Operation $\cdot$
    \begin{itemize}
      \item $R$-algebras (Algebra over a Commutative Unital Ring
        \S\ref{sec:r_algebra}) -- an $R$-module with a Bilinear Product
        (\S\ref{sec:bilinear_product})
    \end{itemize}
  \item Vector Spaces ($K$-modules \S\ref{sec:vector_space}) -- a Module for
    which the Ring is also a Field, $K$ (each Element has a Multiplicative
    Inverse)
    \begin{itemize}
      \item $K$-algebras (Algebra over a Field \S\ref{sec:k_algebra}) -- Vector
        Space with a Bilinear Product
    \end{itemize}
\end{itemize}

\asterism

\url{https://twitter.com/davidad/status/1204904687356411904}:

``Linear Algebra is a Model of Linear Logic in which $\{\otimes, \parr\}$
exactly coincide (Compact Closed) and $\{\oplus, \&\}$ exactly coincide
(Biproducts), but $\otimes$ and $\oplus$ do NOT coincide. They do Distribute,
and in the case of $\cat{FdVect}$, beautifully categorify the Semiring $\nats$''



% ==============================================================================
\section{Module}\label{sec:module}
% ==============================================================================

A \emph{Module} is a Unital Ring (\S\ref{sec:unital_ring}), $R$, together with
an Abelian Group (\S\ref{sec:commutative_group}), $(M, +)$, and an Operation
called \emph{Scalar Multiplication} which is either:
\[ R \times M \rightarrow M \]
for a \emph{Left $R$-module $M$}, $_R M$, or:
\[ M \times R \rightarrow M \]
for a \emph{Right $R$-module $M$}, $M_R$.

a Module can also be seen as a Monoid Action (\S\ref{sec:monoid_action}) on an
Commutative Group;
cf. a Vector Space (\S\ref{sec:vector_space}) as a ``Field Action'' on a
Commutative Group

The Scalar Multiplication Operator is required that for all $r,s \in R$ and
$x,y \in M$ in a Left $R$-module $M$:
\begin{enumerate}
    \item $r(x + y) = rx + ry$
    \item $(r + s)x = rx + sx$
    \item $(rs)x = r(sx)$
    \item $1_Rx = x$
\end{enumerate}
or in a Right $R$-module $M$:
\begin{enumerate}
    \item $(x + y)r = xr + yr$
    \item $x(r + s) = xr + xs$
    \item $x(rs) = (sx)r$
    \item $x 1_R = x$
\end{enumerate}
where $1_R$ is the Multiplicative Identity for $R$. If the Ring is not
required to be Unital, then item (4) above can be ommitted, but can be
explicitly required by stating that we are talking about a
\emph{Unital Left/Right $R$-module $M$}.

If $R$ is Commutative (\S\ref{sec:commutative_ring}), then Left $R$-modules are
the same as Right $R$-modules and simply called \emph{$R$-modules}.

A Bimodule (\S\ref{sec:bimodule}) has Left and Right Scalar Operations that are
``compatible'', but not necessarily identical.

A Module Homomorphism is called a \emph{Linear Map} (or \emph{Linear
  Transformation} \S\ref{sec:linear_transformation})

(wiki): every Commutative Group is a \emph{Module} over the Ring of Integers
$\ints$ in a ``unique way''

(wiki): related to Matrices by a Canonical Isomorphism of Abelian Groups (TODO)

and viewing $R$ as a Right $R$-module, a Ring Isomorphism:
\[
  End_R(R^n) \simeq M_n(R)
\]

cf. Differential Graded Module (Homological Algebra
\S\ref{sec:differential_graded_module}), Module Spectrum (Homotopical Algebra
\S\ref{sec:module_spectrum})

\fist Vector Space (or \emph{$K$-module}) \S\ref{sec:vector_space}) -- Module
over a Ring which is also a Field (\S\ref{sec:field}), i.e. all Ring Elements
have Multiplicative Inverses

\fist $D$-module (Algebraic Analysis \S\ref{sec:d_module}) -- a Module over a
Ring $D$ of Differential Operators (\S\ref{sec:differential_operator}); approach
to theory of Linear PDEs (\S\ref{sec:linear_pde})

\fist $R$-algebra (Algebra over a Commutative Unital Ring \S\ref{sec:r_algebra})
-- a Module with a Bilinear Product (\S\ref{sec:bilinear_product})

\fist Commutative Algebra is the study of Commutative Rings
(\S\ref{sec:commutative_ring}), their Ideals (\S\ref{sec:ring_ideal}), and
Modules over Commutative Rings, and Local Algebra (\S\ref{sec:local_algebra})
the study of Local Rings (\S\ref{sec:local_ring}) and their Modules

\fist Differential Graded Module (\S\ref{sec:differential_graded_module})

Modules can be seen as Representations (\S\ref{sec:algebra_representation}) of
Rings or Associative Algebras; every Ring has a ``natural'' $R$-module on itself
where the Module Action is defined as the Multiplication in the Ring; it is
therefore useful to study a Ring by the Category of Modules over a Ring; two
Rings are Morita Equivalent (\S\ref{sec:morita_equivalence}) if their Module
Categories are Equivalent

\fist Module Presentation (\S\ref{sec:module_presentation}) -- Free Presentation
of a Module $M$ over a Commutative Ring $R$



% ------------------------------------------------------------------------------
\subsection{Generating Set}\label{sec:generating_set}
% ------------------------------------------------------------------------------

(wiki):

a \emph{Generating Set} $G$ of a Module $M$ over a Ring $R$ is a Subset of $M$
such that the smallest Submodule of $M$ containing $G$ is $M$ itself; $G$ is
said to \emph{Generate} $M$

if $R$ is a Field, then a Generating Set is the same thing as a Basis
(\S\ref{sec:basis})

cf. Generating Set of a Group (\S\ref{sec:group_generator})



% ------------------------------------------------------------------------------
\subsection{Submodule}\label{sec:submodule}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Linear Transformation}\label{sec:linear_transformation}
% ------------------------------------------------------------------------------

A \emph{Linear Transformation} (or \emph{Linear Map}) is a Module Homomorphism
$L : V \rightarrow W$ preserving Addition and Scalar Multiplication:
\begin{align*}
  L(a\vec{v})              & = aL(\vec{v}) \\
  L(\vec{v}_1 + \vec{v}_2) & = L(\vec{v}_1 + \vec{v}_2)
\end{align*}

\fist cf. Linear Function (\S\ref{sec:linear_function}) -- a Polynomial
Function of Degree Zero or One

\fist cf. Locally Linear Transformations (\S\ref{sec:locally_linear})

(wiki): related to Matrices by a Canonical Isomorphism of Abelian Groups (TODO)
and viewing $R$ as a Right $R$-module, a Ring Isomorphism:
\[
  End_R(R^n) \simeq M_n(R)
\]

Any Linear Map $A : V \rightarrow W$ from an $n$-dimensional Vector Space $V$
to an $m$-dimensional Vector Space $W$ can be represented by an $m \times n$
Matrix of Rank $r$ sending Column Vector $\vec{v} \in V$ to the Column Vector
$A\vec{v} \in W$. Each such Linear Transformation induces Four Fundamental
Subspaces (\S\ref{sec:fundamental_subspace}):
\begin{enumerate}
  \item $im(A) \subset W$ -- the \emph{Image} of $A$ is the \emph{Column Space}
    (\S\ref{sec:column_space}) of Dimension $r$
  \item $coim(A) \subset V$ -- the \emph{Coimage} of $A$ is the \emph{Row Space}
    (\S\ref{sec:row_space}) of Dimension $r$
  \item $ker(A) \subset V$ -- the \emph{Kernel} of $A$ is the \emph{Nullspace}
    (\S\ref{sec:nullspace}) of Dimension $n - r$ (Nullity) %FIXME clarify
  \item $coker(A) \subset W$ -- the \emph{Cokernel} of $A$ is the \emph{Left
    Nullspace} (\S\ref{sec:left_nullspace}) of Dimension $m - r$ (Corank)
    %FIXME clarify
\end{enumerate}
such that:
\begin{enumerate}
  \item $ker(A) = coim(A)^\bot$ -- the Nullspace is the Orthogonal Complement
    (\S\ref{sec:orthogonal_complement}) of the Row Space
  \item $coker(A) = im(A)^\bot$ -- the Left Nullspace is the Orthogonal
    Complement of the Column Space
\end{enumerate}

\url{https://www.youtube.com/watch?v=VmfTXVG9S0U&feature=youtu.be&t=1m39s}: in
a Linear Transformation of a Space, all of the ``grid lines'' of the Space
remain \emph{Parallel} and \emph{evenly spaced}

Linear Transformations are Morphisms in the Category of Modules over a given
Ring (\S\ref{sec:ring}).

Applying a Linear Transformation to a Set of Cartesian Coordinates defines an
\emph{Affine Coordinate System} (\S\ref{sec:affine_coordinates}); cf.
application of a Locally Invertible Transformation
(\S\ref{sec:locally_invertible}) to a Set of Cartesian Coordinates defines a
Curvilinear Coordinate System (\S\ref{sec:curvilinear_coordinates}).

--FIXME: is this equivalent to the condition that a transformation has
components that are Invertible, Real-valued with Continuous $2$nd Partial
Derivatives everywhere ??? \url{https://www.youtube.com/watch?v=XtpVVcKXfnA}

All Linear Transformations are \emph{Affine Transformations}
(\S\ref{sec:affine_transformation}), but not every Affine Transformation is
Linear; Affine Transformations are not required to preserve the Zero Point in a
Linear Space (FiXME: clarify)

\fist General Linear Group (\S\ref{sec:general_linear_group}): Group of $n
\times n$ Invertible Matrices

\fist Special Linear Group (\S\ref{sec:special_linear_group}): Group of $n
\times n$ Matrices with Determinant $1$; Normal Subgroup of the General Linear
Group

A \emph{Linear Operator} (\S\ref{sec:linear_operator}) is an Endomorphic Linear
Map (i.e. the Domain and Codomain are the same Module).

\fist Linear Differential Operators (\S\ref{sec:linear_differential_operator})

A \emph{Linear Functional} (or \emph{Linear Form} \S\ref{sec:linear_form}) is a
Linear Map from a Vector Space $V$ to its Field of Scalars $K$ (viewed as a
Vector Space over itself).

A Linear Map between Topological Vector Spaces
(\S\ref{sec:topological_vectorspace}) can be Continuous
(\S\ref{sec:continuous_map}) and if the Domain and Codomain are the same it is a
\emph{Continuous Linear Operator} (\S\ref{sec:continuous_linear}).

Positive Definite Matrices (\S\ref{sec:positive_definite}) are Symmetric
Matrices $M$ such that $\vec{x}^T M \vec{x} > 0$; this is a kind of analogue to
``Positive Real Numbers'' for Square Matrices

Homothety (\S\ref{sec:homothety}): a specific kind of Linear Transformation of
an Affine Space called a \emph{Dilation} or \emph{Scale Transformation}, with
Uniform Scaling (\S\ref{sec:scaling}) as a special case; in Projective Geometry
(\S\ref{sec:projective_geometry}) it is a Similarity Transformation
(\S\ref{sec:similarity_transformation}) that leaves the Line at Infinity
Pointwise Invariant

Linear Maps (Endomorphisms) of a Vector Space as Representation of a Lie Algebra
(\S\ref{sec:lie_algebra_representation})

(Fong16):

%FIXME: are linear relations different from linear maps in general?

\emph{Linear Relation} $L : U \rightsquigarrow V$ is a Subspace $L
\subseteq U \oplus V$

Linear Relations as Corelations in $\cat{Vect}$; Category of Vector
Spaces and Linear Relations $\cat{LinRel}$

2018 - Conal Elliott - \emph{The simple essence of automatic differentiation}
(video lecture): \url{https://www.youtube.com/watch?v=Shl3MtWGu18}

Linear Maps form a Cartesian Category (\S\ref{sec:cartesian_category});
Automatic Differentiation (\S\ref{sec:automatic_differentiation}) can be
extended from Linear Maps to arbitrary Cartesian Categories with Multiplication
generalized in terms of \emph{Scale} and (Cocartesian) \emph{Join} operations



\subsubsection{Fundamental Subspace}\label{sec:fundamental_subspace}

\fist Fundamental Theorem of Linear Algebra
(\S\ref{sec:fundamental_linear_algebra_theorem})



\paragraph{Column Space}\label{sec:column_space}\hfill

\emph{Image} or \emph{Range}

Rank (\S\ref{sec:rank})

the Left Nullspace (\S\ref{sec:left_nullspace}) of a Matrix $A$ is the Subspace
that is Orthogonal to the Column Space of $A$

\begin{align*}
  \vec{x} & \in \mathrm{null}(A^*) \Leftrightarrow         \\
  \vec{x} & \bot \text{all Rows of }A^* \Leftrightarrow    \\
  \vec{x} & \bot \text{all Columns of }A \Leftrightarrow   \\
  \vec{x} & \in \mathrm{image}(A)^\bot
\end{align*}



\paragraph{Row Space}\label{sec:row_space}\hfill

\emph{Coimage}

Rank (\S\ref{sec:rank})

the Nullspace (\S\ref{sec:nullspace}) of a Matrix $A$ is the Subspace that is
orthogonal to the Row Space of $A$

\begin{align*}
  \vec{x} & \in \mathrm{null}(A) \Leftrightarrow         \\
  \vec{x} & \bot \text{all Rows of }A \Leftrightarrow    \\
  \vec{x} & \bot \text{all Columns of }A^* \Leftrightarrow   \\
  \vec{x} & \in \mathrm{coimage}(A)^\bot
\end{align*}



\paragraph{Nullspace}\label{sec:nullspace}\hfill

The \emph{Nullspace} of a Linear Map $L : V \rightarrow W$ between Vector
Spaces $V$ and $W$ is the Set of all $\vec{v}\in{V}$ such that $L(\vec{v}) =
\vec{0}$ where $\vec{0}$ is the Zero Vector in $W$:
\[
  \text{ker}(L) = \{\vec{v}\in{V} \;|\; L(\vec{v}) = \vec{0}\}
\]

The Kernel of $L$ is a Linear Subspace of the Domain $V$.

Nullity (\S\ref{sec:nullity}): Dimension of the Null Space

\emph{Rank-Nullity Theorem}

for an $m \times n$ Matrix $A$, the Rank of $A$ plus the Nullity
of $A$ is equal to $n$



\paragraph{Left Nullspace}\label{sec:left_nullspace}\hfill

or \emph{Cokernel}

Corank (\S\ref{sec:rank})

the Left Nullspace of a Matrix $A$ is the Subspace that is Orthogonal to the
Column Space (\S\ref{sec:column_space}) of $A$

\begin{align*}
  \vec{x} & \in \mathrm{null}(A^*) \Leftrightarrow         \\
  \vec{x} & \bot \text{all Rows of }A^* \Leftrightarrow    \\
  \vec{x} & \bot \text{all Columns of }A \Leftrightarrow   \\
  \vec{x} & \in \mathrm{image}(A^*)^\bot
\end{align*}



\subsubsection{Linear Operator}\label{sec:linear_operator}

A \emph{Linear Operator} is an Endomorphic Linear Map, i.e. a Linear Map from a
Module to itself (Operator \S\ref{sec:operator}), $M : V \rightarrow V$.

There is a correspondence between $n\times{n}$ Square Matrices
(\S\ref{sec:square_matrix}) and Linear Operators on an $n$-dimensional Vector
Space, but not every Infinite Matrix corresponds with a Linear Operator on an
Infinite-dimensional Vector Space: only Column-finite Matrices (Infinite
Matrices with a Finite number of non-zero entries in each Column) represent
Linear Operators.

examples:
\begin{itemize}
  \item Identity Operator (\S\ref{sec:identity_operator})
  \item Shift Operator (\S\ref{sec:shift_operator}) -- $T_h$
  \item Forward Difference Operator (\S\ref{sec:forward_difference}) --
    $\Delta_h$
  \item Differentiation Operator (\S\ref{sec:derivative}) -- $D^n$
  \item Antidifference Operator (Indefinite Sum Operator
    \S\ref{sec:antidifference}) -- $\Delta^{-1}_h$
  \item Antidifferentiation Operator (Indefinite Integration Operator
    \S\ref{sec:antiderivative}) -- $D^{-n}$
  \item Unbounded Linear Operators (Topological Vector Spaces
    \S\ref{sec:unbounded_linear_operator})
  \item ... MORE
\end{itemize}

The Set of all Linear Operators $End(V)$ together with Addition, Composition,
and Scalar Multiplication forms an Associative Algebra
(\S\ref{sec:associative_algebra}) with Identity Element over the Field $K$ with
the Identity Map for the Multiplicative Identity.

$End(V)$ -- Associative Algebra (Ring) of Endomorphisms

$GL(V)$ -- General Linear Group (\S\ref{sec:general_linear_group}):
Automorphism Group of Isomorphic Endomorphisms (Invertible Matrices); Group of
Units of the Ring $End(V)$; without the Invertibility requirement (i.e.
requiring Nonzero Determinants), the structure is the Full Linear Semigroup
(\S\ref{sec:full_linear_semigroup})

$SL(V)$ -- Special Linear Group (\S\ref{sec:special_linear_group}): Normal
Subgroup of $GL(V)$ of Matrices with Determinant $1$, i.e. \emph{Volume and
  Orientation Preserving} Linear Operators

$O(V)$ -- Orthogonal Group (\S\ref{sec:orthogonal_group}): Group of Orthogonal
Matrices (Determinant $+1$ or $-1$)

$U(V)$ -- Unitary Group (\S\ref{sec:unitary_group}): Group of Unitary Matrices
(i.e. $U^*U = UU^* = I$, or equivalently $U$ is Invertible with $U^{-1} =
U^*$); Maximal Compact Subgroup of $GL(V)$ (FIXME: correct in general ???)

$Sp(V)$ -- Symplectic Group (\S\ref{sec:symplectic_group}): Preserves a
Symplectic Form (\S\ref{sec:symplectic_form}) on $V$ (i.e. a Non-degenerate
Alternating Form)

$SO(V)$ -- Special Orthogonal Group (\S\ref{sec:orthogonal_group}): Group of
Orthogonal Matrices of Determinant $+1$

\begin{itemize}
  \item the Integral (\S\ref{sec:integral}) is a Linear Operator (FIXME:
    explain)
  \item every Integral Transform (\S\ref{sec:integral_transform}) is a Linear
    Operator, and if the Kernel is allowed to be a Generalized Function
    (\S\ref{sec:generalized_function}), then all Linear Operators are Integral
    Transforms (\emph{Schwartz Kernel Theorem})
  \item the Laplace Transform (\S\ref{sec:laplace_transform})
\end{itemize}

Similar Matrices (\S\ref{sec:matrix_similarity}) represent the same Linear
Operator under two possibly different Bases with $P$ being a Change of Basis
Matrix (\S\ref{sec:change_of_basis})

\fist Operator Theory (\S\ref{sec:operator_theory}): study of Linear Operators
on Function Spaces (\S\ref{sec:function_space})



\paragraph{Isospectral}\label{sec:isospectral}\hfill

two Linear Operators are \emph{Isospectral} (or \emph{Cospectral}) if they have
the same Spectrum (\S\ref{sec:spectrum})



\paragraph{Identity Operator}\label{sec:identity_operator}\hfill

\paragraph{Composition Operator}\label{sec:composition_operator}\hfill

or \emph{Koopman Operator}

Dynamic Mode Decomposition (\S\ref{sec:dmd})



\paragraph{Shift Operator}\label{sec:shift_operator}\hfill

or \emph{Translation Operator}

$T_h$



\paragraph{Linear Projection}\label{sec:linear_projection}\hfill

A \emph{Linear Projection} $P$ is an Idempotent Linear Operator, i.e. $P^2 = P$.

Singular Matrix (\S\ref{sec:singular_matrix}) FIXME

the Columns of $I - P$ Span $\mathrm{null}(P)$

$P(I - P) = 0$

$(I - P)$ is also a Projection

$P$ gives a ``Decomposition'' of a Vector $\vec{x}$ into Components in the
Range and Nullspace of $P$:
\[
  \vec{x} = P\vec{x} = (I - P)\vec{x}
\]

$P$ is an Orthogonal Projection (\S\ref{sec:orthogonal_projection}) if
$P\vec{x}$ and $(I-P)\vec{x}$ are Orthogonal

\fist cf. the ``\emph{Projection Matrix}'' (\emph{Influence Matrix} or \emph{Hat
  Matrix} \S\ref{sec:influence_matrix}) in Statistics



\subparagraph{Orthogonal Projection}\label{sec:orthogonal_projection}\hfill

in an Orthogonal Projection, the Direction of the Projection is Orthogonal to
the Image (Column Space) of the Projection

$P$ is an Orthogonal Projection if $P\vec{x}$ and $(I-P)\vec{x}$ are Orthogonal

Thm. \emph{A Matrix $P \in \comps^{n \times n}$ is an Orthogonal Projection if
  and only if $P^2 = P$ and $P = P^*$}

i.e. an Orthogonal Projection is exactly an Idempotent, Symmetric (Hermitian)
Linear Operator

for an $n$-dimensional Unit Vector $\hat{u}$, the $n \times n$ Orthogonal
Projection Matrix $P_u$ can be computed by the Outer Product:
\[
  P_u = \hat{u}\hat{u}^T
\]
(if $\hat{u}$ is Complex the Transpose is the Hermitian Transpose) which
Projects any given Vector onto the Line defined by $\hat{u}$

\begin{itemize}
  \item Vector Projection (\S\ref{sec:vector_projection})
\end{itemize}

(wiki):

An \emph{Orthogonal Projection} on a Hilbert Space (\S\ref{sec:hilbert_space}),
i.e. a Complete Vector Space with an Inner Product,
$W$ is a Projection for which the Range (Column Space \S\ref{sec:column_space})
$U$ and the Nullspace (\S\ref{sec:nullspace}) $V$ are \emph{Orthogonal
  Subspaces} (\S\ref{sec:orthogonal_subspace}), i.e. every Vector in $U$ is
Orthogonal (\S\ref{sec:orthogonality}) to every Vector in $V$:
\[
  Px \bot y - Py
\]

A Projection is Orthogonal if and only if it is Self-adjoint
(\S\ref{sec:self_adjoint_operator}).

an Orthogonal Projection is a Bounded Linear Operator
(\S\ref{sec:bounded_linear_operator})


UC Math 352 \url{https://www.youtube.com/watch?v=3HS-BRbJOd0}:

\emph{Orthogonal Projection onto an arbitrary Basis}

for Set of Linearly Independent Vectors $\vec{a}_1, \ldots, \vec{a}_n$

Orthogonal Projection onto $\mathrm{span}\{\vec{a}_1, \ldots, \vec{a}_n\}$

Define $A = [\vec{a}_1 \cdots \vec{a}_n] \in \comps^{m \times n}, m \geq n$

($A$ is Full Rank because the Column Vectors are Linearly Independnt)

from the Reduced QR Decomposition (\S\ref{sec:qr_decomposition}):
\[
  A = \hat{Q}\hat{R}
\]
one can Define the Orthogonal Projection $P$:
\[
  P = \hat{Q}\hat{Q}^*
\]


\url{https://www.youtube.com/watch?v=ZWGIchXVbho}:

Orthogonal Projection onto the Column Space of $A$:
\[
  P = A(A^TA)^{-1}A^T
\]



\paragraph{Continuous Linear Operator}\label{sec:continuous_linear}\hfill

A \emph{Continuous Linear Operator} is a Continuous
(\S\ref{sec:continuous_map}) Linear Operator between Topological
Vector Spaces (\S\ref{sec:topological_vectorspace})

a C$^*$-algebra (\S\ref{sec:cstar_algebra}) is a Complex Algebra $A$
of Continuous Linear Operators on a Complex Hilbert Space
(\S\ref{sec:hilbert_space}) with the additional Properties that $A$ is
Topologically Closed in the Norm Topology of Operators and Closed
under the Operation of taking Adjoints of Operators

(wiki):

a Linear Operator on a Normed Vector Space
(\S\ref{sec:normed_vectorspace}) is Continuous if and only if it is a
Bounded Linear Operator (\S\ref{sec:bounded_linear_operator}), e.g.
when the Domain is Finite-dimensional: every Linear Operator on a
Finite-dimensional Space is Continuous



\paragraph{Density Operator}\label{sec:density_operator}\hfill

(Witten18): the Quantum analog of a Classical Probability Distribution
(\S\ref{sec:probability_distribution}) is a \emph{Density Matrix}
(\S\ref{sec:density_matrix})-- a representation of the Density Operator,
obtained from the Density Operator by a choice of Basis in the underlying Space

Self-adjoint (Hermitian), Positive Semi-definite, Trace One, and may be
Infinite-dimensional

describes the Statistical State of a Quantum System
(\S\ref{sec:quantum_system}), including Mixed States



\subsubsection{Linear Form}\label{sec:linear_form}

A \emph{Linear Form} (or \emph{Linear Functional} or \emph{Covector}) is a
Linear Map from a Vector Space to its Field of Scalars viewed as a Vector Space
over itself.

For a Vector Space $V$ over a Field $k$, a Linear Functional $f$ is a Function
from $V$ to $k$ that is \emph{Linear}:
\begin{itemize}
\item $f(\vec{v} + \vec{w}) = f(\vec{v}) + f(\vec{w})$ for all $\vec{v},
  \vec{w} \in V$
\item $f(a\vec{v} = af(\vec{v}$ for all $\vec{v} \in V, a \in k$
\end{itemize}

The action of a Linear Functional on a Vector is given by the Dot Product
(\S\ref{sec:inner_product}), or the Matrix Product with a Row Vector on the
Left and a Column Vector on the right.

The \emph{(Algebraic) Dual Space} (\S\ref{sec:dual_space}) of a Vector Space
$V$ is the Set of all Linear Forms on $V$ together with the Vector Space
structure of Pointwise Addition and Multiplication by Constants.

the Space of Linear Functionals is Isomorphic to $V$ itself:
$V \multimap K \cong V$

A \emph{Bilinear Form} (\S\ref{sec:bilinear_form}) is a Bilinear Map $V \times
V \rightarrow K$ (i.e. Linear in each argument separately).

Distribution (\S\ref{sec:distribution})

\fist a Tensor is a collection of \emph{Vectors} and \emph{Covectors} combined
using \emph{Tensor Product}



\subsubsection{Semilinear Map}\label{sec:semilinear_map}

(or \emph{Semilinear Transformation})

generalizes the class of Antilinear Maps (\S\ref{sec:antilinear_map})



\paragraph{Antilinear Map}\label{sec:antilinear_map}\hfill

or \emph{Conjugate-linear Map}

is a Mapping $f : V \rightarrow W$ between Complex Vector Spaces $V$ and $W$
when:
\[
  f (ax+by) = \bar{a}f(x) + \bar{b}f(y)
\]
for all Complex Numbers $a,b \in \comps$ with Complex Conjugates $\bar{a},
\bar{b}$ and all $x,y \in V$

may be equivalently described in terms of the Linear Map $\bar{f} : V
\rightarrow \bar{W}$ from $V$ to the Complex Conjugate Vector Space $\bar{W}$

A Sesquilinear Form (\S\ref{sec:sesquilinear_form}) is a Bilinear Form where
the Codomain $K$ is the Field of Complex Numbers and the mapping is Antilinear
in one argument.


Quantum mechanics: Time Reversal

Spinor Calculus

\fist Antiunitary Operator (\S\ref{sec:antiunitary_operator})



\subsubsection{Short Linear Map}\label{sec:short_linear}

$\cat{Hilb}$



\subsubsection{Eigenvector}\label{sec:eigenvector}

An \emph{Eigenvector} or \emph{Characteristic Vector} of a Linear
Transformation is a Non-zero Vector whose Direction does not change when the
Linear Transformation is applied to it:
\[
  A\vec{v} = \lambda\vec{v}
\]
where $\lambda$ is the \emph{Eigenvalue} corresponding to $\vec{v}$.

An \emph{Eigendecomposition} (\S\ref{sec:eigendecomposition}) of a Matrix is a
Factorization of the Matrix in terms of its Eigenvalues and Eigenvectors and
only Diagonlizable Matrices (\S\ref{sec:diagonalizable_matrix}) can be Factored
in this way.

\fist Linear Dynamical Systems (\S\ref{sec:linear_dynamical_system}):
$\dot{\vec{p}} = A\vec{p}$ always has a Fixed Point at the Origin with Phase
Portrait (\S\ref{sec:phase_plane}) around the Origin given by the Eigenvectors
and Eigenvalues of $A$

\fist Spectral Theory (\S\ref{sec:spectral_theory}): extends Eigenvectors and
Eigenvalues to Elements of a Banach Algebra (\S\ref{sec:banach_algebra})



\paragraph{Eigenvalue}\label{sec:eigenvalue}\hfill

The \emph{Eigenvalue} (or \emph{Characteristic Value} or \emph{Characteristic
  Root}) of an Eigenvector $\vec{v}$ with respect to a Linear Operator $T$, is a
Scalar $\lambda$ such that:
\[
  T(\vec{v}) = \lambda\vec{v}
\]

the Function for the maximum Eigenvalue of a Symmetric Matrix $X \in
\mathsf{S}^n$ is a Convex Function (\S\ref{sec:convex_function}):
\[
  \lambda_{max}(X) = \mathrm{sup}_{\|\vec{y}\|_2=1} \vec{y}^T X \vec{y}
\]



\subparagraph{Matrix Spectrum}\label{sec:matrix_spectrum}\hfill

the \emph{Spectrum} of a Matrix is the Set of its Eigenvalues

\fist \emph{Spectrum} (Analysis \S\ref{sec:spectrum}): generalization of Matrix
Spectrum general \emph{Linear Operators} (\S\ref{sec:linear_operator})

\fist Spectral Graph Theory (\S\ref{sec:spectral_graph_theory}): two Graphs are
Isospectral (\S\ref{sec:isospectral}) if their Adjacency Matrices
(\S\ref{sec:adjacency_matrix}) have the same Spectrum


\textbf{Spectral Gap}

the \emph{Spectral Gap} is the difference between the Moduli (Absoute Values) of
the two largest Eigenvalues of a Matrix (or Operator)

2015 - Cubitt, Perez-Garcia, Wolf - \emph{Undecidability of the Spectral Gap} --
with respect to ``emergent behavior'' in Physics
(\url{http://lambda-the-ultimate.org/node/5550}):

``For example, our results show that adding even a single particle to a lump of
matter, however large, could in principle dramatically change its properties.``

discrete case: Expander Graph

continuous case: Poincare Inequality



% ------------------------------------------------------------------------------
\subsection{Locally Linear Transformation}\label{sec:locally_linear}
% ------------------------------------------------------------------------------

%FIXME: move this section ?

(FIXME: what is the criterea for a transformation betwen vector spaces to be
``locally linear'')

\fist cf. Local Linearization (\S\ref{sec:linearization})

\fist Jacobian (\S\ref{sec:jacobian})



% ------------------------------------------------------------------------------
\subsection{Additive Map}\label{sec:additive_map}
% ------------------------------------------------------------------------------

$f(x + y) = f(x) + f(y)$

Special case of Subadditive Function (\S\ref{sec:subadditive_function})

Norm (\S\ref{sec:norm})



% ------------------------------------------------------------------------------
\subsection{Free Module}\label{sec:free_module}
% ------------------------------------------------------------------------------

A \emph{Free Module} is a Freely Generated (\S\ref{sec:free_object}) Module
over a given Basis (\S\ref{sec:basis}), i.e. a Generating Set
(\S\ref{sec:generating_set}).

(wiki):

every Vector Space is a Free Module, but if the Ring of Coefficients is not a
Division Ring (not a \emph{Field} in the Commutative case), then there exist
Non-free Modules

Free Vector Space



\subsubsection{Group Ring}\label{sec:group_ring}

\subsubsection{Free Abelian Group}\label{sec:free_commutative_group}

a Free Abelian Group is exacly a Free Module over the Ring of Integers $\ints$



\paragraph{Formal Sum}\label{sec:formal_sum}\hfill



% ------------------------------------------------------------------------------
\subsection{Torsion}\label{sec:torsion}
% ------------------------------------------------------------------------------

(wiki):

an Element $m$ of an $R$-module $M$ is a \emph{Torsion Element} if there exists
a Regular Element (i.e. \emph{not} a Zero Divisor \S\ref{sec:zero_divisor})
$r \in R$ such that $r m = 0$
(FIXME: is this specifically for a \emph{Left $R$-module} ???)

an $R$-module $M$ is a \emph{Torsion Module} if \emph{all} its Elements are
Torsion Elements

an $R$-module $M$ is \emph{Torsion-free} if $0$ is the only Torsion Element

Torsion-free Rank (\S\ref{sec:torsionfree_rank})

Scalar Field (\S\ref{sec:scalar_field}) $\tau$; Fundamental Theorem of Curves



% ------------------------------------------------------------------------------
\subsection{Bimodule}\label{sec:bimodule}
% ------------------------------------------------------------------------------

A \emph{Bimodule} is an Abelian Group (\S\ref{sec:commutative_group}) that is
both a Left and a Right Module such that the Left and Right Multiplications are
compatible.

Rings $R$, $S$, $R$-$S$-bimodule is an Abelian Group $M$ such that:

\begin{enumerate}
\item $M$ is a Left $R$-module and a Right $S$-module
\item $\forall r \in R, s \in S, m \in M, (rm)s = r(ms)$
\end{enumerate}

An $R$-$R$-bimodule is known as an $R$-bimodule

Generalization of Algebra Homomorphism
(\S\ref{sec:algebra_homomorphism})

Categorical generalization: Profunctor (\S\ref{sec:profunctor})



% ==============================================================================
\section{Vector Space}\label{sec:vector_space}
% ==============================================================================

or \emph{Linear Space}

\emph{Vector} (\S\ref{sec:vector}); a \emph{Position Vector} is a Vector based
at the Origin; a \emph{Free Vector} is equivalent to a Position Vector of the
same Direction and Magnitude

\emph{Span}

\emph{Finite-dimensional Vector Space} -- has a Span; all
Finite-dimensional Vector Spaces are Nuclear Spaces
(\S\ref{sec:nuclear_space})

\emph{Infinite Dimensional Vector Space} -- does not have a Span

\emph{Basis} (\S\ref{sec:basis}) - Spans and is Linearly Independent

Vector Space as a Fiber Bundle (\S\ref{sec:fiber_bundle}) on a Set of Basis
Vectors

Field (\S\ref{sec:field})

a Vector Space can be seen as a ``Field Action'' on a Commutative Group cf. a
Module (\S\ref{sec:module}) can be seen as a Monoid Action
(\S\ref{sec:monoid_action}) on an Commutative Group

given a Field Extension (\S\ref{sec:field_extension}) $L/K$, the Extension Field
$L$ is a $K$-vector Space

A Cartesian Space (\S\ref{sec:cartesian_space}) $\reals^n$ is canonically a
Vector Space over the Field of Real Numbers.

every Vector Space is a \emph{Free Vector Space}: every Vector Space
looks like a Function $X \rightarrow R$ where $R$ is a Field and $X$
some Set of Basis Vectors, i.e. Vectors can be decomposed into a sum
of Scalars times the choice of Basis Vectors %FIXME

every Vector Space is a Free Module (\S\ref{sec:free_module}), but if the Ring
of Coefficients is not a Division Ring (not a \emph{Field} in the Commutative
case), then there exist Non-free Modules

every Vector Space may be considered as an Affine Space
(\S\ref{sec:affine_space}) over itself, $(V,V)$, allowing every element of $V$
to be considered either as a Point or as a Vector

the Set of One-dimensional (Linear) Subspaces (\S\ref{sec:linear_subspace}) of a
$n$-dimensional Vector Space $V$ for some Finite $n$ is the $n-1$-dimensional
\emph{Projective Space} (\S\ref{sec:projective_space}) of $V$

All Bases of a Vector Space $\mathbf{V}$ have the same number of
Elements equal to the \emph{Dimension} of $\mathbf{V}$,
$dim(\mathbf{V})$. The Dimension of a Vector Space is uniquely defined
because for any Vector Space, a Basis exists, and all Bases of a
Vector space have equal Cardinality (\S\ref{sec:cardinality}).

For a Finite Dimensional Vector Space a Subset of a Span defines a
Basis, and a Linearly Independent Subset can be extended to form a
Basis.

The number of Elements in a Spanning Subset of $\mathbf{V}$ is greater
than or equal to the Dimension of $\mathbf{V}$.

The number of Elements in a Linearly Independent Subset of
$\mathbf{V}$ is less than or equal to the Dimension of $\mathbf{V}$.

A Basis defines an Isomorphism of Vector Spaces:
\[
    \mathbf{V} \xrightarrow{f} F^n
\]

\emph{Tensor Product}, \emph{Outer Product} (\S\ref{sec:outer_product})

Scalar (0th-order Tensor \S\ref{sec:scalar}) is an Element of the
Field used to define a Vector Space

Vector (1st-order Tensor \S\ref{sec:vector}) is an Element of a Vector
Space

Functional Analysis: Sequence Space (\S\ref{sec:sequence_space})

a Euclidean Vector Space with the Group Operation of Vector Addition is an
example of a Non-compact (\S\ref{sec:compact_space}) Lie Group
(\S\ref{sec:lie_group})

2019 - Greco, Liang, Moortgat, Palmigiano - \emph{Vector Spaces as Kripke
  Frames}: a Sound Semantics for Associative, Commutative, Unital Lambek
Calculus (\S\ref{sec:lambek_grammar}) can be based on Vector Spaces by
interpreting Fusion as the Tensor Product of Vector Spaces



% ------------------------------------------------------------------------------
\subsection{Linear Combination}\label{sec:linear_combination}
% ------------------------------------------------------------------------------

\fist Affine Combinations (\S\ref{sec:affine_combination}) -- a Linear
Combination where the coefficients sum to $1$

\fist Convex Combinations (Convex Geometry \S\ref{sec:convex_combination}) -- an
Affine Combination where the coefficients are non-negative

\emph{Linear Combination Theorem}: The GCD (\S\ref{sec:gcd}) of two non-zero
Integers $a$ and $b$ is equal to the smallest possible Positive Linear
Combination of $a$ and $b$.



\subsubsection{Linear Independence}\label{sec:linear_independence}

a Set of Vectors are \emph{Linearly Independent} if no Vector in the Set can be
defined as a Linear Combination of the others

\fist Affine Independence (\S\ref{sec:affine_independence})

\fist cf. \emph{Matroids} (Combinatorics \S\ref{sec:matroid})-- generalizes
Linear Independence



% ------------------------------------------------------------------------------
\subsection{Fundamental Theorem of Linear Algebra}
\label{sec:fundamental_linear_algebra_theorem}
% ------------------------------------------------------------------------------

1993 - Strang - \emph{The Fundamental Theorem of Linear Algebra}

An $m \times n$ Matrix $A$ of Rank (\S\ref{sec:rank}) $r$ with Singular
Value Decomposition (\S\ref{sec:svd}):
\[
  A = U \Sigma V^*
\]
induces \emph{Four Fundamental Subspaces} (\S\ref{sec:fundamental_subspace}):
\begin{itemize}
  \item \emph{Column Space} (\S\ref{sec:column_space}) -- the \emph{Image},
    $im(A)$, of Dimension $r$ (Rank \S\ref{sec:rank})
  \item \emph{Row Space} (\S\ref{sec:row_space}) -- the \emph{Coimage},
    $coim(A)$, of Dimension $r$ (Rank)
  \item \emph{Nullspace} (\S\ref{sec:nullspace}) -- the \emph{Kernel},
    $ker(A)$, of Dimension $n - r$ (Nullity \S\ref{sec:nullity})
  \item \emph{Left Nullspace} (\S\ref{sec:left_nullspace}) -- the
    \emph{Cokernel}, $coker(A)$, of Dimension $m - r$ (Corank
    \S\ref{sec:corank})
\end{itemize}
such that:
\begin{enumerate}
  \item $ker(A) = coim(A)^\bot$ -- the Nullspace is the Orthogonal Complement
    (\S\ref{sec:orthogonal_complement}) of the Row Space
  \item $coker(A) = im(A)^\bot$ -- the Left Nullspace is the Orthogonal
    Complement of the Column Space
\end{enumerate}



% ------------------------------------------------------------------------------
\subsection{Linear Span}\label{sec:linear_span}
% ------------------------------------------------------------------------------

The \emph{Linear Span} (or \emph{Linear Hull} or \emph{Span}) of a Set of
Vectors in a Vector Space is the Intersection of all Linear Subspaces
(\S\ref{sec:linear_subspace}) containing that Set.

A \emph{Basis} (\S\ref{sec:basis}) of a Vector Space is a Linearly Independent
Spanning Set.

The \emph{Rank} (\S\ref{sec:rank}) of a Matrix is the Dimension of the Vector
Space Spanned by its Columns, i.e. the maximal number of Linearly Independent
Columns of the Matrix.

generalized to Matroids (\S\ref{sec:matroid}) and Modules (\S\ref{sec:module})
%FIXME explain

Closed Linear Span (\S\ref{sec:closed_linear_span})

\fist Root Systems (\S\ref{sec:root_system}) -- Set of Spanning Vectors of a
Euclidean Vector Space with special properties



\subsubsection{Basis}\label{sec:basis}

A Set of Vectors in a Vector Space is a \emph{Basis} if the Vectors are
\emph{Linearly Independent} and therefore every Vector in the Vector Space is a
Linear Combination of the Set, that is, a \emph{Linearly Independent Spanning
  Set} (\S\ref{sec:linear_span}).

a Basis is exactly a Generating Set (\S\ref{sec:generating_set}) of a Module
over a Field

\fist a \emph{Free Object} (\S\ref{sec:free_object}) is a direct generalization
to Categories of the notion of Basis in a Vector Space
(FIXME: clarify)

\fist Affine Basis (\S\ref{sec:affine_basis})

a Set of $n$ Vectors in an $n$-dimensional Vector Space are Linearly
Independent if and only if the Points they define in Projective Space
(\S\ref{sec:projective_space}) of Dimension $n-1$ are in General Linear
Position (\S\ref{sec:general_position})

to allow for Uncountable Bases, the Axiom of Choice
(\S\ref{sec:choice_axiom}) is required as it is equivalent to the statement
that ``every Vector Space has a Basis''
(\url{https://math.stackexchange.com/a/884332/239953})



\paragraph{Ordered Basis}\label{sec:ordered_basis}\hfill

(wiki):

definition of a Tensor using the Representations of the General Linear Group:
for the Action of the General Linear Group on the Set of all Ordered Bases of an
$n$-dimensional Vector Space (TODO), the Set of all Ordered Bases is a Principal
Homogeneous Space for $GL(n)$, and for a Vector Space $W$ and
$\rho : GL(n) \to GL(W)$ the Representation of $GL(n)$ on $W$ (a Group
Homomorphism), then a \emph{Tensor} of ``Type $\rho$'' is an Equivariant Map
$T : F \to W$, i.e.:
\[
  T(FR) = \rho(R^{-1})T(F)
\]
when $\rho$ is a \emph{Tensor Representation} of the General Linear Group, this
is the usual definition of Tensors as Multidimensional Arrays
--TODO: xrefs



\paragraph{Change of Basis}\label{sec:change_of_basis}\hfill

Similar Matrices (\S\ref{sec:matrix_similarity}) represent the same Linear
Operator under two possibly different Bases with $P$ being a Change of Basis
Matrix



\paragraph{Covariant Transformation}\label{sec:covariant_transformation}\hfill

Coordinate-free (\S\ref{sec:coordinate_free})

Covariant Transformation Law

change of Basis

precise Transformation Law determines the Valence
(\S\ref{sec:valence}) of a Tensor (\S\ref{sec:linear_tensor})



\paragraph{Orthogonal Basis}\label{sec:orthogonal_basis}\hfill

\emph{Gram-Schmidt Process}: method for Orthogonalizing a Set of Linearly
Independent Vectors in an Inner Product Space



\subparagraph{Orthonormal Basis}\label{sec:orthonormal_basis}\hfill

a Basis for an Inner Product Space consisting of an Orthonormal
(\S\ref{sec:orthonormality}) Set of Vectors

\begin{itemize}
  \item an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix}) is an $n \times n$
    Matrix with Columns forming an Orthonormal Basis for Euclidean Space
    $\reals^n$
  \item the Sines and Cosines in the Fourier Series
    (\S\ref{sec:fourier_series}) form an Orthonormal Basis
\end{itemize}



\paragraph{Algebraic Basis}\label{sec:algebraic_basis}\hfill

or \emph{Hamel Basis}

Linear Combinations that are Finite Sums

a Hamel Bases for $\reals$ over $\rats$ are Uncountable



\paragraph{Countable Basis}\label{sec:countable_basis}\hfill

or \emph{Shauder Basis}

Linear Combinations that may be Infinite Sums

Basis for $\reals^\reals$ -- Cardinality
$|B| = |\reals^\reals| = 2^\mathfrak{c} = \beth_2$



% ------------------------------------------------------------------------------
\subsection{Linear Subspace}\label{sec:linear_subspace}
% ------------------------------------------------------------------------------

A \emph{Linear Subspace} (or \emph{Vector Subspace}) is a Subset of a Vector
Space that is Closed under Addition and Scalar Multiplication. For Field $K$
and $V$ a Vector Space over $K$, a Subset $W \subset K$ is a \emph{Subspace} of
$V$ if:
\begin{enumerate}
  \item $\vec{0} \in W$ -- the Zero Vector is in $W$
  \item $\vec{u}, \vec{v} \in W \Rightarrow \vec{u} + \vec{v} \in W$
    -- $W$ is Closed under Vector Addition
  \item $c\vec{u} \in W \forall c \in K \vec{u} \in W$
    -- $W$ is Closed under Scalar Multiplication
\end{enumerate}

a $1$-dimensional Linear Subspace may be called a ``\emph{Vector Line}'', and a
$2$-dimensional Linear Subspace may be called a ``\emph{Vector Plane}''

\fist Fundamental Subspaces (\S\ref{sec:fundamental_subspace})

The Grassmanians (\S\ref{sec:grassmanian}) of a Vect or Space $V$ are
$k$-dimensional parameterizations of the Linear Subspaces
(\S\ref{sec:linear_subspace}) of $V$. When $V$ is a Real or Complex Vector
Space, the Grassmanians of $V$ are Compact Smooth Manifolds
(\S\ref{sec:smooth_manifold}).

\fist an \emph{$n$-dimensional Projective Space} (\S\ref{sec:projective_space})
is the Set of $1$-dimensional Linear Subspaces in an $n+1$ Dimensional Vector
Space

\fist Quanum Logic (\S\ref{sec:quantum_logic}): Propositional Calculus
corresponding to a Calculus of Linear Subspaces with respect to Set Products,
Linear Sums, and Orthogonal Complements



\subsubsection{Orthogonal Subspace}\label{sec:orthogonal_subspace}

Two Linear Subspaces $A, B \subset V$ of an Inner Product Space $V$ are called
\emph{Orthogonal Subspaces} if each Vector in $A$ is Orthogonal to each Vector
in $B$.

The largest Subspace of $V$ that is Orthogonal to a given Subspace is its
Orthogonal Complement (\S\ref{sec:orthogonal_complement}).

An Orthogonal Projection (\S\ref{sec:orthogonal_projection}) on a Hilbert Space
(\S\ref{sec:hilbert_space}) $W$ is a Projection for which the Range (Column
Space \S\ref{sec:column_space}) $U$ and the Nullspace (\S\ref{sec:nullspace})
$V$ are Orthogonal Subspaces.



\paragraph{Orthogonal Complement}\label{sec:orthogonal_complement}\hfill

The \emph{Orthogonal Complement} of a Linear Subspace $W \subset V$ with a
Bilinear Form (\S\ref{sec:bilinear_form}) $B$ is the Set $W^\bot$ of all
Vectors in $V$ that are Orthogonal to every Vector in $W$ and is itself a
Linear Subspace of $V$. The Orthogonal Complement of $W$ is the largest
Subspace of $V$ that is Orthogonal to $W$.



% ------------------------------------------------------------------------------
\subsection{Homogeneous Function}\label{sec:homogeneous_function}
% ------------------------------------------------------------------------------

Scale Invariance (\S\ref{sec:scale_invariance})

for a Function $f : V \rightarrow W$ between Vector Spaces $V$ and $W$ over a
Field $F$, for an Integer $n$, $f$ is \emph{Homogeneous of Degree $n$} if:
\[
  f(k{\vec{v}}) = k^n f(\vec{v})
\]
for all Nonzero $k \in F$ and $\vec{v} \in V$

Homogeneous Polynomials (\S\ref{sec:homogeneous_polynomial})

Homogeneous Distributions (\S\ref{sec:homogeneous_distribution})



% ------------------------------------------------------------------------------
\subsection{Scalar Field}\label{sec:scalar_field}
% ------------------------------------------------------------------------------

%FIXME: move scalar, vector, tensor fields to vector calculus?

Gradient (\S\ref{sec:gradient}): the Vector Derivative of a Scalar Field

cf. Vector Field (\S\ref{sec:vector_field})

cf. Tensor Field (\S\ref{sec:tensor_field})

Line Integral of Parametric Curves (\S\ref{sec:parametric_curve}) in a Scalar
Field; unlike Line Integrals of Curves in Vector Fields, the Line Integral in a
Scalar Field is \emph{independent} of Path direction

when a Vector Field is the \emph{Gradient Field} (\S\ref{sec:gradient}) of some
Scalar Field, it is a \emph{Conservative} (\emph{Path Independent}
\S\ref{sec:conservative_vector_field}) Vector Field and the Line Integral of
any Path between two Points is equivalent and the Line Integral of any Closed
Path from a Point to itself is Zero

\begin{itemize}
  \item Curvature (\S\ref{sec:curvature})
  \item Torsion (\S\ref{sec:torsion})
  \item ...
\end{itemize}



\subsubsection{Scalar-valued Function}\label{sec:scalar_function}

\emph{Scalar-valued Function}

(wiki):

for a Multivariate Scalar-valued Function $f(x_1, \ldots, x_n)$ on $\reals^n$
for which a Partial Derivative $\pderiv{f}{x_j}$ exists for each Variable $x_j$
at a Point $a = (a_1, \ldots, a_n)$ the Partial Derivatives define the
\emph{Gradient Vector} (\S\ref{sec:gradient}) of $f$ as $a$:
\[
  \nabla{f}(a_1, \ldots, a_n) = \big(
    \pderiv{f}{x_1}(a_1, \ldots, a_n), \ldots, \pderiv{f}{x_n}(a_1, \ldots, a_n)
  \big)
\]
and if $f$ is Differentiable at every Point in some Domain, then the Gradient
determines a Vector Field (\S\ref{sec:vector_field})



% ------------------------------------------------------------------------------
\subsection{Vector Field}\label{sec:vector_field}
% ------------------------------------------------------------------------------

a Vector Field on a Manifold $M$ is a Smooth Map $V : M \rightarrow T M$ where
$TM$ is the Tangent Bundle (\S\ref{sec:tangent_bundle}) of $M$, i.e. a Cross
Section (\S\ref{sec:cross_section}) of $TM$

\fist a Vector Field implies there a Vector Space at each Point, i.e. a Vector
Bundle (\S\ref{sec:vector_bundle})

\fist Waves (\S\ref{sec:wave})

the Set of all Vector Fields on $M$ is denoted $\Gamma(TM)$ and has the
structure of a Module over the Commutative Algebra of Smooth Functions on $M$,
$C^\infty(M)$

\emph{Canonical Vector Field} (Liouville Vector Field
\S\ref{sec:liouville_vector_field}) on $TM$:
\[
  V : TM \rightarrow TTM
\]
as the Diagonal Map on the Tangent Space at each Point

FIXME: does a Vector Field require that the Domain and Range have the same
Dimension ???

\begin{equation*}
  \vec{v}(x,y,z) = \begin{bmatrix}
    p(x,y,z) \\
    q(x,y,z) \\
    r(x,y,z)
  \end{bmatrix}
\end{equation*}

Partial Derivative (\S\ref{sec:partial_derivative}) ... TODO

Frobenius Theorem (\S\ref{sec:frobenius_theorem})

Vector Field Line Integral (\S\ref{sec:line_integral}) of a Parametric Curve
(\S\ref{sec:parametric_curve}) is a Sum of the Dot Products of the Derivative
of the Path Function $C = \vec{r}(t)$ with the Vectors in the Vector Field
$\vec{f}(x,y)$:
\[
  \int_C \vec{f} \bullet d\vec{r}
\]
represents the amount of ``Work'' done on a Path in the Field (Physics)
that unlike Line Integrals of Curves in a Scalar Field
(\S\ref{sec:scalar_field}), the Line Integral in a Vector Field is
\emph{dependent} on Path direction (it is Negative in the opposite direction)

the Gradient (\S\ref{sec:gradient}) $\nabla f$ of a Function $f$ gives a Vector
Field and a Vector Field is called \emph{Path Independent} (or
\emph{Conservative} \S\ref{sec:conservative_vector_field}) if it is equal to
the Gradient Field of a Scalar Field; if so the Line Integral of all Paths
between Points are equal and the Line Integral from a Point to itself (Closed
Path) is Zero

\fist an Affine Connection (\S\ref{sec:affine_connection}) \emph{Connects}
nearby Tangent Spaces (\S\ref{sec:tangent_space}) permitting Tangent Vector
Fields to be Differentiated (\S\ref{sec:derivative}) as if they were Functions
on the Manifold with Values in a fixed Vector Space

wiki:

Divergence (\S\ref{sec:divergence}) of a Vector Field: Scalar-valued Function
(\S\ref{sec:scalar_function}) associating a Scalar Value with each Point in
the Vector Field

Curl (\S\ref{sec:curl}) of a Vector Field: Vector Function
(\S\ref{sec:vector_function}) associating each Point in the Vector field with
the proportional ``on-axis'' Torque to which a ``tiny pinwheel'' would be
subjected if it were centered at the Point

a Vector Field attaches to every Point of a Manifold (\S\ref{sec:manifold}) a
Vector from the Tangent Space (\S\ref{sec:tangent_space}) at that Point in a
\emph{Smooth} (\S\ref{sec:smooth_function}) manner and such a Vector Field
defines a generalized Ordinary Differential Equation (ODE \S\ref{sec:ode}) on a
Manifold where a Solution to such an Equation is a Diefferentiable Curve on the
Manifold with Derivative at any Point equal to the Tangent Vector attached to
that Point by the Vector Field

\fist Vector Calculus (\S\ref{sec:vector_calculus})

cf. Scalar Field (\S\ref{sec:scalar_field})

cf. Tensor Field (\S\ref{sec:tensor_field}) -- Tensor Calculus
(\S\ref{sec:tensor_calculus})

Autonomous Vector Field (???)


MAE 5790 Lec. 8 - \url{https://www.youtube.com/watch?v=O2fcpxLT5wk}:

``Index'' of a Simple Closed Curve (\S\ref{sec:simple_closed_curve}) in a
Vector Field \fist cf. Winding Number (\S\ref{sec:winding_number})



\subsubsection{Vector Flow}\label{sec:vector_flow}

Flow (\S\ref{sec:flow})

cf. Gradient Flow (\S\ref{sec:gradient_flow})

cf. Geometric Flow (\S\ref{sec:geometric_flow})



\paragraph{Flux}\label{sec:flux}\hfill

Flux as Flow Rate per Arclength \fist 2D Divergence Theorem (Green's Theorem
\S\ref{sec:greens_theorem})

Flux as Flow Rate per Unit Area \fist Surface Integral
(\S\ref{sec:surface_integral})

Flux Densities \fist (\S\ref{sec:volume_integral})

Stokes' Theorem (\S\ref{sec:stokes_theorem}), 3D Divergence Theorem



\subsubsection{Conservative Vector Field}\label{sec:conservative_vector_field}

a Vector Field is called \emph{Conservative} (or \emph{Path Independent}) if
the Line Integral (\S\ref{sec:line_integral}) of all Paths between Points are
equal and the Line Integral of a Closed Path (from a Point to itself) is Zero

this is true when the Vector Field is the \emph{Gradient Field}
(\S\ref{sec:gradient}) of some Scalar Field

\fist Green's Theorem (\S\ref{sec:greens_theorem})



% ------------------------------------------------------------------------------
\subsection{Covector Field}\label{sec:covector_field}
% ------------------------------------------------------------------------------

Cross Section of the Cotangent Bundle (FIxME: xref)



% ------------------------------------------------------------------------------
\subsection{Normed Vector Space}\label{sec:normed_vectorspace}
% ------------------------------------------------------------------------------

A \emph{Normed Vector Space} is a Vector Space $V$ over the Real or Complex
Numbers on which a \emph{Norm} (\S\ref{sec:norm}) $\|\cdot\| : V \rightarrow
\reals^{0\leq}$ which is a Real-valued Function defined on the Vector Space
formalizing the notion of \emph{Distance} between Vectors.

Any Normed Vector Space is a Metric Space (\S\ref{sec:metric_space}) by
defining the Metric (Distance Function \S\ref{sec:metric}) $d(x,y) = \|y-x\|$.

Any Inner Product Space (\S\ref{sec:innerproduct_space}) with Inner Product
$\langle{\cdot,\cdot}\rangle$ induces a Norm by:
\[
  \|\vec{v}\| = \sqrt{\langle{\vec{v},\vec{v}}\rangle}
\]
A Complete Inner Product Space is a Hilbert Space (\S\ref{sec:hilbert_space}).

Bounded Linear Operator (\S\ref{sec:bounded_linear_operator})

Functional Analysis (\S\ref{sec:functional_analysis})

\begin{itemize}
  \item Banach Space (\S\ref{sec:banach_space}) -- a Complete
    (\S\ref{sec:complete_metric_space}) Normed Vector Space; every
    Finite-dimensional Normed Vector Space over $\reals$ or $\comps$ is a Banach
    Space; every Hilbert Space (\S\ref{sec:hilbert_space}) is by definition a
    Banach Space
  \item ...
\end{itemize}



\subsubsection{Norm}\label{sec:norm}

The \emph{Norm} $\|\cdot\| : V \rightarrow \reals$ of a Normed Vector Space is
a Real-valued Function defined on the Vector Space with the properties:
\begin{enumerate}
  \item $0 \leq \|\vec{v}\|$ and
    $\|\vec{v}\| = 0 \Leftrightarrow \vec{v} = \vec{0}$
  \item $\|c\vec{v}\| = |c|\|\vec{v}\|$
  \item $\|\vec{u} + \vec{v}\| \leq \|\vec{u}\| + \|\vec{v}\|$ (\emph{Triangle
    Inequality})
\end{enumerate}

a Seminorm (\S\ref{sec:seminorm}) lacks the Property $|v| = 0 \Longrightarrow v
= 0$

\fist Matrix Norm (\S\ref{sec:matrix_norm})

the Vector Derivative (\S\ref{sec:vector_derivative}) represents a change of
\emph{Length} of the Vector

\emph{Seminorm}

\emph{Quasinorm}

Subadditive Function (\S\ref{sec:subadditive_function})

Norm Balls, Norm Cones

by definition, Norms are Convex Functions (\S\ref{sec:convex_function})



\paragraph{Triangle Inequality}\label{sec:triangle_inequality}\hfill

Real Line (\S\ref{sec:real_line}) with Absolute Value as Norm:\\
$|x + y| \leq |x| + |y|$



\paragraph{$p$-norm}\label{sec:p_norm}\hfill

\fist Metric (Distance Function \S\ref{sec:metric})

\fist $L^p$-space (\S\ref{sec:lp_space}) -- generalization of $p$-norm to
Function Spaces;
$\reals^n$ with the $p$-norm is a Banach Space called the
\emph{$L^p$-space over $\reals^n$}

\[
  \|\vec{x}\|_p = \Big(\sum_{i=1}^m|x_i|^p\Big)^{\frac{1}{p}}
\]

$1$-norm -- Taxicab Norm; Absolute Difference (\S\ref{sec:absolute_difference});
all components are ``weighted'' equally

$2$-norm -- \emph{Euclidean Norm}; large elements have ``more weighting''

$\infty$-norm -- Max Norm or \emph{Uniform Norm}; only the largest element
contributes to the length; Uniform Norm Topology
(\S\ref{sec:uniform_norm_topology}); \emph{Stone-Weierstrass Theorem}: the Set
of all Continuous Functions on a Closed Interval is the Uniform Closure of the
Set of Polynomials on the Interval;
the Uniform Closure of Complex Continuous Functions over a Compact Space is a
C$^*$ Algebra (\S\ref{sec:cstar_algebra})



\paragraph{Quadrance}\label{sec:quadrance}\hfill

the Squared Length of a Vector

may be computed by the self Dot Product (\S\ref{sec:inner_product}) of a Vector
with itself



\paragraph{Seminorm}\label{sec:seminorm}\hfill

a Seminorm lacks the Property $|v| = 0 \Longrightarrow v = 0$

\fist a Place (\S\ref{sec:place}) of a Commutative Unital Ring is an
Equivalence Class of ``Absolute Value'' (Non-trivial Multiplicative Seminorm)



\subsubsection{Inner Product Space}\label{sec:innerproduct_space}

An \emph{Inner Product Space} $V$ is a Normed Vector Space with Norm
(\S\ref{sec:norm}) $\|\cdot\|$ induced by the \emph{Inner Product}
(\S\ref{sec:inner_product}) $\langle{\cdot,cdot}\rangle : V \times V
\rightarrow \reals$ which is a Sesquilinear Form
(\S\ref{sec:sesquilinear_form}).

An Inner Product on a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}).

\fist Orthogonality (\S\ref{sec:orthogonality})

generalizes case of Euclidean Space (\S\ref{sec:euclidean_space}) to
Vector Spaces of any, possibly Infinite, Dimension

Functional Analysis (\S\ref{sec:functional_analysis})



\paragraph{Inner Product}\label{sec:inner_product}\hfill

associates each pair of Vectors in a Vector Space with a Scalar known
as the \emph{Inner Product} of the Vectors

two Vectors in an Inner Product Space are \emph{Orthogonal} when the Inner
Product is Zero, denoted $\vec{u} \bot \vec{y}$

in Euclidean Spaces the Inner Product is the Dot Product (aka Scalar Product
\S\ref{sec:dot_product})

Symmetric Bilinear Form (\S\ref{sec:symmetric_bilinear})

the Inner Product on a Real Vector Space is a Positive-definite Symmetric
Bilinear Form

An Inner Product induces a Norm (\S\ref{sec:norm}) $\|\cdot\|$ by:
\[
  \|\vec{v}\| = \sqrt\langle{\vec{v},\vec{v}}\rangle
\]

an Inner Product on a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}) and a Positive-definite Hermitian Form
(\S\ref{sec:hermitian_form})

Inner Product of Euclidean Vectors (\S\ref{sec:euclidean_vector}) is Positive
Definite (\S\ref{sec:positive_definite}):
\begin{align*}
  \langle{x,x}\rangle & \geq 0 \\
  \langle{x,x}\rangle & =    0 \Leftrightarrow x = \vec{0} \\
\end{align*}

the Dot Product of a Vector with itself gives the Squared Length or
\emph{Quadrance} (\S\ref{sec:quadrance}) of a Vector

$L^2$ Inner Product for Functions on an Interval $[a, b]$:
\[
  \langle{f(x), g(x)}\rangle_2 \defeq \int_a^b f(x) \overline{g(x)} dx
\]
(\url{https://www.youtube.com/watch?v=7ICYxBuS2iw})

A \emph{Hilbert Space} (\S\ref{sec:hilbert_space}) is an Inner Product Space
that is also a Complete Metric Space (\S\ref{sec:complete_metric_space}).

\fist Sliding Inner-product (Cross-correlation \S\ref{sec:cross_correlation})

\fist Gramian Matrix (\S\ref{sec:gramian_matrix}): Hermitian Matrix of Inner
Products

\emph{Correlation} (\S\ref{sec:statistical_correlation}) --
\url{https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/}:
given two Unit Vectors $v$, $w$ in a Real Inner Product Space, the
\emph{Correlation} between the Vectors is the Inner Product
$\langle{v,w}\rangle$; the Correlation of two non-constant Square-integrable
Real-valued Random Variables $X$ and $Y$ is the same as the Correlation between
two Unit Vectors $v$, $w$ in the Hilbert Space $L^2(\Omega)$ of
Square-integrable Random Variables, where $v$ is the Normalization of $X$
defined by subtracting the Mean (Expected Value) $E[X]$ and dividing by the
Standard Deviation of $X$, and similarly for $w$ and $Y$



\subparagraph{Cauchy-Schwartz Inequality}\label{sec:cauchy_schwarz}\hfill

\emph{Cauchy-Schwarz Inequality}:
\[
  |\langle{\vec{u},\vec{v}}\rangle|^2 \leq
    \langle{\vec{u},\vec{u}}\rangle \cdot \langle{\vec{v},\vec{v}}\rangle
\]
or equivalently:
\[
  |\langle{\vec{u},\vec{v}}\rangle| \leq \|\vec{u}\| \|\vec{v}\|
\]

special case of H\"older's Inequality (\S\ref{sec:holders_inequality})

application to Probability Theory:
\[
  E|XY|^2 \leq \sqrt{E(X^2)E(Y^2)}
\]
where Random Variables (\S\ref{sec:random_variable}) $X$ and $Y$ have Finite
Variances (\S\ref{sec:variance})



\paragraph{Orthonormality}\label{sec:orthonormality}\hfill

two Vectors in an Inner Product Space are \emph{Orthonormal} if they are
Orthogonal and Unit Vectors

\fist Orthogonality (\S\ref{sec:orthogonality})

\fist Orthonormal Basis (\S\ref{sec:orthonormal_basis})

an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix}) is a Real-valued Square
Matrix with Columns and Rows that are Orthonormal Vectors



\paragraph{Rotational Invariance}\label{sec:rotational_invariance}\hfill

\subparagraph{Isotropy}\label{sec:isotropy}\hfill

%FIXME: move this section ???

\fist Isotropy Group (\S\ref{sec:isotropy_group})



\paragraph{Unitary Space}\label{sec:unitary_space}\hfill

Inner Product Space over the Field of Complex Numbers



% ------------------------------------------------------------------------------
\subsection{Scalar Product Space}\label{sec:scalar_product_space}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Hermitian Product Space}\label{sec:hermitian_product_space}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Outer Product}\label{sec:outer_product}
% ------------------------------------------------------------------------------

Tensor Product (\S\ref{sec:tensor_product}) of two Vectors

of a $n$-dimensional Vector $\vec{v}$ and an $m$-dimensional Vector $\vec{u}$ is
given by the Matrix Multiplication:
\[
  \vec{v}\vec{u}^T
\]
of an $n \times 1$ Column Vector with a $1 \times m$ Row Vector, resulting in a
$n \times m$ Matrix

the Outer Product of two Nonzero Vectors is always a Matrix of Rank 1
(\S\ref{sec:rank}), i.e. the Dimension of the Subspace ``Spanned'' by the Matrix
is $1$-dimensional

\fist Orthogonal Projection (\S\ref{sec:orthogonal_projection})



% ------------------------------------------------------------------------------
\subsection{Real Vector Space}\label{sec:real_vector_space}
% ------------------------------------------------------------------------------

Vector Space over Scalar Field $F = \reals$

\fist \emph{Real Coordinate Space} (\S\ref{sec:real_coordinate_space})
$\reals^n$ -- prototypical Real Vector Space; Models Euclidean Space
(\S\ref{sec:euclidean_space}) with Cartesian Coordinates
(\S\ref{sec:cartesian_coordinates})

\begin{itemize}
  \item for any Manifold $M$ with $n$ Connected Components (Maximal Connected
    Subset Ordered by Inclusion \S\ref{sec:connected_space}), the $0$-th de Rham
    Cohomology Group (\S\ref{sec:derham_complex}) $H_{\mathrm{dR}}^0(M)$ is
    Isomorphic to $\reals^n$:
    \[
      H_{\mathrm{dR}}^0(M) \cong \reals^n
    \]
    following from the fact that any Smooth Function on $M$ with Zero Derivative
    (Locally Constant) is Constant on each of the Connected Components of $M$
\end{itemize}


% ------------------------------------------------------------------------------
\subsection{Complex Vector Space}\label{sec:complex_vector_space}
% ------------------------------------------------------------------------------

Vector Space over Scalar Field $F = \comps$



% ------------------------------------------------------------------------------
\subsection{Symplectic Vector Space}\label{sec:symplectic_vectorspace}
% ------------------------------------------------------------------------------

A Vector Space $V$ over a Field $F$ with a Non-degenerate Alternating
(Skew-symmetric) Bilinear Form called a \emph{Symplectic Bilinear Form}
(\S\ref{sec:symplectic_bilinear}) $\omega : V \times V \rightarrow F$.

If the underlying Field has Characteristic (\S\ref{sec:ring_characteristic})
$\neq 2$, Alternation is \emph{equivalent} to Skew-symmetry; for Characteristic
$=2$, the Skew-symmetry is \emph{implied by} (but does not imply)
\emph{Alternation}-- every Symplectic Form is a \emph{Symmetric Form}
(\S\ref{sec:symmetric_bilinear}) but not every Symmetric Form is Symplectic.

Given a choice of Basis, $\omega$ can be represented as a Skew-symmetric
(\S\ref{sec:skew_symmetric_matrix}), Invertible (Non-singular
\S\ref{sec:invertible_matrix}), Hollow (\S\ref{sec:hollow_matrix}) Matrix; if
$V$ is Finite-dimensional then it must be \emph{Even-dimensional} since every
Skew-symmetric Hollow Matrix of odd size has Determinant \emph{Zero} (i.e. is
Non-invertible).

note this Matrix is \emph{not} the same as a Symplectic Matrix (a Symplectic
Transformation of the Space TODO: xref)

\fist Symplectic Geometry (\S\ref{sec:symplectic_geometry})

``Black-box Functor'': Category of Circuits $\rightarrow$ Category of
Symplectic Vector Spaces % FIXME Baez 15' Passive Linear Networks

\url{https://golem.ph.utexas.edu/category/2018/04/props_in_network_theory.html}

in an Electrical Circuit, there is associated to each ``\emph{Wire}'' the
Attributes $\phi$ Potential and $I$ Current; Black-boxing a Circuit records
only the Relation imposed between Variables on its Input and Output Wires, that
is a Relation between Even-dimensional Vector Spaces (Pairs $(\phi,I)$); in
this case these Vector Spaces are \emph{Symplectic}, i.e. they are equipped
with a Nondegenerate Alternating (Skew-symmetric) Bilinear Form called a
\emph{Symplectic Form} \S\ref{sec:symplectic_form})

the Black-box Functor respects the Symplectic Structure: it is a
\emph{'Lagrangian' Relation} (\S\ref{sec:lagrangian_system})

for any sort of System goverened by a \emph{Minimum Principle} (e.g. Principle
of Least Action, Principle of Minimum Power), Black-boxing should give a
Functor to some Category where the Morphisms are Lagrangian Relations

$\blacksquare : \cat{Circ}_k \rightarrow \cat{LagRel}_k$

Black-boxing Non-linear Circuits, i.e. including Voltage and Current Sources,
gives Lagrangian Affine Relations between Symplectic Vector Spaces

2021 - Kissinger - \emph{A Graphical Calculus for Lagrangian Relations}



% ------------------------------------------------------------------------------
\subsection{Finite-dimensional Vector Space}
\label{sec:finite_dimensional_vectorspace}
% ------------------------------------------------------------------------------

Finite-dimensional Vector spaces of Equal Dimension are Isomorphic
%FIXME

Linear Transformations (\S\ref{sec:linear_transformation}) from an
$n$-dimensional Vector Space to an $m$-dimensional Vector Space are in
one-to-one correspondence with $m \times n$ Transformation Matrices
(\S\ref{sec:matrix_theory}).



% ------------------------------------------------------------------------------
\subsection{Infinite-dimensional Vector Space}
\label{sec:infinite_dimensional_vectorspace}
% ------------------------------------------------------------------------------

Cylinder Set Measure (\S\ref{sec:cylinder_set_measure})

Linear Operators (\S\ref{sec:linear_operator}) on an Infinite-dimensional Vector
Space are in one-to-one correspondence with Column-finite (Infinite) Matrices
(\S\ref{sec:matrix_theory}).



% ------------------------------------------------------------------------------
\subsection{Graded Vector Space}\label{sec:graded_vectorspace}
% ------------------------------------------------------------------------------

Graded Algebra (\S\ref{sec:graded_algebra})



\subsubsection{Differential Graded Vector Space}\label{sec:differential_graded}

a Chain Complex (\S\ref{sec:chain_complex}) in the Abelian Category
$\cat{Vect}_k$



% ------------------------------------------------------------------------------
\subsection{Dual Space}\label{sec:dual_space}
% ------------------------------------------------------------------------------

the \emph{Dual Space} (or \emph{Algebraic Dual Space}) of any Vector Space $V$
is the set of all Linear Functionals (Linear Forms \S\ref{sec:linear_form}) of
$V$ together with the Vector Space structure of Pointwise Addition and
Multiplication by Scalars

for a Topological Vector Space (\S\ref{sec:topological_vectorspace}), there is
a Subspace of the Dual Space corresponding to Continuous Linear Functionals
called the \emph{Continuous Dual Space} (\S\ref{sec:continuous_dual_space})

$V \cong V^*$

the Space of Linear Functionals is Isomorphic to $V$ itself:
$V \multimap K \cong V$

$V \cong V^**$ ``Naturally'' (\S\ref{sec:natural_transformation})

Contravariant Representable Functor
(\S\ref{sec:representable_functor}):
\[
  (-)^* = Vect(-,\mathbb{R}) :
    \mathbf{Vect}^op \rightarrow \mathbf{Vect}
\]

\[
  A^* = \pow(A) \cong \mathbf{Set}(A,2)
\]\cite{awodey06}

cf. Dual Numbers (\S\ref{sec:dual_number})

(FIXME: does the following actually relate to linear logic ???):

given a Vector Space $V$ over a Field $R$, the Dual Space $V^*$ is just the Set
of Linear Functions into $V \mapsto R$
--\url{https://cstheory.stackexchange.com/questions/39440/algebraic-account-of-gaussian-elimination}

Metric Tensor (\S\ref{sec:metric_tensor}) is a Non-degenerate Symmetric Bilinear
Form (\S\ref{sec:symmetric_bilinear}) on each Tangent Space that varies Smoothly
from Point to Point

the Dual Bundle (\S\ref{sec:dual_bundle}) of a Vector Bundle
(\S\ref{sec:vector_bundle}) is a Vector Bundle whose Fibers are the Dual Spaces
of the Fibers of the original Vector Bundle



% ------------------------------------------------------------------------------
\subsection{Relative Dimension}\label{sec:relative_dimension}
% ------------------------------------------------------------------------------

\subsubsection{Codimension}\label{sec:codimension}

cf. Fractal Co-dimension (\S\ref{sec:fractal_codimension})



% ------------------------------------------------------------------------------
\subsection{Vector Bundle}\label{sec:vector_bundle}
% ------------------------------------------------------------------------------

%FIXME: move to topological bundle ?

A \emph{Vector Bundle} is a Fiber Bundle (\S\ref{sec:fiber_bundle}) with Fibers
that are Vector Spaces.

The prototypical example of a Vector Bundle is the Tangent Bundle
(\S\ref{sec:tangent_bundle}) of Tangent Spaces (\S\ref{sec:tangent_space}) of a
Differentiable Manifold.

a Vector Field (\S\ref{sec:vector_field}) implies that there is a Vector Space
at each Point of the underlying Space, i.e. a Vector Bundle

a Topological construction making precise the idea of a Family of Vector Space
parameterized by another Space $X$ (e.g. a Topological Space, Manifold, or
Algebraic Veriety) associating for every point $x \in X$ a Vector Space $V(x)$
such that all the Vector Spaces ``fit together'' to form another Space of the
same kind as $X$, called the \emph{Vector Bundle over $X$}

the Dual Bundle (\S\ref{sec:dual_bundle}) of the Tangent Bundle is the
a Cotangent Bundle (\S\ref{sec:cotangent_bundle}) of
Cotangent Spaces (\S\ref{sec:cotangent_space})

the Pushforward (Differential \S\ref{sec:pushforward}) of a Smooth Map induces a
Vector Bundle Homomorphism (Bundle Map \S\ref{sec:bundle_map})



\subsubsection{Sub-bundle}\label{sec:subbundle}

\fist a Subbundle of the Tangent Bundle (\S\ref{sec:tangent_bundle}) of a
Smooth manifold is called a \emph{Distribution}
(\S\ref{sec:tangent_bundle_distribution}) of Tangent Vectors
(\S\ref{sec:tangent_space})

%FIXME: different frobenius theorem ?

\emph{Frobenius' Theorem} (\S\ref{sec:frobenius_theorem}): the Subbundle of the
Tangent Bundle of a Manifold is Integrable (or Involutive) if and only if it
arises from a \emph{Regular Foliation} (\S\ref{sec:foliation})



\subsubsection{Dual Bundle}\label{sec:dual_vectorbundle}

the \emph{Dual Bundle} of a Vector Bundle is a Vector Bundle whose Fibers are
Dual Spaces (\S\ref{sec:dual_space}) of the Fibers of the Vector Bundle

the Dual Bundle of the Tangent Bundle (\S\ref{sec:tangent_bundle}) of a the
Tangent Spaces of Differentiable Manifold is the a Cotangent Bundle
(\S\ref{sec:cotangent_bundle}) of the Cotangent Spaces of the Manifold



\subsubsection{Complex Vector Bundle}\label{sec:complex_vector_bundle}



% ==============================================================================
\section{Matrix Theory}\label{sec:matrix_theory}
% ==============================================================================

%FIXME: merge with matrix calculus ?

A \emph{Matrix} represents a \emph{Linear Map} (Linear
Transformation\S\ref{sec:linear_transformation}) between two Vector Spaces
(\S\ref{sec:vector_space}). Any Linear Map between Finite-dimensional Vector
Spaces $A : V \rightarrow W$ where $V$ has Dimension $n$ and $W$ has Dimension
$m$ can be represented as an $m \times n$ Matrix sending the Column Vector
$\vec{v} \in V$ to the Column Vector $A\vec{v} \in W$. Another way of viewing
this is that each of the components in the Vector $\vec{v}$ defines a
coefficient for the $n$th Column of $A$, and each of these scaled Column
Vectors are added together to produce the $n \times 1$ Vector in $W$, i.e.
Multiplication by $\vec{v}$ determines a \emph{Linear Combination} of the
Columns of $A$.

\fist Matrix (Second-order Tensor \S\ref{sec:matrix})

\fist Jacobian Matrix (Multivariable Calculus \S\ref{sec:jacobian})

\fist Matrix Calculus (\S\ref{sec:matrix_calculus})

\fist Matrix Difference Equation (\S\ref{sec:matrix_difference_equation})

\fist Fundamental Subspaces (\S\ref{sec:fundamental_subspace})

\emph{Column Vector} -- an $m \times 1$ Matrix

\emph{Row Vector} -- an $1 \times m$ Matrix

The Transpose of a Column Vector is a Row Vector, and the Transpose of a Row
Vector is a Column Vector.

The Set of all Row Vectors forms a Vector Space called the \emph{Row Space}
(\S\ref{sec:row_space}) and the Set of all Column Vectors forms a Vector Space
called the \emph{Column Space} (\S\ref{sec:column_space}).

FIXME: how do the 'row space' and 'column space' defined here relate to the
'row space' and 'column space' of a matrix ?

The Column Space can be viewed as the Dual Space (\S\ref{sec:dual_space}) to
the Row Space since any Linear Functional (Linear Form \S\ref{sec:linear_form})
on the Column Space can be represented uniquely as an Inner Product with a
specific Row Vector.

When Column and Row Vectors viewed as $m \times 1$ and $1 \times n$ Matrices,
respectively, a Row Vector times a Column Vector is the Dot Product
(Inner Product \S\ref{sec:inner_product}), resulting in a single Scalar value,
and a Column Vector times a Row Vector is a Dyadic Product
(\S\ref{sec:dyadic_product}), which results in an $m \times n$ Matrix

(FIXME) Relations and Boolean-valued Matrices --
\url{https://twitter.com/johncarlosbaez/status/1083042865540870145}


Systems of Linear Equations (\S\ref{sec:linear_equation_system})

an ``tall'' $m \times n$ Rectangular Matrix $A$ (where $m > n$) can be used to
represent the Coefficients of an \emph{Overdetermined System of Equations}
(\S\ref{sec:overdetermined_system}) $A\vec{x} = \vec{b}$; such a System usually
does not have an exact Solution, so methods such as Least Squares
(\S\ref{sec:lls}) is used to find the nearest Point to $\vec{b}$ in the Column
Space of $A$ (i.e. $\mathrm{min}_{\vec{x}} \|A\vec{x} - \vec{b}\|^2$) by
Orthogonal Projection \fist Normal Equations (\S\ref{sec:normal_equation}), QR
Factorization (\S\ref{sec:qr_factorization})

a ``fat'' $m \times n$ Rectangular Matrix $A$ (where $m < n$) can be used to
represent the Coefficients of an \emph{Underdetermined System of Equations}
(\S\ref{sec:underdetermined_system}) $A\vec{x} = \vec{b}$; such a System
usually has infinitely many Solutions, so methods are used to find the
``smallest'' Solution (i.e. $\mathrm{min}_{\vec{x}} \|\vec{x}\|^2$)
--FIXME: clarify

Matrices as Bipartite Graphs (\S\ref{sec:bigraph}):
\url{https://www.math3ma.com/blog/matrices-probability-graphs}

2019 - Bradley - \emph{Matrices as Tensor Network Diagrams} (article) -
\url{https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams}

\asterism

TODO

For the One-dimensional Vector Space over the Reals $\reals$, a 1x1 Matrix is
just a Real Number. The kinds of Linear Transformations are:
\begin{itemize}
  \item Identity -- $A = 1$
  \item Zero -- $A = 0$
  \item Reflection -- $A = -1$
  \item Scaling -- $A > 0$
  \item Scaling + Reflection -- $A < 0$
\end{itemize}
all Non-zero $A$ are Invertible

In $\reals^2$, a 2x2 Matrix may additionally represent:
\begin{itemize}
  \item Shear -- Horizontal, Vertical; every Shear Matrix has an Inverse which
    is a Shear Matrix with the Shear Element negated
\end{itemize}



% ------------------------------------------------------------------------------
\subsection{Rank}\label{sec:rank}
% ------------------------------------------------------------------------------

The \emph{Rank} of a Matrix $A$ is the Dimension of the Vector Space
``generated'' or \emph{Spanned} (\S\ref{sec:linear_span}) by its Columns, that
is, equal to the maximal number of Linearly Independent Columns of $A$ and is
identical to the Dimension of the Space Spanned by its Rows.

\fist Rank of a Matrix can be computed by Gaussian Elimination
(\S\ref{sec:gaussian_elimination})

the Maximum Rank of an $m \times n$ Matrix is $\mathrm{min}(m,n)$

the Column Rank and the Row Rank are always equal

Dimension of the Column Space (\S\ref{sec:column_space}) and the Row Space
(\S\ref{sec:row_space})

a Matrix is \emph{Full Rank} if its Rank equals the largest possible for a
Matrix of the same Dimensions, which is the lesser of the number of Rows or
Columns; for a Square Matrix this is equivalent to the statement that the
Matrix is \emph{Invertible} (\S\ref{sec:invertible_matrix})

if a Matrix $A$ is Full Rank, then $A^*A$ is Invertible

the Outer Product (\S\ref{sec:outer_product}) $\vec{u}\vec{v}^T$ of two Nonzero
Vectors is always a Matrix of Rank $1$

every Full Rank $m \times n$ Matrix with $m \geq n$ has a unique QR
Factorization (\S\ref{sec:qr_decomposition}) with $r_{jj} > 0$

\emph{Rank-Nullity Theorem}

for an $m \times n$ Matrix $A$, the Rank of $A$ plus the Nullity of $A$ is
equal to $n$

\fist the Rank (\S\ref{sec:rank}) of an Undirected Graph is the Rank of the
Graph's Adjacency Matrix



\subsubsection{Rank Update}\label{sec:rank_update}

UC Math 352 Lec. 8 - \url{https://www.youtube.com/watch?v=56vogBfKJu0}:

the \emph{Rank 1 Update} of a Matrix $A$:
\[
  A_+ = A + \vec{u}\vec{v}^T
\]

$\vec{u}\vec{v}^T$ is always a Rank 1 Matrix for Nonzero $\vec{u}$ and
$\vec{v}$

can replace a Row or a Column, and ``cheaply'' update derived Matrices such as
$A^{-1}$ (see Sherman-Morrison Formula \S\ref{sec:sherman_morrison}) or the
$QR$ Factors (\S\ref{sec:qr_decomposition})



\paragraph{Woodbury Matrix Identity}\label{sec:woodbury_matrix_identity}\hfill

states that the Inverse of a Rank-$k$ Correction of some Matrix can be computed
by doing a Rank-$k$ Correction to the Inverse of the original Matrix



\subparagraph{Sherman-Morrison Formula}\label{sec:sherman_morrison}\hfill

computes the Inverse of the Sum of an Invertible Matrix $A$ and the Outer
Product $\vec{u}\vec{t}^T$ (a Rank 1 Matrix)

UC Math 352 Lec. 8 - \url{https://www.youtube.com/watch?v=56vogBfKJu0}

\begin{align*}
  A_+^{-1} & = (A + \vec{u}\vec{v}^T)^{-1} \\
  A_+^{-1} & = A^{-1} -
    \frac{A^{-1}\vec{u}\vec{v}^TA^{-1}}{1 + \vec{v}^TA^{-1}\vec{u}}
\end{align*}

computing $A^{-1}\vec{u}$ and $\vec{v}^TA^{-1}$ are both $O(n^2)$ operations,
and whereas recomputing $A^{-1}$ is $O(n^3)$

\asterism

Example -- Solving two Systems of Equations:
\begin{align*}
  A\vec{x}                     & = \vec{b} \\
  A + \vec{u}\vec{v}^T\vec{x}' & = \vec{b}
\end{align*}
$\vec{x}'$ is called the ``Rank 1 Update of $\vec{x}$''

\begin{enumerate}
  \item Factor $A = LU$ -- LU Factorization is $O(n^3)$
  \item Solve $\vec{x} = A^{-1}b$ -- Computing Inverse of $A$ by Forward and
    Backward Substitution is $O(n^2)$
  \item Compute $\vec{w} = A^{-1}\vec{u}$ -- is $O(n^2)$ ($A$ has already been
    Factorized)
  \item Compute
    $\vec{x}' = \vec{x} - \vec{v}^T\vec{v}
      \frac{\vec{v}^T\vec{x}}{1 + \vec{v}^T\vec{w}} \vec{w}$ -- $O(n^2)$
\end{enumerate}



\subparagraph{Capacitance Matrix}\label{sec:capacitance_matrix}\hfill



% ------------------------------------------------------------------------------
\subsection{Nullity}\label{sec:nullity}
% ------------------------------------------------------------------------------

Dimension of the Nullspace (\S\ref{sec:nullspace})

\emph{Rank-Nullity Theorem}

for an $m \times n$ Matrix $A$, the Rank of $A$ plus the Nullity of $A$ is
equal to $n$



% ------------------------------------------------------------------------------
\subsection{Corank}\label{sec:corank}
% ------------------------------------------------------------------------------

Dimension of the Left Nullspace (\S\ref{sec:left_nullspace})



% ------------------------------------------------------------------------------
\subsection{Matrix Product}\label{sec:matrix_product}
% ------------------------------------------------------------------------------

\fist Tensor Product (\S\ref{sec:module_tensor_product})

The Multiplication of an $m \times n$ Matrix $A$ with entries $a_{xy}$
on the left by an $n \times p$ Matrix $B$ with entries $b_{wv}$ on the
right results in an $m \times p$ Matrix $C$ with the entry $c_{ij}$
defined as:
\[
  \sum_{k=1}^n a_{ik} b_{kj}
\]
The columns of the resulting Matrix $C$ are the result of Multiplying
the corresponding Column of $B$ on the left by the Matrix $A$. And the
rows of the resulting Matrix $C$ are the result of Multiplying the
corresponding row of $A$ on the right by the Matrix $B$.

the entry in the $i$th Row and the $j$th Column of the result is equal to the
Dot Product of the $i$th Row of the LHS with the $j$th Column of the RHS

the Columns of the result are combinations of the Columns of the LHS and the
Rows of the result are combinations of the Rows of the RHS

equivalently, the result is the Sum of multiplying Columns of the LHS times the
Rows of the RHS as individual Matrices Multiplications

an $m \times n$ Matrix $A$ Multiplied by an $n \times 1$ Matrix (Column Vector)
gives an $n \times 1$ Vector that is a Linear Combination of the Columns of $A$

an $m \times n$ Matrix $A$ Multiplied by an $1 \times m$ Matrix (Row Vector)
gives an $1 \times m$ Vector that is a Linear Combination of the Rows of $A$

\begin{itemize}
  \item $A(BC) = (AB)C$ -- Associative
\end{itemize}

Inverses and Transposes:

$AA^{-1} = I = A^{-1}A$

$(A^T)^{-1} = (A^{-1})^T = A^{-T}$

$(A^{-1})^T A^T = I$

$ABB^{-1}A^{-1} = I$

$B^{-1}A^{-1}AB = I$

$AB = C \Leftrightarrow B = A^{-1}C$



% ------------------------------------------------------------------------------
\subsection{Entrywise Product}\label{sec:entrywise_product}
% ------------------------------------------------------------------------------

\emph{Hadamard Product} or \emph{Schur Product}

Associative, Distributive, Commutative

the Grand Sum of the Enterywise Product $A \circ B$ is equal to the Trace of
$AB^T$



% ------------------------------------------------------------------------------
\subsection{Matrix Square Root}\label{sec:matrix_square_root}
% ------------------------------------------------------------------------------

$BB = A$

if a Matrix is Symmetric and Positive Definite, its Square Root exists



% ------------------------------------------------------------------------------
\subsection{Transpose}\label{sec:transpose}
% ------------------------------------------------------------------------------

Conjugate Transpose (\S\ref{sec:conjugate_transpose}) for Complex-valued
Matrices

if a Matrix $A$ is Full Rank, then $A^TA$ is Invertible

\begin{enumerate}
  \item $(A^T)^T = A$ -- Involutive
  \item $(A+B)^T = A^T + B^T$
  \item $(AB)^T = B^T A^T$
  \item $(cA)^T = c(A^T)$
  \item $|A^T| = |A|$ -- Determinant is unchanged (FIXME: is this only for
    square matrices ???)
  \item $\vec{a} \bullet \vec{b} = \vec{a}^T\vec{b}$
  \item if $A$ has only Real entries, then $A^T A$ is a Positive Semi-definite
    Matrix (\S\ref{sec:positive_semidefinite})
  \item $(A^T)^{-1} = (A^{-1})^T$ -- ``Inverse Transpose'' sometimes written
    $A^{-T}$
  \item if $A$ is a Square Matrix then its Eigenvalues are equal to the
    Eigenvalues of its Transpose since they share the same Characteristic
    Polynomial (\S\ref{sec:characteristic_polynomial})
\end{enumerate}



% ------------------------------------------------------------------------------
\subsection{Reduced Row Echelon Form}\label{sec:reduced_row_echelon}
% ------------------------------------------------------------------------------

Coefficient Matrix (System of Linear Equations \S\ref{sec:coefficient_matrix})



% ------------------------------------------------------------------------------
\subsection{Matrix Norm}\label{sec:matrix_norm}
% ------------------------------------------------------------------------------

Vector Norm (\S\ref{sec:norm}) in a Vector Space in which Elements are Matrices
of a given Dimension

is a measure of how much a Matrix $A$ can \emph{Scale} Vectors

\emph{Induced Matrix Norm}

Vector Norm $\|\cdot\|_p$ and Matrix $A$, Induced Matrix Norm $\|A\|_p$, is the
length of the largest Vector that can result from multiplying $A$ by a Unit
Vector:
\[
  \|A\| = \mathrm{sup}_{\vec{x}} \frac{\|A\vec{x}\|}{\|\vec{x}\|}
\]
and therefore:
\[
  \|A\| \geq \frac{\|A\vec{x}\|}{\|\vec{x}\|}
\]
and:
\[
  \|A\vec{x}\| \leq \|A\| \|\vec{x}\|
\]

Induced $p$-norms (\S\ref{sec:p_norm})

the Induced $1$-norm (Taxicab Norm) of a Matrix is equal to the maximum Column
Sum (Sum of Absolute Value of individual Components) of $A$

the Induced $\infty$-norm (Max Norm) is the maximum Row Sum of $A$


\emph{Frobenius Norm} -- Square Root of the Sum of the Squares of all the
entries; equal to the Square Root of the Trace of $A^*A$


the Frobenius Norm and the Euclidean Norm ($2$-norm) are both invariant under
Unitary Multiplication, i.e. Left or Right Multiplying a Matrix $A$ by an
Unitary Matrix $Q$ results in a Matrix with the same Norm as the original
Matrix $A$

the Absolute Condition Number (\S\ref{sec:absolute_condition_number}),
$\hat{\kappa}$, of a Differentiable Function $f(x)$ is equal to the Matrix Norm
of its Jacobian Matrix (\S\ref{sec:jacobian}) $J$:
\[
  \hat{\kappa} = \|J(x)\|
\]

the Relative Condition Number (\S\ref{sec:relative_condition_number}),
$\kappa$, of a Differentiable Function $f(x)$ is equal to:
\[
  \kappa = \frac{\|J(x)\| \|x\|}{\|f(x)\|}
\]

the worst case $\kappa$ for the Matrix/Vector Multiplication Function defined
by a Matrix $A$ is the Sharp Inequality (i.e. is Equal for \emph{some}
$\vec{x}$):
\[
  \kappa \leq \|A\|\|A^{-1}\|
\]
where $\|A\|\|A^{-1}\|$ is called the Condition Number $\kappa(A)$ of Matrix
$A$



\subsubsection{Spectral Norm}\label{sec:spectral_norm}

cf. Matrix Spectrum (\S\ref{sec:matrix_spectrum}) -- Set of all Eigenvalues of
a Matrix



% ------------------------------------------------------------------------------
\subsection{Nonnegative Matrix}\label{sec:nonnegative_matrix}
% ------------------------------------------------------------------------------

$A \geq 0$

$a_{ij} \geq 0 \ \ \ \forall i,j$



\subsubsection{Positive Matrix}\label{sec:positive_matrix}

\fist not to be confused with Positive-definite Matrices
(\S\ref{sec:positive_definite})



% ------------------------------------------------------------------------------
\subsection{Matrix Decomposition}\label{sec:matrix_decomposition}
% ------------------------------------------------------------------------------

Factorization of a Matrix into a Product of Matrices



\subsubsection{Eigendecomposition}\label{sec:eigendecomposition}

or \emph{Spectral Decomposition}

Factorization of a Matrix where the Matrix is represented in terms of its
Eigenvalues and Eigenvectors (\S\ref{sec:eigenvector})

only Diagonalizable Matrices (\S\ref{sec:diagonalizable_matrix}) can be
Factorized in this way



\subsubsection{LU Decomposition}\label{sec:lu_decomposition}

``Matrix form'' of Gaussian Elimination (Row Reduction
\S\ref{sec:gaussian_elimination})

cf. QR Factorization (\S\ref{sec:qr_decomposition})

Cholesky and Modified Cholesky Factorization
(\S\ref{sec:cholesky_decomposition}) are about half as much computation as the
LU Decomposition

$A = LU$

where $L$ is a Lower-triangular Matrix and $U$ is an Upper-triangular Matrix

$EA = U$

Elimination Steps $\cdots E_{32}E_{31}E_{21}A = U$

$A = E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}\cdots U$

Complexity: $O(n^3)$



\subsubsection{QR Decomposition}\label{sec:qr_decomposition}

(or \emph{QR Factorization} \S\ref{sec:qr_factorization})

\emph{Full QR Decomposition}: Decomposition of a Matrix $A \in \comps^{m \times
  n}, m \geq n$ into a Product $A = QR$ of an Orthogonal (Unitary) Matrix
(\S\ref{sec:orthogonal_matrix}) $Q$ (i.e. $Q^TQ = I$) and an Upper (Right)
Triangular Matrix (\S\ref{sec:upper_triangular}) $R$

this Factorization exists for all $A \in \comps^{m \times n}, m \geq n$, and is
\emph{unique} if $A$ is Full Rank (\S\ref{sec:rank}) when the Diagonal Elements
of $R$ are required to be Positive

Numerically Stable way to solve Least Squares Problem (\S\ref{sec:lls});
Overdetermined Systems (\S\ref{sec:overdetermined_system})

solving Eigenvalue Problems (QR Algorithm)

\fist QR Method (Nullspace Method) for solving Equality Constrained Quadratic
Programming Problems (\S\ref{sec:quadratic_programming})


\url{https://www.youtube.com/watch?v=3HS-BRbJOd0}:

\emph{Reduced QR Factorization}

a Full Rank $m \times n, m \geq n$ Matrix $A$ with Columns:
\[
  A = [ \vec{a}_1, \ldots, \vec{a}_n ]
\]
can have its Column Space decomposed into nested Subspaces by taking the Spans
(\S\ref{sec:linear_span}) of the First $1$ to $n$ Columns:
\[
  \mathrm{span} \{ \vec{a}_1 \} \subseteq
  \mathrm{span} \{ \vec{a}_1, \vec{a}_2 \}
  \subseteq \cdots \subseteq
  \mathrm{span} \{ \vec{a}_1, \ldots, \vec{a}_n \}
  = \mathrm{span} A
\]

find Orthonormal Bases $\{ \hat{q}_1 \}, \{ \hat{q}_1, \hat{q}_2 \}, \ldots \{
\hat{q}_1, \ldots, \hat{q}_n\}$ for each of these Column Spaces, i.e.:
\[
  \mathrm{span} \{ \hat{q}_1, \hat{q}_2, \ldots, \hat{q}_j \} =
  \mathrm{span} \{ \vec{a}_1, \vec{a}_2, \ldots, \vec{a}_j \}
\]
which can be expressed as the Multiplication of the $m \times n$ Matrix
$\hat{Q} = [ \hat{q}_1 \cdots \hat{q}_n ]$ by an $n \times n$ Upper Triangular
Matrix, $A = \hat{Q}\hat{R}$:
\[
  [\vec{a}_1, \vec{a}_2, \ldots, \vec{a}_n] =
  [\hat{q}_1, \hat{q}_2, \ldots, \hat{q}_n]
  \begin{bmatrix}
    r_{11} & r_{12} & \cdots & r_{1n} \\
    0      & r_{22} & \cdots & r_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    0      & \cdots & 0      & r_{nn} \\
  \end{bmatrix}
\]
yielding the Equations:
\begin{align*}
  \vec{a}_1 & = r_{11}\hat{q}_1 \\
  \vec{a}_2 & = r_{12}\hat{q}_1 + r_{22}\hat{q}_2 \\
            & \vdots \\
  \vec{a}_n & = r_{1n}\hat{q}_1 + r_{2n}\hat{q}_2 + \cdots + r_{nn}\hat{q}_n \\
\end{align*}

Reduced QR Factorization can be used to compute minimal $\|\vec{x}\|^2$ for
Underconstrained Systems (\S\ref{sec:underdetermined_system})


\emph{Full QR Factorization}

\[
  A = QR = [\hat{Q}Q_N]\begin{bmatrix} \hat{R} \\ 0 \end{bmatrix}
\]
$Q$ is an Orthogonal (or Unitary) $m \times m$ Matrix formed by extending
$\hat{Q}$ by $m - n$ additional Orthonormal Columns and $R$ is an $m \times n$
Upper Triangular Matrix formed by extending $\hat{R}$ by $m - n$ additional
Rows of $1 \times m$ Zero Vectors

the Columns of $Q_N$ form an Orthonormal Basis for the Left Nullspace of $A$,
viz. $\mathrm{null}(A^*)$


\emph{Existence and Uniqueness}

Thm. \emph{Every Matrix $A \in \comps^{m \times n}, m \geq n$ has a Full (and
  Reduced) QR Factorization}

Thm. \emph{Every Matrix $A \in \comps^{m \times n}, m \geq n$ of Full Rank has
  a unique Reduced QR Factorization $A = \hat{Q}\hat{R}$ with $r_{jj} \geq 0$}


\emph{Orthogonal Projections} (\S\ref{sec:orthogonal_projection})

one may define an Orthogonal Projection $P$ onto the Column Space of a Full Rank
$m \times n, m \geq n$ Matrix $A$ using the $\hat{Q}$ Matrix from the Reduced
QR Factorization:
\[
  P = \hat{Q}\hat{Q}^*
\]


\emph{Algorithm}

cf. the process of finding Elementary Operations
(\S\ref{sec:elementary_operation}) that produce an LU Factorization
(\S\ref{sec:lu_decomposition}) \fist Gaussian Elimination
(\S\ref{sec:gaussian_elimination})

computing $R$:
\[
  \cdots Q_2 Q_1 A = R
\]

using Elementary Orthogonal Matrices (\S\ref{sec:elementary_orthogonal_matrix}):
\begin{itemize}
  \item Givens Matrices (\emph{Plane Rotations} \S\ref{sec:plane_rotation})
  \item Householder Matrices (\emph{Elementary Reflector}
    \S\ref{sec:elementary_reflector})
\end{itemize}

using Givens Rotations, Multiplying by $G_{ij}$ can be used to make an entry
$A[ji]$ Zero; combining a number of such Rotations can be used to yield $R$:
\begin{align*}
  \cdots G_{ij}G_{kl}A & = R \\
  A & = (\cdots G_{ij}G_{kl})^{-1}R \\
  A & = G_{kl}^T G_{ij}^T \cdots R = QR
\end{align*}


\emph{Rank 1 Update of QR Factors}

UC Math 352 Lec. 8 \url{https://www.youtube.com/watch?v=56vogBfKJu0}

Rank Update (\S\ref{sec:rank_update})

$A = QR$

$A_+ = QR + \vec{u}\vec{v}^T$

$Q^T A_+ = R + \vec{w}\vec{v}^T$

where $\vec{w} = Q^T\vec{u}$

compute $A_+ = Q_+ R_+$

use Plane Rotations (\S\ref{sec:plane_rotation}) to make all but the first two
elements of $\vec{w}$ Zero, starting from the end of $\vec{w}$:
\[
  G_{2,3}G_{3,4}\cdots G_{m-2,m-1}G_{m-1,m} \vec{w} = [w_1 \ w_2 0 \ \cdots \ 0]^T
\]
these $G_{i,j}$ Matrices turn $R$ into an Upper Hessenberg Matrix
(\S\ref{sec:hessenberg_matrix}); then:
\begin{align*}
  Q^T A_+ & = R + \vec{w}\vec{v}^T \\
  G_{2,3}G_{3,4}\cdots G{m-1,m} Q^T A_+ & =
    G_{2,3}G_{3,4}\cdots G{m-1,m} R +
    G_{2,3}G_{3,4}\cdots G{m-1,m} \vec{w}\vec{v}^T  = H\\
\end{align*}
the RHS is equal to an Upper Hessenberg Matrix $H$

then apply another sequence of Plane Rotations $G'_{1,2}, G'_{2,3}, \ldots,
G'_{m-1,m}$ to rotate the Subdiagonal entries out to Zeros, giving back an
Upper Triangular Matrix:
\[
  G'_{m-1,m}\cdots G'_{3,4}G'_{2,3}G'_{1,2}G_{2,3}G_{3,4}\cdots G_{m-1,m}
    Q^T A_+ =
  G'_{m-1,m}\cdots G'_{3,4}G'_{2,3}G'_{1,2}H
\]
and so:
\begin{align*}
  G'_{m-1,m}\cdots G'_{3,4}G'_{2,3}G'_{1,2}G_{2,3}G_{3,4}\cdots G_{m-1,m} Q^T
    & = Q_+^T \\
  G'_{m-1,m}\cdots G'_{3,4}G'_{2,3}G'_{1,2}H = R_+
\end{align*}

each Plane Rotation is $O(n)$, so the QR Update is $O(mn)$ Operations vs.
$O(m^3)$ for a full QR



\subsubsection{Cholesky Decomposition}\label{sec:cholesky_decomposition}

$A = LL^*$

the Cholesky Decomposition exists if $A$ is Positive Definite
(\S\ref{sec:positive_definite})

Naive Gaussian Elimination (\S\ref{sec:gaussian_elimination}) needs to result
in Positive Diagonal entries (Pivots) in $U$

\emph{Modified Cholesky Factorization} $A = L D^2 L^T$ where $D$ is the
Diagonal Matrix with entries taken from the Upper Triangular Result $U$ of
Naive Gaussian Elimination -- avoids Square Roots

Cholesky and Modified Cholesky Factorization are about half as much computation
as the LU Decomposition (\S\ref{sec:lu_decomposition})

solving Normal Equations (\S\ref{sec:normal_equation}) in Ordinary Least Squares
(\S\ref{sec:ols})-- Ill-conditioned, see QR Factorization
(\S\ref{sec:qr_factorization}) for standard method with better Condition Number



\subsubsection{Polar Decomposition}\label{sec:polar_decomposition}

The \emph{Polar Decomposition} of a Square Matrix $A$ is a Matrix Decomposition
of the form:
\[
  A = U P
\]
where $U$ is a Unitary Matrix (\S\ref{sec:unitary_matrix}) and $P$ is a
Positive-semidefinite (\S\ref{sec:positive_definite}) Hermitian
(\S\ref{sec:hermitian_matrix}) Matrix



\subsubsection{Singular-value Decomposition}\label{sec:svd}

(SVD)

Factoriztion of a Real or Complex Matrix generalizing the Eigendecomposition
(\S\ref{sec:eigendecomposition}) of a Positive Semidefinite
(\S\ref{sec:positive_definite}) Normal (\S\ref{sec:normal_matrix}) Matrix (e.g.
a Symmetric Matrix with Positive Eigenvalues) to any $m \times n$ Matrix via an
extension of the Polar Decomposition (\S\ref{sec:polar_decomposition})

The Singular-value Decomposition of an $m \times n$ Real or Complex Matrix $M$
is a Factorization of the form:
\[
  U \Sigma V^*
\]
where $U$ is an $m \times m$ Real or Complex Unitary Matrix
(\S\ref{sec:unitary_matrix}) of \emph{Left-singular Vectors}, $\Sigma$ is an $m
\times n$ Rectangular Diagonal Matrix (\S\ref{sec:diagonal_matrix}) with
Non-negative Real Numbers known as the \emph{Singular Values} of $M$ on the
Diagonal, and $V$ is an $n \times n$ Real or Complex Unitary Matrix of
\emph{Right-singular Vectors}.



% ------------------------------------------------------------------------------
\subsection{Square Matrix}\label{sec:square_matrix}
% ------------------------------------------------------------------------------

there is a correspondence between $n\times{n}$ Square Matrices and Linear
Operators (\S\ref{sec:linear_operator}) on an $n$-dimensional Vector Space

a Square Matrix is Invertible (\S\ref{sec:invertible_matrix}) if and only if
its Determinant (\S\ref{sec:determinant}) is Nonzero

the Left Inverse of an Invertible Square Matrix is equal to its Right Inverse

the Product of all Non-zero Eigenvalues (\S\ref{sec:eigenvalue}) of a Square
Matrix is the Pseudo-determinant (\S\ref{sec:pseudo_determinant}) and coincides
with the Determinant when the Matrix is Invertible

a Square Jacobian Matrix (\S\ref{sec:jacobian}) and its Determinant
(\S\ref{sec:determinant}) are both called ``the Jacobian''

\begin{itemize}
  \item Transition (Stochastic) Matrix (Markov Processes
    \S\ref{sec:markov_process})
  \item ...
\end{itemize}



\subsubsection{Determinant}\label{sec:determinant}

A Square Matrix is Invertible (\S\ref{sec:invertible_matrix}) if and only if
its Determinant is non-zero, or equivalently if its Rank (\S\ref{sec:rank})
equals the size of the Matrix, and if it is Invertible the Determinant of the
Inverse Matrix is given by $det(A^{-1}) = \frac{1}{det(A)}$.

Cofactor Matrix (\S\ref{sec:cofactor_matrix}) --TODO

\fist the Determinant of a Matrix can be computed by Gaussian Elimination
(\S\ref{sec:gaussian_elimination})

\fist Jacobian Determinant (\S\ref{sec:jacobian_determinant})

The Special Linear Group $SL(n,F)$ of Degree $n$ over a Field $F$ is the Set of
$n \times n$ Matrices with Determinant $1$ with the Group Operations of
ordinary Matrix Multiplication and Matrix Inversion.

The Determinant of a Square Jacobian Matrix and the Matrix itself are both
called ``the Jacobian''.

$n!$ Complexity ??? (FIXME)

the Space of Positive Determinant Matrices is Path-connected



\paragraph{Minor}\label{sec:minor}\hfill

the Determinant of some smaller Square Matrix formed by removing one or more
Rows or Columns

$[A]_{I,J}$ where $I$ and $J$ are a Subset of Rows and Columns, respectively

\emph{First Minor} -- a Minor formed by removing one Row and one Column from a
Square Matrix

if $I = J$ then $[A]_{I,J}$ is called a \emph{Principal Minor}

Leading Principle Minor

the Leading Principle Minor for Hermitian Matrices
(\S\ref{sec:hermitian_matrix}) can be used to test for Positive Definiteness
(\S\ref{sec:positive_definite}), and the Principal Minors can be used to test
for Postive Semi-definiteness (\S\ref{sec:positive_semidefinite})



\paragraph{Pseudo-determinant}\label{sec:pseudo_determinant}\hfill

Product of all Non-zero Eigenvalues (\S\ref{sec:eigenvalue}) of a Square Matrix

coincides with the regular Determinant when the Matrix is Non-singular
(Invertible \S\ref{sec:invertible_matrix})



\paragraph{Wronskian}\label{sec:wronskian}\hfill



\subsubsection{Identity Matrix}\label{sec:identity_matrix}

A Square Diagonal Matrix (\S\ref{sec:diagonal_matrix}) with $1$ on the main
diagonal

the Identity Matrix is (FIXME: a/the ???) Unit Tensor (\S\ref{sec:unit_tensor})

Scalar Matrices (\S\ref{sec:scalar_matrix}): Identity Matrix times a Scalar

Positive Definite (\S\ref{sec:positive_definite})



\subsubsection{Cofactor Matrix}\label{sec:cofactor_matrix}

calculated from Minors (\S\ref{sec:minor})

$A^{-1} = \frac{1}{det(A)}C^T$

used in computing both Determinant and Inverse of a Square Matrix

Cramer's Rule



\subsubsection{Permutation Matrix}\label{sec:permutation_matrix}

A \emph{Permutation Matrix} is a Square Binary Matrix with exactly one
$1$ in each Row and each Column and $0$s elsewhere.

Permutation Matrices are Orthogonal Matrices (\S\ref{sec:orthogonal_matrix}):

$P_\pi^{-1} = P_\pi^T$

$n \times n$ Permutation Matrices form a Symmetic Group
(\S\ref{sec:symmetric_group}) under Matrix Multiplication with
with the Identity Matrix as the Identity Element



\subsubsection{Invertible Matrix}\label{sec:invertible_matrix}

(or \emph{Non-singular} or \emph{Non-degenerate})

Non-zero Determinant

\fist the Inverse of an Invertible Matrix can be computed through Gauss-Jordan
(Gaussian) Elimination (\S\ref{sec:gaussian_elimination})

for a Square Matrix, the Left Inverse is equal to the Right Inverse

in general, a Square Matrix over a Commutative Ring is Invertible if and only
if its Determinant is a Unit in that Ring

\fist General Linear Group (\S\ref{sec:general_linear_group}): Group of $n
\times n$ Invertible Matrices

\fist Special Linear Group (\S\ref{sec:special_linear_group}): Group of $n
\times n$ Matrices with Determinant $1$; Normal Subgroup of the General Linear
Group

the Classical Subgroups of the General Linear Group $GL(V)$ preserving a kind
of Bilinear Form (\S\ref{sec:bilinear_form}) on a Vector Space $V$:
\begin{itemize}
  \item the Orthogonal Group (\S\ref{sec:orthogonal_group}) $O(V)$ -- preserves
    a Non-degenerate Quadratic Form (\S\ref{sec:quadratic_form}) on $V$
  \item the Symplectic Group (\S\ref{sec:symplectic_group}) $Sp(V)$
    -- preserves a Symplectic Form (Non-degenerate Alternating Form
    \S\ref{sec:symplectic_form}) on $V$
  \item the Unitary Group (\S\ref{sec:unitary_group}) $U(V)$
    -- preserves a Non-degenerate Hermitian Form (\S\ref{sec:hermitian_form})
    on $V$
\end{itemize}

the Pseudo-determinant (\S\ref{sec:pseudo_determinant}) of a Square Matrix
coincides with the Determinant when the Matrix is Invertible

a Square Matrix $A$ is Diagonalizable (\S\ref{sec:diagonalizable_matrix}) if
there exists an Invertible Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix
(\S\ref{sec:diagonal_matrix})

for a Square Matrix, the Matrix is Full Rank (\S\ref{sec:rank}) is
equivalent to being Invertible

doing Gaussian Elimination to compute the Inverse of a Matrix is $O(n^3)$

$ABB^{-1}A^{-1} = I$

$B^{-1}A^{-1}AB = I$

if a Matrix $A$ is Full Rank, then $A^*A$ is Invertible

two Square Matrices $A$ and $B$ are \emph{Similar}
(\S\ref{sec:matrix_similarity}) if:
\[
  B = P^{-1}AP
\]
for some Invertible Matrix $P$

\fist Representation of Groups (\S\ref{sec:group_representation}) in terms of
Invertible Matrices



\paragraph{Involutory Matrix} (\label{sec:involutory_matrix})\hfill

$A^2 = I$

a Matrix that is its own Inverse



\subsubsection{Singular Matrix}\label{sec:singular_matrix}

Non-invertible Matrix

a Square Matrix is Singular if and only if its Determinant
(\S\ref{sec:determinant}) is $0$

\fist in a System of DAEs (\S\ref{sec:dae_system}), the Jacobian Matrix
(\S\ref{sec:jacobian}) is a Singular (Non-invertible) Matrix

examples:
\begin{itemize}
  \item Projections (\S\ref{sec:linear_projection}) -- $A^2 = A$
\end{itemize}



\subsubsection{Circulant Matrix}\label{sec:circulant_matrix}

fully specified by one Vector



\subsubsection{Matrix Similarity}\label{sec:matrix_similarity}

two Square Matrices $A$ and $B$ are \emph{Similar} if:
\[
  B = P^{-1}AP
\]
for some Invertible Matrix $P$

Similar Matrices represent the same Linear Operator under two possibly
different Bases with $P$ being a Change of Basis Matrix
(\S\ref{sec:change_of_basis})

the Characteristic Polynomial (\S\ref{sec:characteristic_polynomial}) of a
Square Matrix is a Polynomial that is Invariant under Matrix Similarity and has
the Eigenvalues as Roots



\subsubsection{Orthogonal Matrix}\label{sec:orthogonal_matrix}

An \emph{Orthogonal Matrix} $Q$ is an $n \times n $ Real-valued Square Matrix
with Columns and Rows that are \emph{Orthonormal Vectors}
(\S\ref{sec:orthonormality}), i.e. forms an Orthonormal Basis
(\S\ref{sec:orthonormal_basis}) for Euclidean Space $\reals^n$, resulting in
the identity:
\[
  Q^TQ = QQ^T = I
\]
i.e. a Matrix $Q$ is Orthogonal if its Transpose is equal to its Inverse:
\[
  Q^T = Q^{-1}
\]

the Set of $n \times n$ Orthogonal Matrices forms the Orthogonal Group
$O(n)$ (\S\ref{sec:orthogonal_group})

an Orthogonal Matrix has Determinant of either $1$ or $-1$; the Orthogonal
Matrices with Determinant $+1$ are \emph{Rotation Matrices}
(\S\ref{sec:rotation_matrix}), forming the Subgroup $SO(n)$ of Special
Orthogonal Matrices (\S\ref{sec:special_orthogonal_group})

\fist an Unitary Matrix (\S\ref{sec:unitary_matrix}) is the Complex analogue of
an Orthogonal Matrix

QR Decomposition (\S\ref{sec:qr_decomposition}): Decomposition of a Matrix $A$
into a Product $A = QR$ of an Orthogonal Matrix $Q$ and an Upper Triangular
Matrix (\S\ref{sec:upper_triangular}) $R$ \fist Elementary Orthogonal Matrices
(\S\ref{sec:elementary_orthogonal_matrix})

cf. LU Decomposition (\S\ref{sec:lu_decomposition}) of a Matrix into a sequence
of Elementary Operations (\S\ref{sec:elementary_operation}), which can be
represented as Matrix Multiplications

\begin{itemize}
  \item Identity Matrix (\S\ref{sec:identity_matrix})
  \item Permutation Matrices (\S\ref{sec:permutation_matrix})
  \item Discrete Fourier Transform (\S\ref{sec:dft})
  \item ...
\end{itemize}



\paragraph{Rotation Matrix}\label{sec:rotation_matrix}\hfill

Orthogonal Matrix with Determinant $1$

Special Orthogonal Group $SO(n)$ (\S\ref{sec:special_orthogonal_group})



\paragraph{Elementary Orthogonal Matrix}
\label{sec:elementary_orthogonal_matrix}\hfill

Elementary Matrix (\S\ref{sec:elementary_matrix})

QR Factorization (\S\ref{sec:qr_decomposition})



\subparagraph{Plane Rotation}\label{sec:plane_rotation}\hfill

or \emph{Givens Rotation}

\[
  G_{ij}(\theta) = \begin{bmatrix}
    1 &        &   &            &   &        &   &             &   &        &   \\
      & \ddots &   &            &   &        &   &             &   &        &   \\
      &        & 1 &            &   &        &   &             &   &        &   \\
      &        &   & \cos\theta &   &        &   & -\sin\theta &   &        &   \\
      &        &   &            & 1 &        &   &             &   &        &   \\
      &        &   &            &   & \ddots &   &             &   &        &   \\
      &        &   &            &   &        & 1 &             &   &        &   \\
      &        &   & \sin\theta &   &        &   & \cos\theta  &   &        &   \\
      &        &   &            &   &        &   &             & 1 &        &   \\
      &        &   &            &   &        &   &             &   & \ddots &   \\
      &        &   &            &   &        &   &             &   &        & 1 \\
  \end{bmatrix}
\]
where the ``square'' formed by the $\cos$ and $\sin$ terms are in the $i$th and
$j$th Rows and Columns:
\begin{itemize}
  \item $G_{ij}[ii] = \cos\theta$
  \item $G_{ij}[ij] = -\sin\theta$
  \item $G_{ij}[ji] = \sin\theta$
  \item $G_{ij}[jj] = \cos\theta$
\end{itemize}

\fist may be used in computing QR Factorization (\S\ref{sec:qr_decomposition})
and in computing Rank 1 Update (\S\ref{sec:rank_update}) of QR Factors



\subparagraph{Elementary Reflector}\label{sec:elementary_reflector}\hfill

or \emph{Householder Transformation}

\emph{Reflection} about a Plane containing the Origin

Plane Normal $\hat{w}$

\begin{align*}
  H\vec{x} & = \vec{x} - 2(\hat{w}^T\vec{x})\hat{w} \\
  H\vec{x} & = (I - 2\hat{w}\hat{w}^T)\vec{x}       \\
         H & = I - 2\hat{w}\hat{w}^T
\end{align*}

\fist may be used in computing QR Factorization (\S\ref{sec:qr_decomposition})



\subsubsection{Diagonalizable Matrix}\label{sec:diagonalizable_matrix}

A Square Matrix $A$ is \emph{Diagonalizable} if there exists an Invertible
Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix
(\S\ref{sec:diagonal_matrix}).

A Square Matrix that is \emph{not} Diagonalizable is called \emph{Defective}.

Only Diagonlizable Matrices can be Eigendecomposed
(\S\ref{sec:eigendecomposition}).



\subsubsection{Symmetric Matrix}\label{sec:symmetric_matrix}

A \emph{Symmetric Matrix} $A$ is a Square Matrix that is equal to its
Transpose:
\[
  A = A^T
\]

the Complex extension of a Symmetric Matrix is a \emph{Hermitian Matrix}
(\S\ref{sec:hermitian_matrix})

an Orthogonal Projection (\S\ref{sec:orthogonal_projection}) is exactly an
\emph{Idempotent} Symmetric Linear Operator

Symmetric Bilinear Forms (\S\ref{sec:symmetric_bilinear}) on
Finite-dimensional Vector Spaces correspond precisely to Symmetric Matrices
given a Basis for $V$, and correspond one-to-one with Quadratic Forms
(\S\ref{sec:quadratic_form}) over the Vector Space;
a Quadratic Form $f(\vec{x})$ on $n$ Real Variables $x_1,\ldots,x_n$ can always
be written as $\vec{x}^T M \vec{x}$ for $M$ a Symmetric Real Matrix; $M$ is a
Positive-definite Matrix (\S\ref{sec:positive_definite}) when $f$ has a unique
Minimum (Zero) when $\vec{x}$ is Zero and is Strictly Positive for any other
$\vec{x}$

(\emph{Jacobi's Theorem}) any Symmetric Matrix can be transformed into a
Diagonal Matrix $B$ by an Orthogonal Matrix $S$:
\[
  B = S^T A S
\]
and the Diagonals of $B$ are uniquely determined

(\emph{Sylvester's Law of Inertia}) if $S$ is allowed to be any Invertible
Matrix then the Diagonals of $B$ may be made to be $0$, $1$, and $-1$, the
number of each type giving the Signature of the associated Quadratic Form

when the Diagonals are all $1$, the Quadratic Form is Positive Definite, and
when all are $-1$ it is Negative Definite

the Function for the maximum Eigenvalue (\S\ref{sec:eigenvalue}) of a Symmetric
Matrix $X \in \mathsf{S}^n$ is a Convex Function (\S\ref{sec:convex_function}):
\[
  \lambda_{max}(X) = \mathrm{sup}_{\|\vec{y}\|_2=1} \vec{y}^T X \vec{y}
\]

\begin{itemize}
  \item Covariance Matrix (\S\ref{sec:covariance_matrix})
  \item ...
\end{itemize}



\paragraph{Euclidean Distance Matrix}\label{sec:euclidean_distance_matrix}\hfill

(wiki):

for a Set of $n$ Points $x_1, x_2, \ldots, x_n$ in $m$-dimensional Euclidean
Space, the \emph{Euclidean Distance Matrix} $A$ is given by:
\begin{align*}
  A      & = (a_{ij}) \\
  a_{ij} & = d_{ij}^2 = \|x_i - x_j\|_2^2 \\
\end{align*}
so that $A$ is a Symmetric Hollow Matrix (all Diagonal elements of $A$ are
Zero) with $a_{ij} \geq 0$, i.e. its elements are the Squares of the Euclidean
Distances between the Vectors of some Configuration

a Euclidean Distance Matrix has Rank less than or equal to $m+2$

if the Points $x_1, \ldots, x_n$ are in General Position
(\S\ref{sec:general_position}) then the Rank is exactly $\mathrm{min}(n,m+2)$

(2004 Boyd, Vanderberghe \emph{Convex Optimization} pp.409):

a Euclidean Distance Matrix $D$ and the Length-squared Vector $\vec{z}$ (i.e.
the Vector of squared lengths $[l_1 \ \cdots \ l_n]^T$ of each Vector $x_1,
\ldots, x_n$), the original Configuration can be reconstructed



\subsubsection{Skew-symmetric Matrix}\label{sec:skew_symmetric_matrix}

(or \emph{Antisymmetric})

Bivectors (\S\ref{sec:bivector}) are Isomorphic to Skew-symmetric Matrices

\fist cf. Skew-symmetric Bilinear Form (\S\ref{sec:skew_symmetric_bilinear})

a Symplectic Bilinear Form (\S\ref{sec:symplectic_bilinear}) can be represented
by an Invertible, Hollow, Skew-symmetric Matrix; note this is \emph{not} the
same as a Symplectic Matrix (i.e. a Symplectic Transformation of the Space
TODO: xref)



\subsubsection{Hamiltonian Matrix}\label{sec:hamiltonian_matrix}

\subsubsection{Characteristic Polynomial}\label{sec:characteristic_polynomial}

Polynomial which is Invariant under \emph{Matrix Similarity}
(\S\ref{sec:matrix_similarity}) and has the Eigenvalues as Roots



\subsubsection{Hessenberg Matrix}\label{sec:hessenberg_matrix}

an ``almost Triangular'' Matrix (\S\ref{sec:triangular_matrix})

Zeros above/below the first Super/Sub-diagonal

Upper Hessenberg

Lower Hessenberg



\subsubsection{Nilpotent Matrix}\label{sec:nilpotent_matrix}

a \emph{Nilpotent Matrix} is a Square Matrix $N$ such that $N^k = 0$ for some
Positive Integer $k$ where the smallest such $k$ is called the \emph{Index} of
$N$



% ------------------------------------------------------------------------------
\subsection{Triangular Matrix}\label{sec:triangular_matrix}
% ------------------------------------------------------------------------------

the Group of Invertible Upper (or Lower) Triangular Matrices is a Borel
Subgroup of the General Linear Group $GL_n$ of Invertible Matrices

a Matrix that is both Upper Triangular and Lower Triangular is a Diagonal
Matrix (\S\ref{sec:diagonal_matrix})

\fist Hessenberg Matrix (\S\ref{sec:hessenberg_matrix}) -- an ``almost
Triangular'' Matrix



\subsubsection{Upper Triangular Matrix}\label{sec:upper_triangular}

QR Decomposition (\S\ref{sec:qr_decomposition}): Decomposition of a Matrix $A$
into a Product $A = QR$ of an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix})
$Q$ and an Upper Triangular Matrix $R$

the Subgroup of the General Linear Group (\S\ref{sec:general_linear_group})
$GL(V)$ of Invertible Upper Triangular Matrices is a Borel Subgroup
(\S\ref{sec:borel_subgroup})



\paragraph{Back Substitution}\label{sec:back_substitution}\hfill

Solve the Matrix Equation $U\vec{x} = \vec{b}$ for a Upper Triangular Matrix

\fist Forward Substitution (\S\ref{sec:forward_substitution}) for Lower
Triangular Matrices



\subsubsection{Lower Triangular Matrix}\label{sec:lower_triangular}

\paragraph{Forward Substitution}\label{sec:forward_substitution}\hfill

Solve the Matrix Equation $L\vec{x} = \vec{b}$ for a Lower Triangular Matrix

\fist Back Substitution (\S\ref{sec:back_substitution}) for Upper Triangular
Matrices



\subsubsection{Unitriangular Matrix}\label{sec:unitriangular_matrix}

entries on the main Diagonal are all $1$



% ------------------------------------------------------------------------------
\subsection{Diagonal Matrix}\label{sec:diagonal_matrix}
% ------------------------------------------------------------------------------

A Matrix with $0$ values in all non-diagonal entries

both Upper Triangular and Lower Triangular

a Diagonalizable Matrix is a Square Matrix $A$ for which there exists an
Invertible Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix

Positive Definite (\S\ref{sec:positive_definite}) if Diagonal entries are
Positive Real, Positive Semidefinite (\S\ref{sec:positive_semidefinite}) if
Diagonal entries are Zero or Positive Real

the Subgroup of the General Linear Group (\S\ref{sec:general_linear_group})
$GL(n,F)$ of Invertible Diagonal Matrices is Isomorphic to the Multiplicative
Group $(F^\times)^n$, corresponding to \emph{Dilations} and \emph{Contractions}

\begin{itemize}
  \item Identity Matrices (\S\ref{sec:identity_matrix})
\end{itemize}



\subsubsection{Scalar Matrix}\label{sec:scalar_matrix}

a Scalar Multiple of the Identity Matrix

the Subgroup of the General Linear Group (\S\ref{sec:general_linear_group})
$GL(n,F)$ of Nonzero Scalar Matrices is Isomorphic to the Multiplicative Group
$F^\times$, forms the Center (\S\ref{sec:group_center}) of $GL(n,F)$ and is a
Normal Abelian Subroup

the Center of $SL(n,F)$ is the Set of all Scalar Matrices
(\S\ref{sec:scalar_matrix}) with Unit Determinant and is Isomorphic to the
Group of $n$th Roots of Unity (\S\ref{sec:root_of_unity}) of the Field $F$



% ------------------------------------------------------------------------------
\subsection{Hollow Matrix}\label{sec:hollow_matrix}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Elementary Matrix}\label{sec:elementary_matrix}
% ------------------------------------------------------------------------------

a Matrix that differs from the Identity Matrix by a single Elementary Row
Operation (\S\ref{sec:elementary_operation})

Elementary Matrices generate the General Linear Group $GL_n(R)$ of Invertible
Matrices

the LU Decomposition (\S\ref{sec:lu_decomposition}) of a Matrix is the
Decomposition into Elementary Operations represented as Matrices \fist Gaussian
Elimination (\S\ref{sec:gaussian_elimination})

Elementary Orthogonal Matrix (\S\ref{sec:elementary_orthogonal_matrix}): can be
used in QR Decomposition (\S\ref{sec:qr_decomposition})



\subsubsection{Elementary Operation}\label{sec:elementary_operation}

\emph{Elementary Row Operations}

(\emph{Elementary Column Operations})

note that Row Operations are achieved by Left-multiplication by an Elementary
Matrix while Column Operations are achieved by a Right-multiplication by an
Elementary Matrix

\begin{enumerate}
  \item Row Addition by a Scalar Multiple of another Row
  \item Row Exchange
  \item Scalar Row Multiplication
\end{enumerate}

Row Addition by a Scalar Multiple of another Row is a Matrix with $1$ on the
diagonals and a single off-diagonal non-zero entry in the Row matching the
affected Row the Column matching to the Row that is being added

Row Exchange is achieved by a Permutation Matrix
(\S\ref{sec:permutation_matrix})

Scalar Row Multiplication is achieved by a Matrix with all $1$s on the diagonal
except for the diagonal entry for the corresponding Row equal to the desired
Scalar



\subsubsection{Gaussian Elimination}\label{sec:gaussian_elimination}

\emph{Row Reduction} or \emph{Gauss-Jordan Elimination}

Algorithm for solving Systems of Linear Equations
(\S\ref{sec:linear_equation_system})

can be used to find the Rank (\S\ref{sec:rank}) of a Matrix, calculate the
Determinant (\S\ref{sec:determinant}) of a Matrix, and to calculate the Inverse
of an Invertible Square Matrix (\S\ref{sec:invertible_matrix})

doing Gaussian Elimination to compute the Inverse of a Matrix is $O(n^3)$

the Elementary Operations can be represented as Matrices, giving the \emph{LU
  Factorization} (\S\ref{sec:lu_decomposition}) of a Matrix

cf. QR Factorization (\S\ref{sec:qr_decomposition}) as a Decomposition into
Orthogonal Matrices (\S\ref{sec:orthogonal_matrix})

\emph{Naive Gaussian Elimination} -- no Row Swaps or Scalar Multiplications

\asterism

MIT 18.06 Lec. 2 - \url{https://www.youtube.com/watch?v=QVKj3LADCnA}

$Ax = b$

Elementary Operations are performed on the \emph{Augmented Matrix} $[A | b]$ to
transform it into $[U | c]$ where $U$ is an Upper-triangular Matrix and the
Elementary Operations are collected into a Matrix $L$ so that $A = LU$ \fist LU
Decomposition (\S\ref{sec:lu_decomposition})

\emph{Forward Elimination} -- perform Elementary Row Operations to
``eliminate'' Variables from equations to put the System Matrix into an Upper
Triangular Form so that Variables can be substituted upward by \emph{Back
  Substitution}

Pivot -- must be non-zero

Row Exchange is required when there is a Zero in the Pivot Position

$Ux = c$

possibility of Failure

\emph{Back Substitution} -- solving Equations of an Upper-triangular System in
reverse order


MIT 18.06 Lec. 3

\emph{Gauss-Jordan}

Reduced Row Echelon Form

computing Matrix Inverses: augment with the Identity Matrix and continue Row
Operations until the original is equal to the Identity; at the end of the
process the Inverse Matrix will be on the augmented RHS



\paragraph{Echelon Form}\label{sec:echelon_form}\hfill

\emph{Row Echelon Form}

\emph{Column Echelon Form}



\subparagraph{Reduced Echelon Form}\label{sec:reduced_echelon}\hfill

or \emph{Canonical Form}



% ------------------------------------------------------------------------------
\subsection{Diagonally Dominant Matrix}\label{sec:diagonally_dominant}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Irreducible Matrix}\label{sec:irreducible_matrix}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Complex Matrix}\label{sec:complex_matrix}
% ------------------------------------------------------------------------------

\subsubsection{Conjugate Transpose}\label{sec:conjugate_transpose}

The \emph{Conjugate Transpose} (or \emph{Hermitian Transpose}) of an $m \times
n$ Complex Matrix $A$ is the $n \times m$ Matrix $A^*$ resulting from the
Transpose (\S\ref{sec:transpose}) of $A$ followed by taking the Complex
Conjugate (\S\ref{sec:complex_conjugate}) of each entry.

if a Matrix $A$ is Full Rank, then $A^*A$ is Invertible



\subsubsection{Normal Matrix}\label{sec:normal_matrix}

\subsubsection{Unitary Matrix}\label{sec:unitary_matrix}

Complex analogue of an Orthogonal Matrix (\S\ref{sec:orthogonal_matrix})

A \emph{Unitary Matrix} $U$ has the Property that:
\[
  U^*U = UU^* = I
\]
where $U^* = \overline{U^T}$ is the Conjugate Transpose
(\S\ref{sec:conjugate_transpose}) of $U$

Unitary Matrices (like Orthogonal Matrices) are exactly the class of Matrices
that preserve the lengths of their inputs

\fist Special Unitary Group (\S\ref{sec:special_unitary})
$\mathrm{SU}(n)$: the Lie Group of $n \times n$ Unitary Matrices with
Determinant $1$

the Frobenius Norm and the Euclidean Norm ($2$-norm \S\ref{sec:matrix_norm}))
are both invariant under Unitary Multiplication, i.e. Left or Right Multiplying
a Matrix $A$ by an Unitary Matrix $Q$ results in a Matrix with the same Norm as
the original Matrix $A$

only a Unitary Matrix can be used as a Quantum Gate in Quantum Computation

Polar Decomposition (\S\ref{sec:polar_decomposition})



\subsubsection{Hermitian Matrix}\label{sec:hermitian_matrix}

or \emph{Self-adjoint Matrix}

a Complex Square Matrix that is equal to its own Conjugate Transpose:
\[
  A^* = A
\]

Complex extension of Real Symmetric Matrices (\S\ref{sec:symmetric_matrix})

Positive-definite Matrices (\S\ref{sec:positive_definite}): the Leading
Principle Minor (\S\ref{sec:minor}) for Hermitian Matrices can be used to test
for Positive Definiteness, and the Principal Minors can be used to test for
Postive Semi-definiteness (\S\ref{sec:positive_semidefinite})

Polar Decomposition (\S\ref{sec:polar_decomposition})

\begin{itemize}
\item an Orthogonal Projection (\S\ref{sec:orthogonal_projection}) is exactly an
  Idempotent Hermitian Linear Operator
\item Density Operator (\S\ref{sec:density_operator})
\item Self-adjoint Operators (\S\ref{sec:self_adjoint_operator})
\end{itemize}



\paragraph{Gramian Matrix}\label{sec:gramian_matrix}\hfill

or \emph{Gram Matrix}

Hermitian Matrix of Inner Products

Set of Vectors $\vec{v}_1,\ldots,\vec{v}_n$ in an Inner Product Space

Gramian Matrix $G$ with entries:
\[
  G_{ij} = \langle{\vec{v}_i,\vec{v}_j}\rangle
\]

a Set of Vectors is Linearly Independent if and only if the Determinant is
Non-zero

the Gramian Matrix of any Orthonormal Basis is the Identity Matrix

a Matrix is Positive Semidefinite (\S\ref{sec:positive_semidefinite}) if it is
the Gramian Matrix of some Vectors



\subsubsection{Skew Hermitian Matrix}\label{sec:skew_hermitian}

or \emph{Antihermitian}



% ------------------------------------------------------------------------------
\subsection{Positive-definite Matrix}\label{sec:positive_definite}
% ------------------------------------------------------------------------------

A \emph{Positive Definite Matrix} has the Property:
\[
  \vec{x}^* A \vec{x} > 0
\]
for any $\vec{x} \neq \vec{0}$, which implies that $A$ is a Symmetric
(\S\ref{sec:symmetric_matrix}), $A^T = A$, or Hermitian
(\S\ref{sec:hermitian_matrix}), $A^* = A$.

An $n \times n$ Real Matrix $M$ is Positive Definite if the Scalar
$\vec{x}^TM\vec{x}$ is \emph{Strictly Positive} ($>0$) for every Non-zero
Column Vector $\vec{x}$.

A general Quadratic Form (\S\ref{sec:quadratic_form}) $f(\vec{x})$ on $n$ Real
Variables $x_1,\ldots,x_n$ can always be written as $\vec{x}^T M \vec{x}$ with
$M$ a Symmetric Real Matrix; $M$ is a \emph{Positive-definite Matrix} when $f$
has a unique Minimum (Zero) when $\vec{x}$ is Zero and is Strictly Positive for
any other $\vec{x}$. (FIXME: clarify)

Definiteness can be checked by considering all Eigenvalues of $A$, or by
checking the Signs of the Principle Minors (\S\ref{sec:minor}) of $A$.

equivalent statements of Positive Definiteness:
\begin{itemize}
  \item $\vec{x}^*A\vec{x} > 0$, $\forall \vec{x} \neq \vec{0}$
  \item all Eigenvalues of $A$ are Real and Positive
  \item all Principal Minors (\S\ref{sec:minor}) are Positive \fist Second
    Derivative Test (\S\ref{sec:second_derivative_test})
  \item all Pivot Elements are Positive in ``naive'' (no Row Swaps or Scalar
    Multiplications) Gaussian Elimination (\S\ref{sec:gaussian_elimination})
    where Pivots are Diagonal entries of the Row Echelon Form
  \item the Matrix has a Cholesky Factorization
    (\S\ref{sec:cholesky_decomposition})
\end{itemize}

UC Math 352 Lec. 17 \url{https://www.youtube.com/watch?v=5tY3G1895_c}

``quick partial tests'' -- if any of these tests fail, then $A$ is not
Positive-definite:
\begin{enumerate}
  \item $A = A^*$
  \item $\forall{i}, a_{ii} > 0$
  \item the Largest element of $A$ (in Modulus) is on the main Diagonal
    --FIXME: explain
  \item Gerschg\"orin's Circle Theorem (TODO)
  \item if (1.)-(4.) succeed, then compute the Cholesky Factorization
    (\S\ref{sec:cholesky_decomposition})
\end{enumerate}

\emph{Minimizing a Complex Quadratic Form} (\S\ref{sec:quadratic_form})

\fist Definite Quadratic Form (\S\ref{sec:definite_quadratic})

\fist Quadratic Approximation (\S\ref{sec:quadratic_approximation})

Optimization Problems:

\fist Multiple Regression (\S\ref{sec:multiple_regression})

\fist Quadratic Programming (\S\ref{sec:quadratic_programming})

Generally a Real Function $f$ on $n$ Real Variables has local Minimum at
$\vec{x}_0$ if its Gradient (\S\ref{sec:gradient}) $\nabla f$ is Zero and its
Hessian Matrix (\S\ref{sec:hessian}) $H(f)$ is Positive Semi-definite at
that Point

An \emph{Indefinite Matrix} is a Matrix that is not Positive-definite,
Positive-semidefinite ($\geq 0$ \S\ref{sec:positive_semidefinite}),
Negative-definite ($< 0$ \S\ref{sec:negative_definite}), nor
Negative-semidefinite ($\leq 0$ \S\ref{sec:negative_semidefinite}).

\begin{itemize}
  \item $I$ -- Identity Matrix:
    \[
      \vec{x}^*I\vec{x} = \sum |x_i|^2 > 0
    \]
  \item Diagonal Matrices $D$ with Positive Real Diagonal entries ($d_{ii} >
    0$):
    \[
      \vec{x}^*D\vec{x} = \sum d_{ii}|x_i|^2 > 0
    \]
  \item Inner Product (\S\ref{sec:inner_product}) of Euclidean Vectors
    (\S\ref{sec:euclidean_vector})
\end{itemize}


UC Math 352 Lec. 16 - \url{https://www.youtube.com/watch?v=Z9hkjXYY23o}



\paragraph{Sylvester's Law of Inertia}\label{sec:sylversters_law}\hfill



\subsubsection{Negative Definite Matrix}\label{sec:negative_definite}

$A$ is \emph{Negative Definite} if $-A$ is Positive Definite



\subsubsection{Positive Semidefinite Matrix}\label{sec:positive_semidefinite}

A \emph{Positive Semidefinite Matrix} $A$ has the Property:
\[
  \vec{x}^* A \vec{x} \geq 0
\]
for any $\vec{x} \neq \vec{0}$

$A \succeq 0$

a Matrix is Positive Semidefinite if it is the Gramian Matrix
(\S\ref{sec:gramian_matrix}) of some Vectors

Positive-definite Matrices (\S\ref{sec:positive_definite}): the Leading
Principle Minor (\S\ref{sec:minor}) for Hermitian Matrices can be used to test
for Positive Definiteness, and the Principal Minors can be used to test for
Postive Semi-definiteness (\S\ref{sec:positive_semidefinite})

\begin{itemize}
  \item Diagonal Matrices $D$ with Zero or Positive Real Diagonal entries
    ($d_{ii} \geq 0$):
    \[
      \vec{x}^*D\vec{x} = \sum d_{ii}|x_i|^2 \geq 0
    \]
  \item Covariance Matrix (\S\ref{sec:covariance_matrix})
  \item ...
\end{itemize}

\fist Positive Semidefinite Cone (\S\ref{sec:positive_semidefinite_cone})



\subsubsection{Negative Semidefinite Matrix}\label{sec:negative_semidefinite}

$A$ is \emph{Negative Semidefinite} if $-A$ is Positive Semidefinite



% ==============================================================================
\section{Numerical Linear Algebra}\label{sec:numerical_linear_algebra}
% ==============================================================================

\fist Numerical Analysis (\S\ref{sec:numerical_analysis})



% ==============================================================================
\section{Multilinear Algebra}\label{sec:multilinear_algebra}
% ==============================================================================

\emph{Index Calculus} of Tensors and Vectors: Contravariant Vectors
(\S\ref{sec:vector_contravariance}), Covariant Vectors
(\S\ref{sec:vector_covariance}), Metric Tensor (\S\ref{sec:metric_tensor})



% ------------------------------------------------------------------------------
\subsection{Vector Covariance}\label{sec:vector_covariance}
% ------------------------------------------------------------------------------

in Non-Cartesian Coordinates, e.g. Skew Coordinates \S\ref{sec:skew_coordinates}

Coordinates given by Projection onto the Basis Vectors

Metric Tensor (\S\ref{sec:metric_tensor})



% ------------------------------------------------------------------------------
\subsection{Vector Contravariance}\label{sec:vector_contravariance}
% ------------------------------------------------------------------------------

in Non-Cartesian Coordinates, e.g. Skew Coordinates \S\ref{sec:skew_coordinates}

Metric Tensor (\S\ref{sec:metric_tensor})



% ------------------------------------------------------------------------------
\subsection{Multilinear Map}\label{sec:multilinear_map}
% ------------------------------------------------------------------------------

(wiki):

there is a one-to-one correspondence between Multilinear Maps
$f : V_1 \times \cdots \times V_n \rightarrow W$
and Linear Maps (\S\ref{sec:linear_transformation})
$F : V \otimes \cdots \otimes V_n \rightarrow W$
where $V_1 \otimes \cdots \otimes V_n$ is the Tensor Product
(\S\ref{sec:tensor_product}) of $V_1, \ldots, V_n$ and:
\[
  F(v_1 \otimes \cdots \otimes v_n) = f(v_1, \ldots, v_n)
\]



\subsubsection{Bilinear Map}\label{sec:bilinear_map}

A \emph{Bilinear Map} is a Function $B : V \times W \rightarrow X$ where
$V, W, X$ are Vector Spaces over a Field $F$ such that for any $\vec{w} \in W$:
\[
  B_{\vec{w}} = \vec{v} \mapsto B(\vec{v}, \vec{w})
\]
is a Linear Map from $V$ to $X$ and for any $\vec{v} \in V$:
\[
  B_{\vec{v}} = \vec{w} \mapsto B(\vec{v}, \vec{w})
\]
is a Linear Map from $W$ to $X$.

If $V = W$ and
$\forall\vec{v},\vec{w} \in V, B(\vec{v},\vec{w}) = B(\vec{w},\vec{v}])$
then $B$ is a Symmetric Bilinear Map (\S\ref{sec:symmetric_bilinear_map})

\begin{itemize}
\item Bilinear Product (\S\ref{sec:bilinear_product}) -- Algebra $A$ over a Ring
  (\S\ref{sec:r_algebra}) or Field $R$ (\S\ref{sec:k_algebra}):
  \[
    B : A \times A \rightarrow A
  \]
\item Bilinear Form (\S\ref{sec:bilinear_form})
  \[
    B : V \times V \rightarrow F
  \]
\item Sesquilinear Form (\S\ref{sec:sesquilinear_form}) -- Conjugate Linear in
  one Argument
\end{itemize}

(wiki): by the Universal Property (\S\ref{sec:universal_property}) of the Tensor
Product (\S\ref{sec:tensor_product}) there is a canonical correspondence between
\emph{Bilinear Forms} on $V$ and \emph{Linear Maps} $V \otimes V \to K$



\paragraph{Symmetric Bilinear Map}\label{sec:symmetric_bilinear_map}\hfill

Bilinear Map $B : V \times W \rightarrow X$ such that $V = W$ and
$\forall \vec{v},\vec{w} \in V, B(\vec{v},\vec{w}) = B(\vec{w},\vec{v}])$



\subsubsection{Alternating Multilinear Map}\label{sec:alternating_map}

Alternating Multilinear Forms (\S\ref{sec:alternating_form})



% ------------------------------------------------------------------------------
\subsection{Multilinear Form}\label{sec:multilinear_form}
% ------------------------------------------------------------------------------

\subsubsection{Bilinear Form}\label{sec:bilinear_form}

$B : V \times V \rightarrow K$

generalization of Dot Product (\S\ref{sec:inner_product}) of Euclidean Space

Sesquilinear Form (\S\ref{sec:sesquilinear_form}) generalization of Bilinear
Forms to cases where $K$ is the Field of Complex Numbers and the mapping is
Antilinear (Conjugate-linear \S\ref{sec:antilinear_map}) in one argument.

An Inner Product (\S\ref{sec:inner_product}) is a Sesquilinear Form and an
Inner Product over a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}).

\fist System of Bilinear Equations (\S\ref{sec:bilinear_equation_system}) -- a
collection of Equations, each written as a Bilinear Form

the Classical Subgroups of the General Linear Group
(\S\ref{sec:general_linear_group}) $GL(V)$ preserving a kind of Bilinear Form
on a Vector Space $V$:
\begin{itemize}
  \item the Orthogonal Group (\S\ref{sec:orthogonal_group}) $O(V)$ -- preserves
    a Non-degenerate Quadratic Form (\S\ref{sec:quadratic_form}) on $V$
  \item the Symplectic Group (\S\ref{sec:symplectic_group}) $Sp(V)$
    -- preserves a Symplectic Form (Non-degenerate Alternating Form
    \S\ref{sec:symplectic_form}) on $V$
  \item the Unitary Group (\S\ref{sec:unitary_group}) $U(V)$
    -- preserves a Non-degenerate Hermitian Form (\S\ref{sec:hermitian_form})
    on $V$
\end{itemize}

Derived Quadratic Form

Symplectic Vector Spaces (\S\ref{sec:symplectic_vectorspace}) are equipped with
a Nondegenerate Alternating (Skew-symmetric) Bilinear Form called a
\emph{Symplectic Form} \S\ref{sec:symplectic_form})



\paragraph{Orthogonality}\label{sec:orthogonality}\hfill

Two Vectors $\vec{u}, \vec{v}$ in a Vector Space with Bilinear Form $B$ are are
\emph{Orthogonal} (or \emph{Perpendicular}) when $B(u,v) = 0$, denoted:
\[
  \vec{u} \bot \vec{v}
\]
The Zero Vector, $\vec{0}$, is Orthogonal to every Vector (including itself).

\fist Inner Product (\S\ref{sec:inner_product})

\fist Orthonormality (\S\ref{sec:orthonormality})

cf. \emph{Un-correlatedness} (\S\ref{sec:correlation})

Mandelbrot97E, Ch.20: when $\expect(P(t + s) - P(t))^2$ is Finite for all $t$
and $s$ and $P(t)$ is a Martingale (\S\ref{sec:martingale_process}), and price
increments are Uncorrelated and (``Orthogonal'' or ``Spectrally White''); if
$P(t)$ is Gaussian, then Orthogonality becomes synonymous with Independence
(\S\ref{sec:independence}) and a Gaussian Martingale can only be a Gaussian
Random Walk

Two Linear Subspaces (\S\ref{sec:linear_subspace}) $A, B \subset V$ of an Inner
Product Space $V$ are called \emph{Orthogonal Subspaces}
(\S\ref{sec:orthogonal_subspace}) if each Vector in $A$ is Orthogonal to each
Vector in $B$.

a Set of Vectors is Orthogonal if every Pair of Vectors in it is Orthogonal; a
Set is Orthonormal if it is Orthogonal and every Vector in it is a Unit Vector



\subparagraph{Orthogonal Function}\label{sec:orthogonal_function}\hfill

(Wasserman04, Ch.21)

Orthogonal Function Density Estimation (\S\ref{sec:density_estimation}),
Orthogonal Series Regression Estimator

Cosine Basis

Polynomial Basis -- Legendre Polynomials
(\S\ref{sec:orthogonal_polynomial_sequence})

Orthonormal Wavelet Basis, Haar Basis \fist Multiresolution Analysis
(\S\ref{sec:multiresolution_analysis})



\paragraph{Symmetric Bilinear Form}\label{sec:symmetric_bilinear}\hfill

e.g. Dot (Scalar) Product (Inner Product \S\ref{sec:inner_product}) on
Euclidean Vector Spaces

Metric Tensor (\S\ref{sec:metric_tensor}) is a Non-degenerate Symmetric Bilinear
Form on each Tangent Space that varies Smoothly from Point to Point

(wiki):

Symmetric Bilinear Forms on Finite-dimensional Vector Spaces correspond
precisely to Symmetric Matrices (\S\ref{sec:symmetric_matrix}) given a Basis
for $V$

Symmetric Bilinear Forms over a Vector Space correspond one-to-one with
Quadratic Forms (\S\ref{sec:quadratic_form}) over the Vector Space: given a
Symmetric Bilinear Form $B$, the Function $q(x) = B(x,x)$ is the associated
Quadratic Form on the Vector Space

An Inner Product (\S\ref{sec:inner_product}) on a Real Vector Space is
a Positive-definite (\S\ref{sec:definite_quadratic}) Symmetric
Bilinear Form.

Clifford Algebras (\S\ref{sec:clifford_algebra}) represent the same structure
for Non-degenerate Symmetric Bilinear Forms that Weyl Algebras (Symplectic
Clifford Algebras \S\ref{sec:weyl_algebra}) represent for Symplectic Bilinear
Forms (\S\ref{sec:symplectic_form})



\paragraph{Degenerate Bilinear Form}
\label{sec:degenerate_bilinear_form}\hfill

Pseudo-Riemannian Manifold (\S\ref{sec:pseudo_riemannian})



\paragraph{Skew-symmetric Bilinear Form}
\label{sec:skew_symmetric_bilinear}\hfill

(or \emph{Antisymmetric})

$B(\vec{v},\vec{w}) = -B(\vec{w},\vec{v})$

each \emph{Alternating Bilinear Form} (\S\ref{sec:alternating_form}):

$B(\vec{v},\vec{v}) = 0$ for all $\vec{v} \in V$

is Skew-symmetric

\fist cf. Skew-symmetric Matrix (\S\ref{sec:skew_symmetric_matrix})



\subparagraph{Symplectic Bilinear Form}\label{sec:symplectic_bilinear}\hfill

$\omega : V \times V \rightarrow F$

a Non-degenerate Alternating (Skew-symmetric) Bilinear Form
(\S\ref{sec:alternating_form})

a Symplectic Vector Space (\S\ref{sec:symplectic_vectorspace}) is one equipped
with a Symplectic Bilinear Form

given a choice of Basis, $\omega$ can be represented as a Skew-symmetric
(\S\ref{sec:skew_symmetric_matrix}), Invertible (Non-singular
\S\ref{sec:invertible_matrix}), Hollow (\S\ref{sec:hollow_matrix}) Matrix; if
$V$ is Finite-dimensional then it must be \emph{Even-dimensional} since every
Skew-symmetric Hollow Matrix of odd size has Determinant \emph{Zero} (i.e. is
Non-invertible)

note this Matrix is \emph{not} the same as a Symplectic Matrix (a Symplectic
Transformation of the Space TODO: xref)



\subsubsection{Alternating Multilinear Form}\label{sec:alternating_form}

Alternating Multilinear Map (\S\ref{sec:alternating_map})

Set of all Alternating Multilinear Forms is a Vector Space

Alternating Bilinear Form: $B(\vec{v},\vec{v}) = 0$ for all $\vec{v} \in V$

each \emph{Alternating Bilinear Form} is Skew-symmetric
(\S\ref{sec:skew_symmetric_bilinear})

a Symplectic Form (\S\ref{sec:symplectic_form}) is a Non-degenerate Alternating
Bilinear Form



\subsubsection{Sesquilinear Form}\label{sec:sesquilinear_form}

generalization of Bilinear Forms (\S\ref{sec:bilinear_form}) to cases where $K$
is the Field of Complex Numbers and the mapping is Antilinear (Conjugate-linear
\S\ref{sec:antilinear_map}) in one argument

Inner Products (\S\ref{sec:inner_product}) are Sesquilinear Forms



\paragraph{Hermitian Form}\label{sec:hermitian_form}\hfill

or \emph{Symmetric Sesquilinear Form}

an Inner Product (\S\ref{sec:inner_product}) is a Positive-definite
(\S\ref{sec:definite_quadratic}) Hermitian Form

Unitary Group (\S\ref{sec:unitary_group}) $U(V)$ -- Subgroup of General Linear
Group $GL(V)$ preserving a Non-degenerate Hermitian Form on $V$



% ------------------------------------------------------------------------------
\subsection{Tensor}\label{sec:tensor}
% ------------------------------------------------------------------------------

%FIXME: move section ???

a \emph{Tensor} is a collection of Vectors (\S\ref{sec:vector}) and Covectors
(Linear Forms \S\ref{sec:linear_form}) combined using Tensor Product
(\S\ref{sec:tensor_product})

given a Coordinate Basis, a Tensor can be represented as a Multi-dimensional
Array of ``Numerical Values'' %FIXME

\fist Tensor (Linear Algebra \S\ref{sec:linear_tensor})

\fist Metric Tensor (\S\ref{sec:metric_tensor})

the \emph{Order} (or \emph{Degree} or \emph{Rank}) of a Tensor is the
Dimensionality of the Array needed to represent it:
\begin{itemize}
  \item 0th-order Tensor: \emph{Scalar}
  \item 1st-order Tensor: \emph{Vector}
  \item 2nd-order Tensor: \emph{Matrix}
\end{itemize}

the ``\emph{Type}'' of a Tensor is described by a pair $(n, m)$ where $n$ is the
number of Contravariant Indices and $m$ is the number of Covariant indices, and
$n + m$ gives the total Order of the Tensor

wiki: Abstract Object (\S\ref{sec:mathematical_object}) expressing a
``Definite Type'' of some ``Multi-linear Concept''

Linear Algebra (Part \ref{part:linear_algebra})

Multilinear Algebra (\S\ref{sec:multilinear_algebra}) --
(wiki): there is a one-to-one correspondence between Multilinear Maps
(\S\ref{sec:multilinear_map})
$f : V_1 \times \cdots \times V_n \rightarrow W$
and Linear Maps (\S\ref{sec:linear_transformation})
$F : V \otimes \cdots \otimes V_n \rightarrow W$
where $V_1 \otimes \cdots \otimes V_n$ is the Tensor Product
(\S\ref{sec:tensor_product}) of $V_1, \ldots, V_n$ and:
\[
  F(v_1 \otimes \cdots \otimes v_n) = f(v_1, \ldots, v_n)
\]

Symmetric Monoidal Category (\S\ref{sec:symmetric_monoidal})

(wiki): by the Universal Property (\S\ref{sec:universal_property}) of the Tensor
Product (\S\ref{sec:tensor_product}) there is a canonical correspondence between
\emph{Bilinear Forms} (\S\ref{sec:bilinear_form}) on $V$ and \emph{Linear Maps}
$V \otimes V \to K$

(wiki):

definition of a Tensor using the Representations of the General Linear Group:
for the Action of the General Linear Group on the Set of all Ordered Bases of an
$n$-dimensional Vector Space (TODO), the Set of all Ordered Bases is a Principal
Homogeneous Space for $GL(n)$, and for a Vector Space $W$ and
$\rho : GL(n) \to GL(W)$ the Representation of $GL(n)$ on $W$ (a Group
Homomorphism), then a \emph{Tensor} of ``Type $\rho$'' is an Equivariant Map
$T : F \to W$, i.e.:
\[
  T(FR) = \rho(R^{-1})T(F)
\]
when $\rho$ is a \emph{Tensor Representation} of the General Linear Group, this
is the usual definition of Tensors as Multidimensional Arrays
--TODO: xrefs

\asterism

``Component-free Notation'' (Mathematical notation) -- $\nabla_X$

``Abstract Index Calculus'' (Physics notation)

Spivak99

Penrose71: \emph{Tensor Diagram Notation} (\emph{Penrose Graphical Notation})

2019 - Bradley - \emph{Matrices as Tensor Network Diagrams} (article) -
\url{https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams}



\subsubsection{Scalar}\label{sec:scalar}

0th-order Tensor

Element of the Field used to define a Vector Space
(\S\ref{sec:vector_space})



\subsubsection{Vector}\label{sec:vector}

1st-order Tensor

Element of a Vector Space (\S\ref{sec:vector_space})



\subsubsection{Matrix}\label{sec:matrix}

2nd-order Tensor

Matrix (Linear Algebra \S\ref{sec:matrix_theory}):
Linear Transformations (\S\ref{sec:linear_transformation})

Laplacian Matrix (Graph Theory \S\ref{sec:laplacian_matrix})

\url{https://www.youtube.com/watch?v=cu718EbCOPs} Spivak 16: relates
Matrix Operations to Wiring Diagrams %FIXME

Matrix Operations:
\begin{itemize}
\item Transpose -- $M^T$
\item Complex Conjugate -- $\overline{M}$
\item Conjugate Transpose (Hermetian Transpose) -- $M^* = \overline{M^T}$
  (the notation $M^\dag$ is used in Quantum Mechanics)
\item Multiplication -- $MN$
\item Kronecker -- $M \otimes N$
\item Khatri-Rao -- $M \bigodot N$
\item Trace -- $Tr(M)$
\item Entrywise (Hadamard) Product -- $M \circ N$
\item Marginalize -- $\Sigma_i M_{i,j}$
\item Grand Sum -- $\Sigma M_{i,j}$ or $e^T M e$ where $e$ is a Column Vector of
  $1$s
\end{itemize}



\subsubsection{Unit Tensor}\label{sec:unit_tensor}

$\hat{i}\hat{i} + \hat{j}\hat{j} + \cdots = I$ (the Identity Matrix
\S\ref{sec:identity_matrix})



\subsubsection{Symmetric Tensor}\label{sec:symmetric_tensor}

Invariant under a Permutation of its Vector Arguments

the Space of Symmetric Tensors of Order $r$ on a Finite-dimensional Vector
Space is naturally Isomorphic to the Dual of the Space of Homogeneous
Polynomials (\S\ref{sec:homogeneous_polynomial}) of Degree $r$ on $V$



\subsubsection{Infinite Dimensional Tensor}
\label{sec:infinite_dimensional_tensor}

Inequivalent Topologies lead to Inequivalent notions of Tensor and
vaious Isomorphisms may or may not hold depending on the notion of
Tensor

\emph{Tensor Product of Hilbert Spaces} (\S\ref{sec:hilbert_space}):
special case of Topological Tensor Product
(\S\ref{sec:topological_tensor})



% ------------------------------------------------------------------------------
\subsection{Exterior Algebra}\label{sec:exterior_algebra}
% ------------------------------------------------------------------------------

(nlab): for a Vector Space $V$ over a Field $K$, the \emph{Exterior Algebra}
$\Lambda{V}$ is generated from the Elements of $V$ using the operations of
Addition and Scalar Multiplication and the Associative Binary Operation $\wedge$
called the \emph{Exterior Product} or \emph{Wedge Product} subject to the
identities necessary for $\Lambda{V}$ to be an Associative Algebra and the
identity:
\[
  v \wedge v = 0
\]
for all $v \in V$, from which it follows thta $\Lambda{V}$ is a Graded
(Commutative) Algebra (\S\ref{sec:graded_algebra}) where $\Lambda^k{V}$ is
Spanned by $k$-fold Wedge Products, i.e. Elements of the form $v_1 \wedge \cdots
\wedge v_k$

generalized to an Object $V \in \cat{C}$ of a Symmetric Monoidal Category
$\cat{C}$

\fist Differential Forms (\S\ref{sec:differential_form}) of Multivariable
Calculus -- (nlab): a \emph{Differential Form} (or \emph{Exterior Differential
  Form}) on a Generalized Smooth Space $X$ is a Section (\S\ref{sec:section}) of
the Exterior Algebra of the Cotangent Bundle (\S\ref{sec:cotangent_bundle}) over
$X$

\asterism

on an $n$-dimensional Vector Space $V$ is a $2^n$-dimensional Vector Space
$\bigwedge V$ of \emph{$p$-Vectors} called the \emph{Exterior Product Space};
with the Wedge Product $u \wedge v$ on $u,v \in V$ gives an Exterior Algebra
which is Closed with respect to $\wedge$

$\bigwedge V$ consists of $n+1$ Subspaces $\bigwedge^p{V}$ each with Dimension
$\binom{n}{p}$ for $p \in \{0, 1, \ldots, n\}$

Grassmann Algebra (\S\ref{sec:grassmann_algebra})

\fist Grassmann Numbers (\S\ref{sec:grassmann_number}) -- generalization of Dual
Numbers (\S\ref{sec:dual_number}) as the Exterior Algebra over a
One-dimensional Vector Space; generated by \emph{Anti-commuting Elements}, e.g.
Differential Forms

\fist cf. Exterior Derivative (\S\ref{sec:exterior_derivative}) %FIXME

``Oriented Density'' %FIXME

\emph{Exterior Product} (or \emph{Wedge Product} \S\ref{sec:exterior_product})
$\wedge$

\emph{Interior Product} (or \emph{Interior Derivative}
\S\ref{sec:interior_product})

the Exterior Derivative (\S\ref{sec:exterior_derivative}) gives the Exterior
Algebra of Differential Forms on a Manifold the structure of a Differential
Algebra (\S\ref{sec:differential_algebra})

\asterism

Mathview - Exterior Algebra - \url{https://www.youtube.com/watch?v=-6F74TH1i_g}



\subsubsection{Bivector}\label{sec:bivector}

$2$-vector

Isomorphic to Skew-symmetric Matrices (\S\ref{sec:skew_symmetric_matrix})

Psuedovectors (Axial Vectors \ref{sec:pseudovector}) are equivalent to
Three-dimensional Bivectors



\subsubsection{Exterior Product}\label{sec:exterior_product}

(\emph{Wedge Product})

$\wedge$

Antisymmetric Tensor Product of $A, B \in V_n$:
\[
  A \wedge B = A \otimes B - B \otimes A
\]

$\wedge 0 = 1$

is an Alternating Product (\S\ref{sec:alternating_algebra}) on Elements of $V$,
i.e. for all $x \in V$:
\[
  x \wedge x = 0
\]
and it follows that the Exterior Product is also Anticommutative on Elements of
$V$:
\[
  x \wedge y = -(y \wedge x)
\]

Under the Wedge (Exterior) Product the de Rham Complex
(\S\ref{sec:derham_complex}) becomes a Differential Graded Algebra
(\S\ref{sec:differential_graded_algebra})



\subsubsection{Interior Product}\label{sec:interior_product}

or \emph{Interior Derivative}

Degree $-1$ Antiderivation (\S\ref{sec:antiderivation}) on the Exterior Algebra
of Differential Forms (\S\ref{sec:differential_form}) on a Smooth Manifold
(\S\ref{sec:smooth_manifold})




\subsubsection{Exterior Derivative}\label{sec:exterior_derivative}\hfill

or \emph{Differential}

Operation on Differential Forms (\S\ref{sec:differential_form}) that when acting
on a $k$-form produces a $k+1$-form; extends the Differential
(\S\ref{sec:differential}) of a Function and is directly related to the Curl and
Divergence of a Vector Field

an example of an Antiderivation (\S\ref{sec:antiderivation})

Generalized Stokes' Theorem

the Exterior Derivative gives the Exterior Algebra
(\S\ref{sec:exterior_algebra}) of Differential Forms on a Manifold the structure
of a Differential Algebra (\S\ref{sec:differential_algebra})

\fist cf. Interior Derivative (Interior Product \S\ref{sec:interior_product})

``Oriented Density'' %FIXME

\asterism

The \emph{de Rham Complex} (\S\ref{sec:derham_complex}) is the Cochain Complex
of Differential Forms on some Smooth Manifold (\S\ref{sec:smooth_manifold}) $M$
with the Exterior Derivative as the Differential (\S\ref{sec:differential}):
\[
  0 \rightarrow \Omega^0(M) \xrightarrow{d} \Omega^1(M) \xrightarrow{d}
  \Omega^2(M) \xrightarrow{d} \Omega^3(M) \rightarrow \cdots
\]
where $\Omega^0(M)$ is the Space of Smooth Functions on $M$, $\Omega^1(M)$ is
the Space of $1$-forms, etc.; Differential Forms which are the Image of other
Forms under the Exterior Derivative plus the Constant $0$ Function in
$\Omega^0(M)$ are called \emph{Exact} (\S\ref{sec:exact_differential_form}) and
Differential Forms whose Exterior Derivative is $0$ are called \emph{Closed}
(\S\ref{sec:closed_differential_form}); the relationship $d^2 = 0$ says that
Exact Forms are Closed (FIXME: clarify), but the converse is not generally true
(Closed Differential Forms need not be Exact, e.g. for the $1$-form of Angle
Measurement on the Unit Circle $\diffy{\theta}$ there is no actual Function
$\theta$ defined on the whole Circle of which $\diffy{\theta}$ is the
Derivative)

Coboundary Map (i.e. the ``Differential'') in the de Rham Complex is the de Rham
Differential (\S\ref{sec:derham_differential}), ``Exterior Derivative'' acting
on Differential Forms

Under the Wedge (Exterior) Product (\S\ref{sec:exterior_product}) the de Rham
Complex becomes a Differential Graded Algebra
(\S\ref{sec:differential_graded_algebra})



% ------------------------------------------------------------------------------
\subsection{Dyadic Algebra}\label{sec:dyadic_algebra}
% ------------------------------------------------------------------------------

\subsubsection{Dyadic Tensor}\label{sec:dyadic_tensor}

\subsubsection{Dyadic Product}\label{sec:dyadic_product}



% ==============================================================================
\section{$R$-algebra}\label{sec:r_algebra}
% ==============================================================================

Algebra over a Commutative Unital Ring

$A$ is an $R$-module Module (\S\ref{sec:module}) with a \emph{Bilinear Product}
(\S\ref{sec:bilinear_product}) $A \times A \rightarrow A$

Rationals as Initial Algebra
\url{https://ncatlab.org/nlab/show/continued+fraction#rationals_as_initial_algebra}



% ------------------------------------------------------------------------------
\subsection{Bilinear Product}\label{sec:bilinear_product}
% ------------------------------------------------------------------------------

Bilinear Map (\S\ref{sec:bilinear_map}) $\times : A \times A \rightarrow A$



% ------------------------------------------------------------------------------
\subsection{Algebra Homomorphism}\label{sec:algebra_homomorphism}
% ------------------------------------------------------------------------------

$F : A \rightarrow B$ such that:
\begin{enumerate}
  \item $F(k\vec{x}) \rightarrow kF(\vec{x})$
  \item $F(\vec{x} + \vec{y}) = F(\vec{x}) + F(\vec{y})$
  \item $F(\vec{x} \times \vec{y}) = F(\vec{x}) \times F(\vec{y})$
\end{enumerate}

Generalized by Bimodules (\S\ref{sec:bimodule})



% ------------------------------------------------------------------------------
\subsection{Non-associative $R$-algebra}
\label{sec:nonassociative_r_algebra}
% ------------------------------------------------------------------------------

For Commutative Unital Ring $R$, $R$-module $V$ with Bilinear Product
(\S\ref{sec:bilinear_product}) $V \otimes V \rightarrow V$



% ------------------------------------------------------------------------------
\subsection{Associative $R$-algebra}\label{sec:associative_r_algebra}
% ------------------------------------------------------------------------------

\emph{Associative Unital $R$-algebra}, $R$-module $V$ with Bilinear
Product (\S\ref{sec:bilinear_product}) $p : V \otimes V \rightarrow V$
Linear Map $i : R \rightarrow V$ satisfying Associative and Unit Laws



% ------------------------------------------------------------------------------
\subsection{Simple Algebra}\label{sec:simple_algebra}
% ------------------------------------------------------------------------------

\begin{itemize}
  \item contains no Non-trivial Two-sided Ideals
  \item Multiplication Operation is not Zero (i.e. $\exists a, b : ab \neq 0$)
\end{itemize}

examples:
\begin{itemize}
  \item Division Algebras (\S\ref{sec:division_algebra})
\end{itemize}

Simple Rings (\S\ref{sec:simple_ring}) can always be considered as a Simple
Algebra



% ------------------------------------------------------------------------------
\subsection{Graded Algebra}\label{sec:graded_algebra}
% ------------------------------------------------------------------------------

Algebra over a Graded Ring (\S\ref{sec:graded_ring})

Differential Graded Algebra (\S\ref{sec:differential_graded_algebra})

cf. Graded Vector Space (\S\ref{sec:graded_vectorspace})

\begin{itemize}
  \item Exterior Algebra (\S\ref{sec:exterior_algebra}) -- Graded Commutative
\end{itemize}



\subsubsection{Alternating Algebra}\label{sec:alternating_algebra}

Exterior Product (\S\ref{sec:exterior_product})



% ==============================================================================
\section{$K$-algebra}\label{sec:k_algebra}
% ==============================================================================

\emph{Algebra over a Field}

Vector Space (\S\ref{sec:vector_space}) with a \emph{Bilinear Product}
(\S\ref{sec:bilinear_product})

i.e. a Vector Space that has a ``sensible way'' to defined Vector
Multiplication; e.g. $\comps$ (\S\ref{sec:complex_number}) is an
$\reals$-algebra

Coalgebra (\S\ref{sec:coalgebra})

$F$-algebra (\S\ref{sec:f_algebra})

Every Finite Dimensional Associative Division Algebra over $\reals$ is
Isomorphic to either $\reals$, $\comps$, or $\quats$ (Theorem of
Frobenius) %FIXME



% ------------------------------------------------------------------------------
\subsection{Unital Algebra}\label{sec:unital_algebra}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Zero Algebra}\label{sec:zero_algebra}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\subsection{Associative Algebra}\label{sec:associative_algebra}
% ------------------------------------------------------------------------------

Coalgebra (\S\ref{sec:coalgebra}): Dual to a Unital Associative Algebra

\fist Associative Division Algebras (\S\ref{sec:associative_division_algebra})

the Set of all Linear Operators (Endomorphic Linear Transformations
\S\ref{sec:linear_operator}) $End(V)$ together with Addition, Composition, and
Scalar Multiplication forms an Associative Algebra with Identity Element over
the Field $K$ with the Identity Map for the Multiplicative Identity

\fist the Set of all Linear Automorphisms (Isomorphic Endomorphisms, i.e.
Invertible Matrices \S\ref{sec:invertible_matrix}) forms the General Linear
Group (or Automorphism Group) $GL(V$

a Banach Algebra (\S\ref{sec:banach_algebra}) is an Associative Algebra over
the Real or Complex Numbers (or over a Non-archimedean Complete Normed Field
\S\ref{sec:nonarchimedean_field}) that is also a Banach Space
(\S\ref{sec:banach_space})

\begin{itemize}
  \item the Dual Numbers (\S\ref{sec:dual_number}) form a Two-dimensional
    Commutative (\S\ref{sec:commutative_algebra}), Unital, Associative Algebra
    over the Real Numbers
\end{itemize}

\fist Representation of Associative Algebras
(\S\ref{sec:algebra_representation}) by Modules (\S\ref{sec:module})



\subsubsection{Frobenius Algebra}\label{sec:frobenius_algebra}

Representation Theory (\S\ref{sec:representation_theory})

Module Theory (\S\ref{sec:module})

Frobenius Algebras can be defined in any Monoidal Category (or
Polycategory \S\ref{sec:polycategory}) and are sometimes called
\emph{Frobenius Monoids} (\S\ref{sec:frobenius_monoid})

Star-autonomous Categories (\S\ref{sec:star_autonomous}) are Pseudo-Frobenius
Algebras
--\url{https://golem.ph.utexas.edu/category/2017/11/starautonomous_categories_are.html}



\subsubsection{Poisson Algebra}\label{sec:poisson_algebra}

\url{https://golem.ph.utexas.edu/category/2020/06/getting_to_the_bottom_of_noeth.html}

both a Jordan Algebra (\S\ref{sec:jordan_algebra}) and a Lie Algebra
(\S\ref{sec:lie_algebra})

Classical Mechanics -- Observables as Elements of a Poisson Algebra

cf. Complex $*$-algebra (\S\ref{sec:cstar_algebra}) -- Quantum Mechanics



% ------------------------------------------------------------------------------
\subsection{Non-associative Algebra}
\label{sec:nonassociative_algebra}
% ------------------------------------------------------------------------------

\subsubsection{Jordan Algebra}\label{sec:jordan_algebra}

Poisson Algebra (\S\ref{sec:poisson_algebra})



\paragraph{Formally Real Jordan Algebra}\label{sec:formally_real_jordan}\hfill

\url{https://golem.ph.utexas.edu/category/2020/08/octonions_and_the_standard_mod_3.html}

Algebras of Observables, or Spacetimes with ``future cones''



\paragraph{Exceptional Jordan Algebra}\label{sec:exceptional_jordan_algebra}
\hfill

``Exceptional'' (\S\ref{sec:exceptional})

2018 - Baez - \emph{Exceptional Quantum Geometry and Particle Physics} - (blog
article:
\url{https://golem.ph.utexas.edu/category/2018/08/exceptional_quantum_geometry_a.html})



\subparagraph{Albert Algebra}\label{sec:albert_algebra}\hfill



\subsubsection{Griess Algebra}\label{sec:griess_algebra}

Commutative Non-associative Algebra on a Real Vector Space of Dimension 196884
that has the Monaster Group (\S\ref{sec:monster_group}) $M$ as its Automorphism
Group



% ------------------------------------------------------------------------------
\subsection{Division Algebra}\label{sec:division_algebra}
% ------------------------------------------------------------------------------

cf. Fields (\S\ref{sec:field}) which are Commutative

example of a Simple Algebra (\S\ref{sec:simple_algebra})



\subsubsection{Associative Division Algebra}
\label{sec:associative_division_algebra}

\paragraph{Frobenius Theorem}\label{sec:frobenius_theorem}\hfill

every Finite-dimensional Associative Division Algebra over the Real Numbers is
Isomorphic to one of:
\begin{enumerate}
  \item $\reals$ -- Commutative
  \item $\comps$ -- Commutative
  \item $\quats$ -- Non-commutative
\end{enumerate}

the only Division Algebra over $\comps$ is $\comps$ itself

%FIXME: same theorem???
Vector Field (\S\ref{sec:vector_field}) formulation: the Subbundle
(\S\ref{sec:subbundle}) of the Tangent Bundle (\S\ref{sec:tangent_bundle}) of a
Manifold is Integrable (or Involutive) if and only if it arises from a
\emph{Regular Foliation} (\S\ref{sec:foliation})

Differential Forms formulation: TODO



\subsubsection{Composition Algebra}\label{sec:composition_algebra}

or \emph{Normed Division Algebra}



\paragraph{Hurwitz's Theorem}\label{sec:hurwitzs_theorem}\hfill

the only Normed Division Algebras over the Real Numbers are:
\begin{enumerate}
  \item $\reals$
  \item $\comps$
  \item $\quats$
  \item $\octs$
\end{enumerate}



% ------------------------------------------------------------------------------
\subsection{Coalgebra}\label{sec:coalgebra}
% ------------------------------------------------------------------------------

Dual to Unital Associative Algebra (\S\ref{sec:associative_algebra})

Cf. $F$-coalgebra (\S\ref{sec:f_coalgebra})

cf. Continued Fractions (\S\ref{sec:continued_fraction})

Reals as Terminal Coalgebra
\url{https://ncatlab.org/nlab/show/continued+fraction#half-open}



% ------------------------------------------------------------------------------
\subsection{Bialgebra}\label{sec:bialgebra}
% ------------------------------------------------------------------------------

Unital Associative Algebra and Coalgebra Vector Space over Field $K$



\subsubsection{Quasi-bialgebra}\label{sec:quasi_bialgebra}

\subsubsection{Hopf Algebra}\label{sec:hopf_algebra}

simultaneously a Unital Associative Algebra and a Counital
Coassociative Coalgebra with compatibility of these Structures making
it a Bialgebra, also equipped with an Anti-automorphism satisfying a
certain Property %FIXME what property?

Algebraic Topology



% ------------------------------------------------------------------------------
\subsection{Weyl Algebra}\label{sec:weyl_algebra}
% ------------------------------------------------------------------------------

(wiki):

\emph{Weyl Algebra} (or \emph{Symplectic Clifford Algebra}) -- represents the
same structure for Symplectic Bilinear Forms (\S\ref{sec:symplectic_form}) that
Clifford Algebras (\S\ref{sec:clifford_algebra}) represent for Non-degenerate
Symmetric Bilinear Forms (\S\ref{sec:symmetric_bilinear})

Ring of Differential Operators (\S\ref{sec:differential_operator}) with
Polynomial Coefficients in one Variable:
\[
  f_m(X)\partial^m_X + f_m{-1}(X)\partial^{m-1}_X + \cdots +
  f_1(X)\partial_X + f_0(X)
\]

for underlying Field $F$ and $F[X]$ the Ring of Polynomials in one Variable $X$
with Coefficients in $F$, then each $f_i$ lies in $F[X]$

$\partial_X$ is the Derivative with respect to $X$ and the Algebra is Generated
by $X$ and $\partial_X$

\fist Algebraic $D$-modules (\S\ref{sec:algebraic_d_module}) -- Modules over
Weyl Algebra $A_n(K)$ over a Field $K$ of Characteristic Zero; relates Weyl
Algebras to Differential Equations (\S\ref{sec:differential_equation})

a Weyl Algebra is Isomorphic to the Quotient of the Free Algebra
(\S\ref{sec:free_algebra}) on two Generators, $X$ and $Y$, by the Ideal
(\S\ref{sec:ring_ideal}) generated by the Element $YX - XY - 1$

``the'' Weyl Algebra is a Quotient of the Universal Enveloping Algebra
(\S\ref{sec:universal_enveloping_algebra}) of the Heisenberg Algebra (the Lie
Algebra \S\ref{sec:lie_algebra} of the Heisenberg Group) by setting the Central
Element of the Heisenberg Algebra $([X,Y])$ equal to the Unit of the Universal
Enveloping Algebra ($1$)

the $n$-th Weyl Algebra $A_n$ is the Ring of Differential Operators with
Polynomial Coefficients in $n$ Variables, generated by $X_i$ and
$\partial_{X_i}$, $i = 1, \ldots n$

is a Left and Right Noetherian Ring (\S\ref{sec:noetherian_ring})

example of a Simple Ring (\S\ref{sec:simple_ring}) that is not a Matrix Ring
over a Division Ring

example of a Non-commutative Domain Ring (\S\ref{sec:domain_ring})



% ------------------------------------------------------------------------------
\subsection{Geometric Algebra}\label{sec:geometric_algebra}
% ------------------------------------------------------------------------------

(wiki): the \emph{Geometric Algebra} of a Vector Space is an Algebra over a
Field where the Multiplication Operation is called the \emph{Geometric Product}
on a Space of \emph{Multivectors}, containing Scalars $F$ and the Vector Space
$V$

may be defined as a Clifford Algebra (\S\ref{sec:clifford_algebra}) of a Vector
Space with a Quadratic Form (\S\ref{sec:quadratic_form})

cf. \emph{Algebraic Geometry} (Part \ref{part:algebraic_geometry})

2019 - Gunn, De Keninck - \emph{Geometric Algebra for Computer Graphics} -
\url{https://www.youtube.com/watch?v=tX4H_ctggYo}:

Vector Algebra, Quaternions, Dual Quaternions, and Exterior Algebras are all
Sub-algebras of Geometric Algebra

cf. Projective Geometry (\S\ref{sec:projective_geometry}) as a foundation for
Geometric Algebra (Klein)

Projective Points: Euclidean Points + Euclidean Vectors

Quaternion Multiplication: Inner Product + Cross Product

Vector and Matrix Algebra is a natural choice for \emph{Analytic Geometry}:
Geometric Objects are represented by choosing axes, taking
\emph{measurements} of coefficients and transforming these measurements

\emph{Projective Geometric Algebra}

\emph{Elements} are \emph{Geometric Objects}: Points, Lines, Planes, Rotations,
Translations, ...



\subsubsection{Multivector}\label{sec:multivector}

\subsubsection{Grassmann Algebra}\label{sec:grassmann_algebra}

%FIXME: merge with exterior algebra ?

Exterior Algebra (\S\ref{sec:exterior_algebra})

\fist Grassmann Numbers (\S\ref{sec:grassmann_number}) -- generalization of Dual
Numbers (\S\ref{sec:dual_number}) as the Exterior Algebra over a
One-dimensional Vector Space; generated by \emph{Anti-commuting Elements}, e.g.
Differential Forms



\paragraph{$p$-vector}\label{sec:p_vector}\hfill



\subsubsection{Geometric Calculus}\label{sec:geometric_calculus}

cf. Differential Forms (\S\ref{sec:differential_form}), Differential Geometry
(\S\ref{sec:differential_geometry})



% ==============================================================================
\section{Super Linear Algebra}\label{sec:super_linear_algebra}
% ==============================================================================

% ------------------------------------------------------------------------------
\subsection{Superalgebra}\label{sec:superalgebra}
% ------------------------------------------------------------------------------

$Z_2$-graded Algebra



\subsubsection{Clifford Algebra}\label{sec:clifford_algebra}

Clifford Algebras represent the same structure for Non-degenerate Symmetric
Bilinear Forms (\S\ref{sec:symmetric_bilinear}) that Weyl Algebras
(\emph{Symplectic Clifford Algebras} \S\ref{sec:weyl_algebra}) represent for
Symplectic Bilinear Forms (\S\ref{sec:symplectic_form})

\fist a \emph{Geometric Algebra} (\S\ref{sec:geometric_algebra}) may be defined
as a Clifford Algebra of a Vector Space with a Quadratic Form
(\S\ref{sec:quadratic_form})

\url{https://golem.ph.utexas.edu/category/2014/07/the_tenfold_way.html} -- kinds
of matter corresponding to Real Clifford Algebras:
\begin{itemize}
\item $Cl_0$ (equivalent to $\reals$)
\item $Cl_1$
\item $Cl_2$
\item $Cl_3$
\item $Cl_4$ (equivalent to $\quats$)
\item $Cl_5$
\item $Cl_6$
\item $Cl_7$
\end{itemize}
and Complex Clifford Algebras:
\begin{itemize}
\item $\comps{l}_0$ (equivalent to $\comps$)
\item $\comps{l}_1$ (Superalgebra created by adding an Odd Square Root of $-1$
  to the purely Even Algebra $\comps$)
\end{itemize}

\url{https://gist.github.com/pervognsen/b204bf31af0809af078617af1609c8b8}

classification of Clifford Algebras:
\url{https://en.wikipedia.org/wiki/Classification_of_Clifford_algebras}



\paragraph{Dual Quaternion}\label{sec:dual_quaternion}\hfill

%FIXME: move section ???

Coefficients are Dual Numbers (\S\ref{sec:dual_number})

\fist Quaternions (\S\ref{sec:quaternion})

constructed like the Quaternions using the Dual Numbers
(\S\ref{sec:dual_number}) instead of the Reals

Rigid-body Displacements

Rigid Motions in 3D Space can be represented by Dual Quaternions of Unit Length

Parallel Manipulators



\paragraph{Dirac Algebra}\label{sec:dirac_algebra}\hfill

$\mathsf{C}\ell_4(\comps)$



% ==============================================================================
\section{Representation Theory}\label{sec:representation_theory}
% ==============================================================================

(wiki): study of Abstract Algebraic Structures (Part
\ref{part:abstract_algebra}) by \emph{Representing} their Elements as Linear
Transformations (\S\ref{sec:linear_transformation}) of Vector Spaces
(\S\ref{sec:vector_space}), and Modules over those Abstract Algebraic Structures

%FIXME: this section was moved from category theory

\fist \emph{Actions} (\S\ref{sec:action}) in Category Theory:

Given a Functor $R$ from a Group $G$ to a general Category $\cat{C}$:
\[
    R : G \rightarrow \cat{C}
\]
Such a Functor $R$ is termed a \emph{Representation} of $G$ in $\cat{C}$.

Frobenius Algebra (\S\ref{sec:frobenius_algebra})

Special Commutative Frobenius Monoid (\S\ref{sec:frobenius_monoid})

Representation Theory of Boolean Algebras (\S\ref{sec:boolean_algebra}) -- every
Boolean Algebra can be Represented as a Field of Sets (Set Algebra
\S\ref{sec:set_algebra})

\begin{itemize}
  \item Representation Theory of Groups: Group Elements are Represented by
    Invertible Matrices such that the Group Operation is Matrix Multiplication
\end{itemize}



% ------------------------------------------------------------------------------
\subsection{Representation}\label{sec:representation}
% ------------------------------------------------------------------------------

Elements of an Abstract Algebraic Structure are \emph{Represented} as Linear
Transformations of Vector Spaces:
\begin{itemize}
  \item Groups are Represented by Invertible Matrices
    (\S\ref{sec:group_representation})
  \item Associative Algebras are Represented by Modules
    (\S\ref{sec:algebra_representation})
  \item ...
\end{itemize}

Action (\S\ref{sec:action}) -- in general, \emph{Representation} is just another
word for Functor $F : \cat{C} \rightarrow \cat{D}$



\subsubsection{Group Representation}\label{sec:group_representation}

Representation of Groups by Invertible Matrices (\S\ref{sec:invertible_matrix})

the Differentiated form of Representation of a Lie Group is a Lie Algebra
Representation (\S\ref{sec:lie_algebra_representation})



\subsubsection{Algebra Representation}\label{sec:algebra_representation}

Associative Algebras (\S\ref{sec:associative_algebra}) Represented by Modules
(\S\ref{sec:module})

Rings may also be Represented by Modules



\subsubsection{Lie Algebra Representation}\label{sec:lie_algebra_representation}

Representation of Lie Algebras by Endomorphisms (Linear Maps) of a Vector Space

Differentiated form of Representation of a Lie Group

\fist a Universal Enveloping Algebra (\S\ref{sec:universal_enveloping_algebra})
is the most general Unital Associative Algebra containing all Representations
of a Lie Algebra



\subsubsection{Linear Representation}\label{sec:linear_representation}

$R : G \rightarrow \cat{Vect}_K$



\paragraph{Unitary Representation}\label{sec:unitary_representation}\hfill



% ------------------------------------------------------------------------------
\subsection{Intertwiner}\label{sec:intertwiner}
% ------------------------------------------------------------------------------

Homomorphism of Representations (\S\ref{sec:representation}), Actions
(\S\ref{sec:action}), Modules (\S\ref{sec:module})
