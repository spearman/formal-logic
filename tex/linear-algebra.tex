%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Linear Algebra}\label{part:linear_algebra}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Set Theory (Part \ref{part:set_theory}) is Linear Algebra over the
``Field with one Element''

cf. Vector Calculus (\S\ref{sec:vector_calculus})



% ====================================================================
\section{Module}\label{sec:module}
% ====================================================================

A \emph{Module} is a Unital Ring (\S\ref{sec:unital_ring}), $R$,
together with an Abelian Group (\S\ref{sec:abelian_group}), $(M, +)$,
and an Operation called \emph{Scalar Multiplication} which is either:
\[ R \times M \rightarrow M \]
for a \emph{Left $R$-module $M$}, $_R M$, or:
\[ M \times R \rightarrow M \]
for a \emph{Right $R$-module $M$}, $M_R$.

The Scalar Multiplication Operator is required that for all $r,s \in
R$ and $x,y \in M$ in a Left $R$-module $M$:
\begin{enumerate}
    \item $r(x + y) = rx + ry$
    \item $(r + s)x = rx + sx$
    \item $(rs)x = r(sx)$
    \item $1_Rx = x$
\end{enumerate}
or in a Right $R$-module $M$:
\begin{enumerate}
    \item $(x + y)r = xr + yr$
    \item $x(r + s) = xr + xs$
    \item $x(rs) = (sx)r$
    \item $x 1_R = x$
\end{enumerate}
where $1_R$ is the Multiplicative Identity for $R$. If the Ring is not
required to be Unital, then item (4) above can be ommitted, but can be
explicitly required by stating that we are talking about a
\emph{Unital Left/Right $R$-module $M$}.

\emph{Bimodule}

If $R$ is Commutative, then Left $R$-modules are the same as Right
$R$-modules and simply called \emph{$R$-modules}.

A Module Homomorphism is a \emph{Linear Map} (\S\ref{sec:linear_transformation})

Vector Space (\S\ref{sec:vector_space})

Multilinear Algebra (\S\ref{sec:multilinear_algebra}):
\emph{Multilinear Map} (\S\ref{sec:multilinear_map})



% --------------------------------------------------------------------
\subsection{Linear Transformation}\label{sec:linear_transformation}
% --------------------------------------------------------------------

A \emph{Linear Transformation} (or \emph{Linear Map}) is a Module Homomorphism
(\S\ref{sec:module}) $L : V \rightarrow W$ preserving Addition and Scalar
Multiplication:
\begin{align*}
  L(a\vec{v})              & = aL(\vec{v}) \\
  L(\vec{v}_1 + \vec{v}_2) & = L(\vec{v}_1 + \vec{v}_2)
\end{align*}

\fist cf. Linear Function (\S\ref{sec:linear_function}) -- a Polynomial
Function of Degree Zero or One

\fist cf. Locally Linear Transformations (\S\ref{sec:locally_linear})

Any Linear Map $A : V \rightarrow W$ from an $n$-dimensional Vector Space $V$
to an $m$-dimensional Vector Space $W$ can be represented by an $m \times n$
Matrix of Rank $r$ sending Column Vector $\vec{v} \in V$ to the Column Vector
$A\vec{v} \in W$. Each such Linear Transformation induces Four Fundamental
Subspaces (\S\ref{sec:fundamental_subspace}):
\begin{enumerate}
  \item $im(A) \subset W$ -- the \emph{Image} of $A$ is the \emph{Column Space}
    (\S\ref{sec:column_space}) of Dimension $r$
  \item $coim(A) \subset V$ -- the \emph{Coimage} of $A$ is the \emph{Row Space}
    (\S\ref{sec:row_space}) of Dimension $r$
  \item $ker(A) \subset V$ -- the \emph{Kernel} of $A$ is the \emph{Nullspace}
    (\S\ref{sec:nullspace}) of Dimension $n - r$ (Nullity) %FIXME clarify
  \item $coker(A) \subset W$ -- the \emph{Cokernel} of $A$ is the \emph{Left
    Nullspace} (\S\ref{sec:left_nullspace}) of Dimension $m - r$ (Corank)
    %FIXME clarify
\end{enumerate}
such that:
\begin{enumerate}
  \item $ker(A) = coim(A)^\bot$ -- the Nullspace is the Orthogonal Complement
    (\S\ref{sec:orthogonal_complement}) of the Row Space
  \item $coker(A) = im(A)^\bot$ -- the Left Nullspace is the Orthogonal
    Complement of the Column Space
\end{enumerate}

Linear Maps are Morphisms in the Category of Modules over a given Ring
(\S\ref{sec:ring}).

A \emph{Linear Operator} (\S\ref{sec:linear_operator}) is an Endomorphic Linear
Map (i.e. the Domain and Codomain are the same Module).

A Linear Map between Topological Vector Spaces
(\S\ref{sec:topological_vector}) can be Continuous
(\S\ref{sec:continuous}) and if the Domain and Codomain are the same
it is a \emph{Continuous Linear Operator} (\S\ref{sec:continuous_linear}).

A \emph{Linear Functional} (or \emph{Linear Form} \S\ref{sec:linear_form}) is a
Linear Map from a Vector Space $V$ to its Field of Scalars $K$ (viewed as a
Vector Space over itself).

Homothety (\S\ref{sec:homothety}): a specific kind of Linear Transformation of
an Affine Space called a \emph{Dilation} or \emph{Scale Transformation}; in
Projective Geometry (\S\ref{sec:projective_geometry}) it is a Similarity
Transformation (\S\ref{sec:simliarity_transformation}) that leaves the Line at
Infinity Pointwise Invariant

(Fong16):

%FIXME: are linear relations different from linear maps in general?

\emph{Linear Relation} $L : U \rightsquigarrow V$ is a Subspace $L
\subseteq U \oplus V$

Linear Relations as Corelations in $\cat{Vect}$; Category of Vector
Spaces and Linear Relations $\cat{LinRel}$



\subsubsection{Fundamental Subspace}\label{sec:fundamental_subspace}

\fist Fundamental Theorem of Linear Algebra
(\S\ref{sec:fundamental_linear_algebra_theorem})


\paragraph{Column Space}\label{sec:column_space}\hfill

\emph{Image}

Rank (\S\ref{sec:rank})



\paragraph{Row Space}\label{sec:row_space}\hfill

\emph{Coimage}

Rank (\S\ref{sec:rank})



\paragraph{Nullspace}\label{sec:nullspace}\hfill

The \emph{Nullspace} of a Linear Map $L : V \rightarrow W$ between Vector
Spaces $V$ and $W$ is the Set of all $\vec{v}\in{V}$ such that $L(\vec{v}) =
\vec{0}$ where $\vec{0}$ is the Zero Vector in $W$:
\[
  \text{ker}(L) = \{\vec{v}\in{V} \;|\; L(\vec{v}) = \vec{0}\}
\]

The Kernel of $L$ is a Linear Subspace of the Domain $V$.

Nullity (\S\ref{sec:nullity})



\paragraph{Left Nullspace}\label{sec:left_nullspace}\hfill

or \emph{Cokernel}

Corank (\S\ref{sec:rank})



\subsubsection{Linear Operator}\label{sec:linear_operator}

A \emph{Linear Operator} is an Endomorphic Linear Map, i.e. a Linear
Map from a Module to itself $V \rightarrow V$

there is a correspondence between $n\times{n}$ Square Matrices
(\S\ref{sec:square_matrix}) and Linear Operators on an $n$-dimensional Vector
Space



\paragraph{Linear Projection}\label{sec:projection}\hfill

A \emph{Linear Projection} $P$ is an Idempotent Linear Operator, i.e. $P^2 = P$.



\subparagraph{Orthogonal Projection}\label{sec:orthogonal_projection}\hfill

An \emph{Orthogonal Projection} on a Hilbert Space (\S\ref{sec:hilbert_space})
$W$ is a Projection for which the Range (Column Space \S\ref{sec:column_space})
$U$ and the Nullspace (\S\ref{sec:nullspace}) $V$ are \emph{Orthogonal
  Subspaces} (\S\ref{sec:orthogonal_subspace}), i.e. every Vector in $U$ is
Orthogonal (\S\ref{sec:orthogonality}) to every Vector in $V$:
\[
  Px \bot y - Py
\]

A Projection is Orthogonal if and only if it is Self-adjoint
(\S\ref{sec:self_adjoint_operator}).

an Orthogonal Projection is a Bounded Linear Operator
(\S\ref{sec:bounded_linear_operator})



\paragraph{Adjoint Operator}\label{sec:adjoint_operator}\hfill

The \emph{Adjoint Operator} (or \emph{Hermitian Adjoint}) of a
Linear Operator $A : H_1 \rightarrow H_2$ between Hilbert Spaces
(\S\ref{sec:hilbert_space}) is the Linear Operator $A^\dag : H_2 \rightarrow
H_1$ satisfying:
\[
  \langle{Ah_1,Ah_2}\rangle_{H_2} = \langle{h_1,A^{\dag}h_2}\rangle_{H_1}
\]
where $\langle\cdot,\cdot\rangle_{H_i}$ is the Inner Product Space in Hilbert
Space $H_i$.

%FIXME: not required to be an endomorphism (operator) ???



\subparagraph{Self-adjoint Operator}\label{sec:self_adjoint_operator}\hfill

A Linear Operator $A$ on a Hilbert Space (\S\ref{sec:hilbert_space}) is
\emph{Self-adjoint} if it is equal to its Hermitian Adjoint.

A Projection is Orthogonal (\S\ref{sec:orthogonal_projection}) if and only if
it is Self-adjoint.



\paragraph{Continuous Linear Operator}\label{sec:continuous_linear}\hfill

A \emph{Continuous Linear Operator} is a Continuous
(\S\ref{sec:continuous_map}) Linear Operator between Topological
Vector Spaces (\S\ref{sec:topological_vector})

a C$^*$-algebra (\S\ref{sec:cstar_algebra}) is a Complex Algebra $A$
of Continuous Linear Operators on a Complex Hilbert Space
(\S\ref{sec:hilbert_space}) with the additional Properties that $A$ is
Topologically Closed in the Norm Topology of Operators and Closed
under the Operation of taking Adjoints of Operators

(wiki):

a Linear Operator on a Normed Vector Space
(\S\ref{sec:normed_vectorspace}) is Continuous if and only if it is a
Bounded Linear Operator (\S\ref{sec:bounded_linear_operator}), e.g.
when the Domain is Finite-dimensional: every Linear Operator on a
Finite-dimensional Space is Continuous



\subsubsection{Linear Form}\label{sec:linear_form}

A \emph{Linear Form} (or \emph{Linear Functional} or \emph{Covector}) is a
Linear Map from a Vector Space to its Field of Scalars viewed as a Vector Space
over itself.

For a Vector Space $V$ over a Field $k$, a Linear Functional $f$ is a Function
from $V$ to $k$ that is \emph{Linear}:
\begin{itemize}
\item $f(\vec{v} + \vec{w}) = f(\vec{v}) + f(\vec{w})$ for all $\vec{v},
  \vec{w} \in V$
\item $f(a\vec{v} = af(\vec{v}$ for all $\vec{v} \in V, a \in k$
\end{itemize}

The action of a Linear Functional on a Vector is given by the Dot Product
(\S\ref{sec:inner_product}), or the Matrix Product with a Row Vector on the
Left and a Column Vector on the right.

The \emph{(Algebraic) Dual Space} (\S\ref{sec:dual_space}) of a Vector Space
$V$ is the Set of all Linear Forms on $V$ together with the Vector Space
structure of Pointwise Addition and Multiplication by Constants.

A \emph{Bilinear Form} (\S\ref{sec:bilinear_form}) is a Bilinear Map $V \times
V \rightarrow K$ (i.e. Linear in each argument separately).

Distribution (\S\ref{sec:distribution})



\subsubsection{Semilinear Map}\label{sec:semilinear_map}

(or \emph{Semilinear Transformation})

generalizes the class of Antilinear Maps (\S\ref{sec:antilinear_map})



\paragraph{Antilinear Map}\label{sec:antilinear_map}\hfill

or \emph{Conjugate-linear Map}

is a Mapping $f : V \rightarrow W$ between Complex Vector Spaces $V$ and $W$
when:
\[
  f (ax+by) = \bar{a}f(x) + \bar{b}f(y)
\]
for all Complex Numbers $a,b \in \comps$ with Complex Conjugates $\bar{a},
\bar{b}$ and all $x,y \in V$

may be equivalently described in terms of the Linear Map $\bar{f} : V
\rightarrow \bar{W}$ from $V$ to the Complex Conjugate Vector Space $\bar{W}$

A Sesquilinear Form (\S\ref{sec:sesquilinear_form}) is a Bilinear Form where
the Codomain $K$ is the Field of Complex Numbers and the mapping is Antilinear
in one argument.


Quantum mechanics: Time Reversal

Spinor Calculus

\fist Antiunitary Operator (\S\ref{sec:antiunitary_operator})



\subsubsection{Short Linear Map}\label{sec:short_linear}

$\cat{Hilb}$



\subsubsection{Eigenvector}\label{sec:eigenvector}

An \emph{Eigenvector} or \emph{Characteristic Vector} of a Linear
Transformation is a Non-zero Vector whose Direction does not change when the
Linear Transformation is applied to it:
\[
  A\vec{v} = \lambda\vec{v}
\]
where $\lambda$ is the \emph{Eigenvalue} corresponding to $\vec{v}$.

An \emph{Eigendecomposition} (\S\ref{sec:eigendecomposition}) of a Matrix is a
Factorization of the Matrix in terms of its Eigenvalues and Eigenvectors and
only Diagonlizable Matrices (\S\ref{sec:diagonalizable_matrix}) can be Factored
in this way.



\paragraph{Eigenvalue}\label{sec:eigenvalue}\hfill

The \emph{Eigenvalue} (or \emph{Characteristic Value} or \emph{Characteristic
  Root}) of an Eigenvector $\vec{v}$ with respect to a Linear Operator $T$, is a
Scalar $\lambda$ such that:
\[
  T(\vec{v}) = \lambda\vec{v}
\]



% --------------------------------------------------------------------
\subsection{Locally Linear Transformation}\label{sec:locally_linear}
% --------------------------------------------------------------------

%FIXME: move this section ?

(FIXME: what is the criterea for a transformation betwen vector spaces to be
``locally linear'')

\fist cf. Local Linearization (\S\ref{sec:local_linearization})



\subsubsection{Jacobian}\label{sec:jacobian}

%FIXME: merge with jacobian matrix ?

Jacobian Matrix (\S\ref{sec:jacobian_matrix})

\fist the Jacobian Matrix for a System of DAEs (\S\ref{sec:system_of_daes}) is
a Singular (Non-invertible) Matrix (\S\ref{sec:singular_matrix})



% --------------------------------------------------------------------
\subsection{Additive Map}\label{sec:additive_map}
% --------------------------------------------------------------------

$f(x + y) = f(x) + f(y)$

Special case of Subadditive Function (\S\ref{sec:subadditive_function})

Norm (\S\ref{sec:norm})



% --------------------------------------------------------------------
\subsection{Free Module}\label{sec:free_module}
% --------------------------------------------------------------------

A \emph{Free Module} is a Freely Generated (\S\ref{sec:free_object})
Module over a given Basis (\S\ref{sec:basis}).

Free Vector Space



\subsubsection{Group Ring}\label{sec:group_ring}



% --------------------------------------------------------------------
\subsection{Bimodule}\label{sec:bimodule}
% --------------------------------------------------------------------

Abelian Group (\S\ref{sec:abelian_group}) that is both a Left and a
Right Module such that the Left and Right Multiplications are
compatible.

Rings $R$, $S$, $R$-$S$-bimodule is an Abelian Group $M$ such that:

\begin{enumerate}
\item $M$ is a Left $R$-module and a Right $S$-module
\item $\forall r \in R, s \in S, m \in M, (rm)s = r(ms)$
\end{enumerate}

An $R$-$R$-bimodule is known as an $R$-bimodule

Generalization of Algebra Homomorphism
(\S\ref{sec:algebra_homomorphism})

Categorical generalization: Profunctor (\S\ref{sec:profunctor})



% ====================================================================
\section{Vector Space}\label{sec:vector_space}
% ====================================================================

\emph{Vector} (\S\ref{sec:vector}); a \emph{Position Vector} is a Vector based
at the Origin; a \emph{Free Vector} is equivalent to a Position Vector of the
same Direction and Magnitude

\emph{Span}

\emph{Finite-dimensional Vector Space} -- has a Span; all
Finite-dimensional Vector Spaces are Nuclear Spaces
(\S\ref{sec:nuclear_space})

\emph{Infinite Dimensional Vector Space} -- does not have a Span

\emph{Linear Independence}

\emph{Basis} (\S\ref{sec:basis}) - Spans and is Linearly Independent

Field (\S\ref{sec:field})

every Vector Space is a \emph{Free Vector Space}: every Vector Space
looks like a Function $X \rightarrow R$ where $R$ is a Field and $X$
some Set of Basis Vectors, i.e. Vectors can be decomposed into a sum
of Scalars times the choice of Basis Vectors %FIXME

All Bases of a Vector Space $\mathbf{V}$ have the same number of
Elements equal to the \emph{Dimension} of $\mathbf{V}$,
$dim(\mathbf{V})$. The Dimension of a Vector Space is uniquely defined
because for any Vector Space, a Basis exists, and all Bases of a
Vector space have equal Cardinality (\S\ref{sec:cardinality}).

For a Finite Dimensional Vector Space a Subset of a Span defines a
Basis, and a Linearly Independent Subset can be extended to form a
Basis.

The number of Elements in a Spanning Subset of $\mathbf{V}$ is greater
than or equal to the Dimension of $\mathbf{V}$.

The number of Elements in a Linearly Independent Subset of
$\mathbf{V}$ is less than or equal to the Dimension of $\mathbf{V}$.

A Basis defines an Isomorphism of Vector Spaces:
\[
    \mathbf{V} \xrightarrow{f} F^n
\]

\emph{Tensor Product}, \emph{Outer Product} (\S\ref{sec:outer_product})

Scalar (0th-order Tensor \S\ref{sec:scalar}) is an Element of the
Field used to define a Vector Space

Vector (1st-order Tensor \S\ref{sec:vector}) is an Element of a Vector
Space

Functional Analysis: Sequence Space (\S\ref{sec:sequence_space})

a Euclidean Vector Space with the Group Operation of Vector Addition is an
example of a Non-compact (\S\ref{sec:compact_space}) Lie Group
(\S\ref{sec:lie_group})



% --------------------------------------------------------------------
\subsection{Linear Combination}\label{sec:linear_combination}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Fundamental Theorem of Linear Algebra}
\label{sec:fundamental_linear_algebra_theorem}
% --------------------------------------------------------------------

1993 - Strang - \emph{The Fundamental Theorem of Linear Algebra}

An $m \times n$ Matrix $A$ of Rank (\S\ref{sec:rank}) $r$ with Singular
Value Decomposition (\S\ref{sec:svd}):
\[
  A = U \Sigma V^*
\]
induces \emph{Four Fundamental Subspaces} (\S\ref{sec:fundamental_subspace}):
\begin{itemize}
  \item \emph{Column Space} (\S\ref{sec:column_space}) -- the \emph{Image},
    $im(A)$, of Dimension $r$ (Rank \S\ref{sec:rank})
  \item \emph{Row Space} (\S\ref{sec:row_space}) -- the \emph{Coimage},
    $coim(A)$, of Dimension $r$ (Rank)
  \item \emph{Nullspace} (\S\ref{sec:nullspace}) -- the \emph{Kernel},
    $ker(A)$, of Dimension $n - r$ (Nullity \S\ref{sec:nullity})
  \item \emph{Left Nullspace} (\S\ref{sec:left_nullspace}) -- the
    \emph{Cokernel}, $coker(A)$, of Dimension $m - r$ (Corank
    \S\ref{sec:corank})
\end{itemize}
such that:
\begin{enumerate}
  \item $ker(A) = coim(A)^\bot$ -- the Nullspace is the Orthogonal Complement
    (\S\ref{sec:orthogonal_complement}) of the Row Space
  \item $coker(A) = im(A)^\bot$ -- the Left Nullspace is the Orthogonal
    Complement of the Column Space
\end{enumerate}



% --------------------------------------------------------------------
\subsection{Linear Span}\label{sec:linear_span}
% --------------------------------------------------------------------

The \emph{Linear Span} (or \emph{Linear Hull} or \emph{Span}) of a Set of
Vectors in a Vector Space is the Intersection of all Linear Subspaces
(\S\ref{sec:linear_subspace}) containing that Set.

A \emph{Basis} (\S\ref{sec:basis}) of a Vector Space is a Linearly Independent
Spanning Set.

The \emph{Rank} (\S\ref{sec:rank}) of a Matrix is the Dimension of the Vector
Space Spanned by its Columns, i.e. the maximal number of Linearly Independent
Columns of the Matrix.

generalized to Matroids (\S\ref{sec:matroid}) and Modules (\S\ref{sec:module})
%FIXME explain

Closed Linear Span (\S\ref{sec:closed_linear_span})



\subsubsection{Basis}\label{sec:basis}

A Set of Vectors in a Vector Space is a \emph{Basis} if the Vectors are
\emph{Linearly Independent} and therefore every Vector in the Vector Space is a
Linear Combination of the Set, that is, a \emph{Linearly Independent Spanning
  Set} (\S\ref{sec:linear_span}).

\fist a \emph{Free Object} (\S\ref{sec:free_object}) is a direct generalization
to Categories of the notion of Basis in a Vector Space
(FIXME: clarify)



\paragraph{Covariant Transformation}\label{sec:covariant_transformation}\hfill

Coordinate-free (\S\ref{sec:coordinate_free})

Covariant Transformation Law

change of Basis

precise Transformation Law determines the Valence
(\S\ref{sec:valence}) of a Tensor (\S\ref{sec:linear_tensor})



% --------------------------------------------------------------------
\subsection{Linear Subspace}\label{sec:linear_subspace}
% --------------------------------------------------------------------

A \emph{Linear Subspace} (or \emph{Vector Subspace}) is a Subset of a Vector
Space that is Closed under Addition and Scalar Multiplication. For Field $K$
and $V$ a Vector Space over $K$, a Subset $W \subset K$ is a \emph{Subspace} of
$V$ if:
\begin{enumerate}
  \item $\vec{0} \in W$ -- the Zero Vector is in $W$
  \item $\vec{u}, \vec{v} \in W \Rightarrow \vec{u} + \vec{v} \in W$
    -- $W$ is Closed under Vector Addition
  \item $c\vec{u} \in W \forall c \in K \vec{u} \in W$
    -- $W$ is Closed under Scalar Multiplication
\end{enumerate}

\fist Fundamental Subspaces (\S\ref{sec:fundamental_subspace})



\subsubsection{Orthogonal Subspace}\label{sec:orthogonal_subspace}

Two Linear Subspaces $A, B \subset V$ of an Inner Product Space $V$ are called
\emph{Orthogonal Subspaces} if each Vector in $A$ is Orthogonal to each Vector
in $B$.

The largest Subspace of $V$ that is Orthogonal to a given Subspace is its
Orthogonal Complement (\S\ref{sec:orthogonal_complement}).

An Orthogonal Projection (\S\ref{sec:orthogonal_projection}) on a Hilbert Space
(\S\ref{sec:hilbert_space}) $W$ is a Projection for which the Range (Column
Space \S\ref{sec:column_space}) $U$ and the Nullspace (\S\ref{sec:nullspace})
$V$ are Orthogonal Subspaces.



\paragraph{Orthogonal Complement}\label{sec:orthogonal_complement}\hfill

The \emph{Orthogonal Complement} of a Linear Subspace $W \subset V$ with a
Bilinear Form (\S\ref{sec:bilinear_form}) $B$ is the Set $W^\bot$ of all
Vectors in $V$ that are Orthogonal to every Vector in $W$ and is itself a
Linear Subspace of $V$. The Orthogonal Complement of $W$ is the largest
Subspace of $V$ that is Orthogonal to $W$.



% --------------------------------------------------------------------
\subsection{Scalar Field}\label{sec:scalar_field}
% --------------------------------------------------------------------

%FIXME: move scalar, vector, tensor fields to vector calculus?

Gradient (\S\ref{sec:gradient}): the Vector Derivative of a Scalar Field

cf. Vector Field (\S\ref{sec:vector_field})

cf. Tensor Field (\S\ref{sec:tensor_field})

Line Integral of Parametric Curves (\S\ref{sec:parametric_curve}) in a Scalar
Field; unlike Line Integrals of Curves in Vector Fields, the Line Integral in a
Scalar Field is \emph{independent} of Path direction

when a Vector Field is the \emph{Gradient Field} (\S\ref{sec:gradient}) of some
Scalar Field, it is a \emph{Conservative} (\emph{Path Independent}
\S\ref{sec:conservative_vector_field}) Vector Field and the Line Integral of
any Path between two Points is equivalent and the Line Integral of any Closed
Path from a Point to itself is Zero



\subsubsection{Scalar Function}\label{sec:scalar_function}



% --------------------------------------------------------------------
\subsection{Vector Field}\label{sec:vector_field}
% --------------------------------------------------------------------

FIXME: does a Vector Field require that the Domain and Range have the same
Dimension ???

\begin{equation*}
  \vec{v}(x,y,z) = \begin{bmatrix}
    p(x,y,z) \\
    q(x,y,z) \\
    r(x,y,z)
  \end{bmatrix}
\end{equation*}

Partial Derivative (\S\ref{sec:partial_derivative}) ... TODO

Vector Field Line Integral (\S\ref{sec:line_integral}) of a Parametric Curve
(\S\ref{sec:parametric_curve}) is a Sum of the Dot Products of the Derivative
of the Path Function $C = \vec{r}(t)$ with the Vectors in the Vector Field
$\vec{f}(x,y)$:
\[
  \int_C \vec{f} \bullet d\vec{r}
\]
represents the amount of ``Work'' done on a Path in the Field (Physics)
that unlike Line Integrals of Curves in a Scalar Field
(\S\ref{sec:scalar_field}), the Line Integral in a Vector Field is
\emph{dependent} on Path direction (it is Negative in the opposite direction)

a Vector Field is called \emph{Path Independent} (or \emph{Conservative}
\S\ref{sec:conservative_vector_field}) if it is equal to the Gradient Field
(\S\ref{sec:gradient}) of a Scalar Field; if so the Line Integral of all Paths
between Points are equal and the Line Integral from a Point to itself (Closed
Path) is Zero

wiki:

Divergence (\S\ref{sec:divergence}) of a Vector Field: Scalar-valued Function
(\S\ref{sec:scalar_function}) associating a Scalar Value with each Point in
the Vector Field

Curl (\S\ref{sec:curl}) of a Vector Field: Vector Function
(\S\ref{sec:vector_function}) associating each Point in the Vector field with
the proportional ``on-axis'' Torque to which a ``tiny pinwheel'' would be
subjected if it were centered at the Point

a Vector Field attaches to every Point of a Manifold (\S\ref{sec:manifold}) a
Vector from the Tangent Space (\S\ref{sec:tangent_space}) at that Point in a
\emph{Smooth} (\S\ref{sec:smooth_function}) manner and such a Vector Field
defines a generalized Ordinary Differential Equation (ODE \S\ref{sec:ode}) on a
Manifold where a Solution to such an Equation is a Diefferentiable Curve on the
Manifold with Derivative at any Point equal to the Tangent Vector attached to
that Point by the Vector Field

cf. Scalar Field (\S\ref{sec:scalar_field})

cf. Tensor Field (\S\ref{sec:tensor_field})

Autonomous Vector Field (???)



\subsubsection{Vector Flow}\label{sec:vector_flow}

Flow (\S\ref{sec:flow})



\subsubsection{Conservative Vector Field}\label{sec:conservative_vector_field}

a Vector Field is called \emph{Conservative} (or \emph{Path Independent}) if
the Line Integral (\S\ref{sec:line_integral}) of all Paths between Points are
equal and the Line Integral of a Closed Path (from a Point to itself) is Zero

this is true when the Vector Field is the \emph{Gradient Field}
(\S\ref{sec:gradient}) of some Scalar Field



% --------------------------------------------------------------------
\subsection{Normed Vector Space}\label{sec:normed_vectorspace}
% --------------------------------------------------------------------

A \emph{Normed Vector Space} is a Vector Space $V$ over the Real or Complex
Numbers on which a \emph{Norm} (\S\ref{sec:norm}) $\|\cdot\| : V \rightarrow
\reals^{0\leq}$ which is a Real-valued Function defined on the Vector Space
formalizing the notion of \emph{Distance} between Vectors.

Any Normed Vector Space is a Metric Space (\S\ref{sec:metric_space}) by
defining $d(x,y) = \|y-x\|$.

A Complete (\S\ref{sec:complete_metric_space}) Normed Vector Space is a Banach
Space (\S\ref{sec:banach_space}).

Any Inner Product Space (\S\ref{sec:innerproduct_space}) with Inner Product
$\langle{\cdot,\cdot}\rangle$ induces a Norm by:
\[
  \|\vec{v}\| = \sqrt{\langle{\vec{v},\vec{v}}\rangle}
\]
A Complete Inner Product Space is a Hilbert Space (\S\ref{sec:hilbert_space}).

Bounded Linear Operator (\S\ref{sec:bounded_linear_operator})

Functional Analysis (\S\ref{sec:functional_analysis})



\subsubsection{Norm}\label{sec:norm}

The \emph{Norm} $\|\cdot\| : V \rightarrow \reals$ of a Normed Vector Space is
a Real-valued Function defined on the Vector Space with the properties:
\begin{enumerate}
  \item $0 \leq \|\vec{v}\|$ and
    $\|\vec{v}\| = 0 \Leftrightarrow \vec{v} = \vec{0}$
  \item $\|c\vec{v}\| = |c|\|\vec{v}\|$
  \item $\|\vec{u} + \vec{v}\| \leq \|\vec{u}\| + \|\vec{v}\|$ (\emph{Triangle
    Inequality})
\end{enumerate}

\emph{Seminorm}

\emph{Quasinorm}

Subadditive Function (\S\ref{sec:subadditive_function})



\paragraph{Triangle Inequality}\label{sec:triangle_inequality}\hfill

Real Line (\S\ref{sec:real_line}) with Absolute Value as Norm:\\
$|x + y| \leq |x| + |y|$



\subsubsection{Inner Product Space}\label{sec:innerproduct_space}

An \emph{Inner Product Space} $V$ is a Normed Vector Space with Norm
(\S\ref{sec:norm}) $\|\cdot\|$ induced by the \emph{Inner Product}
(\S\ref{sec:inner_product}) $\langle{\cdot,cdot}\rangle : V \times V
\rightarrow \reals$ which is a Sesquilinear Form
(\S\ref{sec:sesquilinear_form}).

An Inner Product on a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}).

\fist Orthogonality (\S\ref{sec:orthogonality})

generalizes case of Euclidean Space (\S\ref{sec:euclidean_space}) to
Vector Spaces of any, possibly Infinite, Dimension

Functional Analysis (\S\ref{sec:functional_analysis})



\paragraph{Inner Product}\label{sec:inner_product}\hfill

associates each pair of Vectors in a Vector Space with a Scalar known
as the \emph{Inner Product} of the Vectors

in Euclidean Spaces the Inner Product is the Dot Product (aka Scalar Product)

An Inner Product induces a Norm (\S\ref{sec:norm}) $\|\cdot\|$ by:
\[
  \|\vec{v}\| = \sqrt\langle{\vec{v},\vec{v}}\rangle
\]

an Inner Product on a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}) and a Positive-definite Hermitian Form
(\S\ref{sec:hermitian_form})

A \emph{Hilbert Space} (\S\ref{sec:hilbert_space}) is an Inner Product Space
that is also a Complete Metric Space (\S\ref{sec:complete_metric_space}).

\fist Sliding Inner-product (Cross-correlation \S\ref{sec:cross_correlation})



\paragraph{Unitary Space}\label{sec:unitary_space}\hfill

Inner Product Space over the Field of Complex Numbers



\paragraph{Hilbert Space}\label{sec:hilbert_space}\hfill

%FIXME: move to analysis ?

A \emph{Hilbert Space} is an Inner Product Space (\S\ref{sec:inner_product})
that is also a Complete Metric Space (\S\ref{sec:complete_metric_space}).

Functional Analysis (\S\ref{sec:functional_analysis}): can be defined
as a Banach Space (\S\ref{sec:banach_space})

Topological Vector Space (\S\ref{sec:topological_vector})

Hermitian Adjoint (\S\ref{sec:hermitian_adjoint})

$\cat{Hilb}$

$\cat{FdHilb}$

An Orthogonal Projection (\S\ref{sec:orthogonal_projection}) on a Hilbert Space
$W$ is a Projection for which the Range (Column Space \S\ref{sec:column_space})
$U$ and the Nullspace (\S\ref{sec:nullspace}) $V$ are Orthogonal Subspaces
(\S\ref{sec:orthogonal_subspace}).



\subparagraph{Hilbert Tensor Product}\label{sec:hilbert_tensor}\hfill

Topological Tensor Product (\S\ref{sec:topological_tensor})



\subparagraph{Antiunitary Operator}\label{sec:antiunitary_operator}\hfill

a Bijective Antilinear Map (\S\ref{sec:antlinear_map}) $U$ between Complex
Hilbert Spaces $H_1$ and $H_2$:
\[
  U : H_1 \rightarrow H_2
\]
such that:
\[
  \langle{Ux,Uy}\rangle = \overline{\langle{x,y}\rangle}
\]

Quantum Theory: Time-reversal Symmetry (Wigner's Theorem, Ray Space) %FIXME



\textbf{Hilbert-Schmidt Mapping}

\emph{Weakly Hilbert-Schmidt Mapping}:
\[
  L : H_1 \otimes H_2 \rightarrow H
\]
is a Bilinear Map for which a $d$ exists such that
$\sum_{i,j=1}^\infty | \langle L(e_i,f_j), u \rangle |^2 \leq d^2
||u||^2$ for all $u \in K$ and one (implies all) Orthonormal Basis
$e_1, e_2, \ldots$ of $H_1$ and $f_1, f_2, \ldots$ of $H_2$

% FIXME

1970 - Blute, Panangaden:

the Ideal of Hilbert-Schmidt Maps contained in the Category of Hilbert
Spaces is an example of a \emph{Nuclear Ideal}: an Ideal contained in
an ambient Monoidal Dagger Category with the structure of a Compact
Closed Category except it is lacking Identities



% --------------------------------------------------------------------
\subsection{Scalar Product Space}\label{sec:scalar_product_space}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Hermitian Product Space}\label{sec:hermitian_product_space}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Outer Product}\label{sec:outer_product}
% --------------------------------------------------------------------

Tensor Product (\S\ref{sec:tensor_product}) of two Vectors



% --------------------------------------------------------------------
\subsection{Real Vector Space}\label{sec:real_vector_space}
% --------------------------------------------------------------------

Vector Space over Scalar Field $F = \reals$

\fist \emph{Real Coordinate Space} (\S\ref{sec:real_coordinate_space})
$\reals^n$ -- prototypical Real Vector Space; Models Euclidean Space
(\S\ref{sec:euclidean_space}) with Cartesian Coordinates
(\S\ref{sec:cartesian_coordinates})



% --------------------------------------------------------------------
\subsection{Complex Vector Space}\label{sec:complex_vector_space}
% --------------------------------------------------------------------

Vector Space over Scalar Field $F = \comps$



% --------------------------------------------------------------------
\subsection{Symplectic Vector Space}\label{sec:symplectic_vectorspace}
% --------------------------------------------------------------------

Symplectic Geometry (\S\ref{sec:symplectic_geometry})

``Black-box Functor'': Category of Circuits $\rightarrow$ Category of
Symplectic Vector Spaces % FIXME Baez 15' Passive Linear Networks



% --------------------------------------------------------------------
\subsection{Finite-dimensional Vector Space}
\label{sec:finite_dimensional_vectorspace}
% --------------------------------------------------------------------

Finite-dimensional Vector spaces of Equal Dimension are Isomorphic
%FIXME



% --------------------------------------------------------------------
\subsection{Graded Vector Space}\label{sec:graded_vectorspace}
% --------------------------------------------------------------------

Graded Algebra (\S\ref{sec:graded_algebra})



% --------------------------------------------------------------------
\subsection{Dual Space}\label{sec:dual_space}
% --------------------------------------------------------------------

the \emph{Dual Space} (or \emph{Algebraic Dual Space}) of any Vector Space $V$
is the set of all Linear Functionals (Linear Forms \S\ref{sec:linear_form}) of
$V$ together with the Vector Space structure of Pointwise Addition and
Multiplication by Scalars

for a Topological Vector Space (\S\ref{sec:topological_vector_space}), there is
a Subspace of the Dual Space corresponding to Continuous Linear Functionals
called the \emph{Continuous Dual Space} (\S\ref{sec:continuous_dual_space})

$V \cong V^*$

$V \cong V^**$ ``Naturally'' (\S\ref{sec:natural_transformation})

Contravariant Representable Functor
(\S\ref{sec:representable_functor}):
\[
  (-)^* = Vect(-,\mathbb{R}) :
    \mathbf{Vect}^op \rightarrow \mathbf{Vect}
\]

\[
  A^* = \pow(A) \cong \mathbf{Set}(A,2)
\]\cite{awodey06}

(FIXME: does this actually relate to linear logic ???):

given a Vector Space $V$ over a Field $R$, the Dual Space $V^*$ is just the Set
of Linear Functions into $V \mapsto R$
--\url{https://cstheory.stackexchange.com/questions/39440/algebraic-account-of-gaussian-elimination}

the Dual Bundle (\S\ref{sec:dual_bundle}) of a Vector Bundle
(\S\ref{sec:vector_bundle}) is a Vector Bundle whose Fibers are the Dual Spaces
of the Fibers of the original Vector Bundle



% --------------------------------------------------------------------
\subsection{Vector Bundle}\label{sec:vector_bundle}
% --------------------------------------------------------------------

FIXME: move to topology ?

a Topological construction making precise the idea of a Family of Vector Space
parameterized by another Space $X$ (e.g. a Topological Space, Manifold, or
Algebraic Veriety) associating for every point $x \in X$ a Vector Space $V(x)$
such that all the Vector Spaces ``fit together'' to form another Space of the
same kind as $X$, called the \emph{Vector Bundle over $X$}

an example of a \emph{Fiber Bundle} (\S\ref{sec:fiber_bundle})

the Tangent Bundle (\S\ref{sec:tangent_bundle}) of Tangent Spaces
(\S\ref{sec:tangent_space}) of a Differentiable Manifold is the prototypical
Vector Bundle

the Dual Bundle (\S\ref{sec:dual_bundle}) of the Tangent Bundle is the
a Cotangent Bundle (\S\ref{sec:cotangent_bundle}) of
Cotangent Spaces (\S\ref{sec:cotangent_space})



\subsubsection{Sub-bundle}\label{sec:subbundle}

\fist a Subbundle of the Tangent Bundle (\S\ref{sec:tangent_bundle}) of a
Smooth manifold is called a \emph{Distribution}
(\S\ref{sec:tangent_bundle_distribution}) of Tangent Vectors
(\S\ref{sec:tangent_space})

\emph{Frobenius' Theorem} (\S\ref{sec:frobenius_theorem}): the Subbundle of the
Tangent Bundle of a Manifold is Integrable (or Involutive) if and only if it
arises from a \emph{Regular Foliation} (\S\ref{sec:foliation})



\subsubsection{Dual Bundle}\label{sec:dual_bundle}

the \emph{Dual Bundle} of a Vector Bundle is a Vector Bundle whose Fibers are
Dual Spaces (\S\ref{sec:dual_space}) of the Fibers of the Vector Bundle

the Dual Bundle of the Tangent Bundle (\S\ref{sec:tangent_bundle}) of a the
Tangent Spaces of Differentiable Manifold is the a Cotangent Bundle
(\S\ref{sec:cotangent_bundle}) of the Cotangent Spaces of the Manifold



\subsubsection{Complex Vector Bundle}\label{sec:complex_vector_bundle}



% ====================================================================
\section{$R$-algebra}\label{sec:r_algebra}
% ====================================================================

Algebra over a Commutative Unital Ring

Module (\S\ref{sec:module}) with a \emph{Bilinear Product}
(\S\ref{sec:bilinear_product})



% --------------------------------------------------------------------
\subsection{Bilinear Product}\label{sec:bilinear_product}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Algebra Homomorphism}\label{sec:algebra_homomorphism}
% --------------------------------------------------------------------

Generalized by Bimodules (\S\ref{sec:bimodule})



% --------------------------------------------------------------------
\subsection{Non-associative $R$-algebra}
\label{sec:nonassociative_r_algebra}
% --------------------------------------------------------------------

For Commutative Unital Ring $R$, $R$-module $V$ with Bilinear Product
(\S\ref{sec:bilinear_product}) $V \otimes V \rightarrow V$



% --------------------------------------------------------------------
\subsection{Associative $R$-algebra}\label{sec:associative_r_algebra}
% --------------------------------------------------------------------

\emph{Associative Unital $R$-algebra}, $R$-module $V$ with Bilinear
Product (\S\ref{sec:bilinear_product}) $p : V \otimes V \rightarrow V$
Linear Map $i : R \rightarrow V$ satisfying Associative and Unit Laws



% ====================================================================
\section{$K$-algebra}\label{sec:k_algebra}
% ====================================================================

\emph{Algebra over a Field}

Vector Space (\S\ref{sec:vector_space}) with a \emph{Bilinear Product}
(\S\ref{sec:bilinear_product})

Coalgebra (\S\ref{sec:coalgebra})

$F$-algebra (\S\ref{sec:f_algebra})

Every Finite Dimensional Associative Division Algebra over $\reals$ is
Isomorphic to either $\reals$, $\comps$, or $\quats$ (Theorem of
Frobenius) %FIXME



% --------------------------------------------------------------------
\subsection{Unital Algebra}\label{sec:unital_algebra}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Zero Algebra}\label{sec:zero_algebra}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Associative Algebra}\label{sec:associative_algebra}
% --------------------------------------------------------------------

Coalgebra (\S\ref{sec:coalgebra}): Dual to a Unital Associative
Algebra



\subsubsection{Frobenius Algebra}\label{sec:frobenius_algebra}

Representation Theory (\S\ref{sec:representation_theory})

Module Theory (\S\ref{sec:module})

Frobenius Algebras can be defined in any Monoidal Category (or
Polycategory \S\ref{sec:polycategory}) and are sometimes called
\emph{Frobenius Monoids} (\S\ref{sec:frobenius_monoid})

Star-autonomous Categories (\S\ref{sec:star_autonomous}) are Pseudo-Frobenius
Algebras
--\url{https://golem.ph.utexas.edu/category/2017/11/starautonomous_categories_are.html}



% --------------------------------------------------------------------
\subsection{Non-associative Algebra}
\label{sec:nonassociative_algebra}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Coalgebra}\label{sec:coalgebra}
% --------------------------------------------------------------------

Dual to Unital Associative Algebra (\S\ref{sec:associative_algebra})

Cf. $F$-coalgebra (\S\ref{sec:f_coalgebra})



% --------------------------------------------------------------------
\subsection{Bialgebra}\label{sec:bialgebra}
% --------------------------------------------------------------------

Unital Associative Algebra and Coalgebra Vector Space over Field $K$



\subsubsection{Quasi-bialgebra}\label{sec:quasi_bialgebra}

\subsubsection{Hopf Algebra}\label{sec:hopf_algebra}

simultaneously a Unital Associative Algebra and a Counital
Coassociative Coalgebra with compatibility of these Structures making
it a Bialgebra, also equipped with an Anti-automorphism satisfying a
certain Property %FIXME what property?

Algebraic Topology



% ====================================================================
\section{Graded Algebra}\label{sec:graded_algebra}
% ====================================================================

Graded Vector Space (\S\ref{sec:graded_vectorspace})



% ====================================================================
\section{Exterior Algebra}\label{sec:exterior_algebra}
% ====================================================================

%FIXME multilinear algebra?

Grassmann Algebra (\S\ref{sec:grassmann_algebra})



% --------------------------------------------------------------------
\subsection{Bivector}\label{sec:bivector}
% --------------------------------------------------------------------

$2$-vector

Isomorphic to Skew-symmetric Matrices (\S\ref{sec:skew_symmetric})



% --------------------------------------------------------------------
\subsection{Exterior Product}\label{sec:exterior_product}
% --------------------------------------------------------------------

(\emph{Wedge Product})



% ====================================================================
\section{Linear Equation}\label{sec:linear_equation}
% ====================================================================

% ====================================================================
\section{Matrix Theory}\label{sec:matrix_theory}
% ====================================================================

%FIXME: merge with matrix calculus ?

A \emph{Matrix} represents a \emph{Linear Map} (Linear
Transformation\S\ref{sec:linear_transformation}) between
two Vector Spaces (\S\ref{sec:vector_space}). Any Linear Map between
Finite-dimensional Vector Spaces $A : V \rightarrow W$ where $V$ has Dimension
$n$ and $W$ has Dimension $m$ can be represented as an $m \times n$ Matrix
sending the Column Vector $\vec{v} \in V$ to the Column Vector $A\vec{v} \in
W$.

\fist Matrix (Second-order Tensor \S\ref{sec:matrix})

\fist Matrix Calculus (\S\ref{sec:matrix_calculus})

\fist Matrix Difference Equation (\S\ref{sec:matrix_difference_equation})

\fist System of Linear Equations (\S\ref{sec:linear_system})

\fist Fundamental Subspaces (\S\ref{sec:fundamental_subspace})

\emph{Column Vector} -- an $m \times 1$ Matrix

\emph{Row Vector} -- an $1 \times m$ Matrix

The Transpose of a Column Vector is a Row Vector, and the Transpose of a Row
Vector is a Column Vector.

The Set of all Row Vectors forms a Vector Space called the \emph{Row Space}
(\S\ref{sec:row_space}) and the Set of all Column Vectors forms a Vector Space
called the \emph{Column Space} (\S\ref{sec:column_space}).

FIXME: how do the 'row space' and 'column space' defined here relate to the
'row space' and 'column space' of a matrix ?

The Column Space can be viewed as the Dual Space (\S\ref{sec:dual_space}) to
the Row Space since any Linear Functional (Linear Form \S\ref{sec:linear_form})
on the Column Space can be represented uniquely as an Inner Product with a
specific Row Vector.

When Column and Row Vectors viewed as $m \times 1$ and $1 \times n$ Matrices,
respectively, a Row Vector times a Column Vector is the Dot Product
(Inner Product \S\ref{sec:inner_product}), resulting in a single Scalar value,
and a Column Vector times a Row Vector is a Dyadic Product
(\S\ref{sec:dyadic_product}), which results in an $m \times n$ Matrix

\asterism

For the One-dimensional Vector Space over the Reals $\reals$, a 1x1 Matrix is
just a Real Number. The kinds of Linear Transformations are:
\begin{itemize}
  \item Identity -- $A = 1$
  \item Zero -- $A = 0$
  \item Reflection -- $A = -1$
  \item Scaling -- $A > 0$
  \item Scaling + Reflection -- $A < 0$
\end{itemize}
all Non-zero $A$ are Invertible

In $\reals^2$, a 2x2 Matrix may additionally represent:
\begin{itemize}
  \item Shear -- Horizontal, Vertical; every Shear Matrix has an Inverse which
    is a Shear Matrix with the Shear Element negated
\end{itemize}



% --------------------------------------------------------------------
\subsection{Rank}\label{sec:rank}
% --------------------------------------------------------------------

The \emph{Rank} of a Matrix $A$ is the Dimension of the Vector Space
``generated'' or \emph{Spanned} (\S\ref{sec:linear_span}) by its Columns, that
is, equal to the maximal number of Linearly Independent Columns of $A$ and is
identical to the Dimension of the Space Spanned by its Rows.

Dimension of the Column Space (\S\ref{sec:column_space}) and the Row Space
(\S\ref{sec:row_space})

\emph{Rank-Nullity Theorem}



% --------------------------------------------------------------------
\subsection{Nullity}\label{sec:nullity}
% --------------------------------------------------------------------

Dimension of the Nullspace (\S\ref{sec:nullspace})

\emph{Rank-Nullity Theorem}



% --------------------------------------------------------------------
\subsection{Corank}\label{sec:corank}
% --------------------------------------------------------------------

Dimension of the Left Nullspace (\S\ref{sec:left_nullspace})



% --------------------------------------------------------------------
\subsection{Matrix Product}\label{sec:matrix_product}
% --------------------------------------------------------------------

\fist Tensor Product (\S\ref{sec:module_tensor_product})

The Multiplication of an $m \times n$ Matrix $A$ with entries $a_{xy}$
on the left by an $n \times p$ Matrix $B$ with entries $b_{wv}$ on the
right results in an $m \times p$ Matrix $C$ with the entry $c_{ij}$
defined as:
\[
  \sum_{k=1}^n a_{ik} b_{kj}
\]
The columns of the resulting Matrix $C$ are the result of Multiplying
the corresponding Column of $B$ on the left by the Matrix $A$. And the
rows of the resulting Matrix $C$ are the result of Multiplying the
corresponding row of $A$ on the right by the Matrix $B$.



% --------------------------------------------------------------------
\subsection{Matrix Decomposition}\label{sec:matrix_decomposition}
% --------------------------------------------------------------------

Factorization of a Matrix into a Product of Matrices



\subsubsection{Eigendecomposition}\label{sec:eigendecomposition}

or \emph{Spectral Decomposition}

Factorization of a Matrix where the Matrix is represented in terms of its
Eigenvalues and Eigenvectors (\S\ref{sec:eigenvector})

only Diagonalizable Matrices (\S\ref{sec:diagonlizable_matrix}) can be
Factorized in this way



\subsubsection{Polar Decomposition}\label{sec:polar_decomposition}

The \emph{Polar Decomposition} of a Square Matrix $A$ is a Matrix Decomposition
of the form:
\[
  A = U P
\]
where $U$ is a Unitary Matrix (\S\ref{sec:unitary_matrix}) and $P$ is a
Positive-semidefinite (\S\ref{sec:positive_definite}) Hermitian
(\S\ref{sec:hermitian_matrix}) Matrix



\subsubsection{Singular-value Decomposition}\label{sec:svd}

(SVD)

Factoriztion of a Real or Complex Matrix generalizing the Eigendecomposition
(\S\ref{sec:eigendecomposition}) of a Positive Semidefinite
(\S\ref{sec:positive_definite}) Normal (\S\ref{sec:normal_matrix}) Matrix (e.g.
a Symmetric Matrix with Positive Eigenvalues) to any $m \times n$ Matrix via an
extension of the Polar Decomposition (\S\ref{sec:polar_decomposition})

The Singular-value Decomposition of an $m \times n$ Real or Complex Matrix $M$
is a Factorization of the form:
\[
  U \Sigma V^*
\]
where $U$ is an $m \times m$ Real or Complex Unitary Matrix
(\S\ref{sec:unitary_matrix}) of \emph{Left-singular Vectors}, $\Sigma$ is an $m
\times n$ Rectangular Diagonal Matrix (\S\ref{sec:diagonal_matrix}) with
Non-negative Real Numbers known as the \emph{Singular Values} of $M$ on the
Diagonal, and $V$ is an $n \times n$ Real or Complex Unitary Matrix of
\emph{Right-singular Vectors}.



% --------------------------------------------------------------------
\subsection{Diagonal Matrix}\label{sec:diagonal_matrix}
% --------------------------------------------------------------------

A Matrix with $0$ values in all non-diagonal entries

a Diagonalizable Matrix is a Square Matrix $A$ for which there exists an
Invertible Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix

\begin{itemize}
  \item Identity Matrices (\S\ref{sec:identity_matrix})
\end{itemize}



% --------------------------------------------------------------------
\subsection{Square Matrix}\label{sec:square_matrix}
% --------------------------------------------------------------------

there is a correspondence between $n\times{n}$ Square Matrices and Linear
Operators (\S\ref{sec:linear_operator}) on an $n$-dimensional Vector Space

a Square Matrix is Invertible (\S\ref{sec:invertible_matrix}) if and only if
its Determinant (\S\ref{sec:determinant}) is $0$

the Product of all Non-zero Eigenvalues (\S\ref{sec:eigenvalue}) of a Square
Matrix is the Pseudo-determinant (\S\ref{sec:pseudo_determinant}) and coincides
with the Determinant when the Matrix is Invertible

a Square Jacobian Matrix (\S\ref{sec:jacobian_matrix}) and its Determinant
(\S\ref{sec:determinant}) are both called ``the Jacobian''



\subsubsection{Determinant}\label{sec:determinant}

A Square Matrix is Invertible (\S\ref{sec:invertible_matrix}) if and only if
its Determinant is non-zero, or equivalently if its Rank (\S\ref{sec:rank})
equals the size of the Matrix, and if it is Invertible the Determinant of the
Inverse Matrix is given by $det(A^{-1}) = \frac{1}{det(A)}$.

The Special Linear Group $SL(n,F)$ of Degree $n$ over a Field $F$ is the Set of
$n \times n$ Matrices with Determinant $1$ with the Group Operations of
ordinary Matrix Multiplication and Matrix Inversion.

The Determinant of a Square Jacobian Matrix and the Matrix itself are both
called ``the Jacobian''.



\paragraph{Pseudo-determinant}\label{sec:pseudo_determinant}\hfill

Product of all Non-zero Eigenvalues (\S\ref{sec:eigenvalue}) of a Square Matrix

coincides with the regular Determinant when the Matrix is Non-singular
(Invertible \S\ref{sec:invertible_matrix})



\subsubsection{Identity Matrix}\label{sec:identity_matrix}

A Square Diagonal Matrix (\S\ref{sec:diagonal_matrix}) with $1$ on the main
diagonal



\subsubsection{Permutation Matrix}\label{sec:permutation_matrix}

A \emph{Permutation Matrix} is a Square Binary Matrix with exactly one
$1$ in each Row and each Column and $0$s elsewhere.




\subsubsection{Invertible Matrix}\label{sec:invertible_matrix}

(or \emph{Non-singular} or \emph{Non-degenerate})

a Square Matrix is Invertible if and only if its Determinant
(\S\ref{sec:determinant}) is $0$

the Pseudo-determinant (\S\ref{sec:pseudo_determinant}) of a Square Matrix
coincides with the Determinant when the Matrix is Invertible

a Square Matrix $A$ is Diagonalizable (\S\ref{sec:diagonalizable}) if there
exists an Invertible Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix
(\S\ref{sec:diagonal_matrix})



\subsubsection{Singular Matrix}\label{sec:singular_matrix}

Non-invertible Matrix

\fist in a System of DAEs (\S\ref{sec:system_of_daes}), the Jacobian Matrix
(\S\ref{sec:jacobian_matrix}) is a Singular (Non-invertible) Matrix



\subsubsection{Diagonalizable Matrix}\label{sec:diagonalizable_matrix}

A Square Matrix $A$ is \emph{Diagonalizable} if there exists an Invertible
Matrix $P$ such that $P^{-1}AP$ is a Diagonal Matrix
(\S\ref{sec:diagonal_matrix}).

A Square Matrix that is \emph{not} Diagonalizable is called \emph{Defective}.

Only Diagonlizable Matrices can be Eigendecomposed
(\S\ref{sec:eigendecomposition}).



\subsubsection{Symmetric Matrix}\label{sec:symmetric_matrix}

A \emph{Symmetric Matrix} $A$ is a Square Matrix that is equal to its
Transpose:
\[
  A = A^T
\]

the Complex extension of a Symmetric Matrix is a \emph{Hermitian Matrix}
(\S\ref{sec:hermitian_matrix})

Positive-definite Matrix (\S\ref{sec:positive_definite})



\subsubsection{Skew-symmetric Matrix}\label{sec:skew_symmetric}

Bivectors (\S\ref{sec:bivector}) are Isomorphic to Skew-symmetric Matrices



\subsubsection{Hamiltonian Matrix}\label{sec:hamiltonian_matrix}



% --------------------------------------------------------------------
\subsection{Upper Triangular Matrix}\label{sec:upper_triangular}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Diagonally Dominant Matrix}\label{sec:diagonally_dominant}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Irreducible Matrix}\label{sec:irreducible_matrix}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Complex Matrix}\label{sec:complex_matrix}
% --------------------------------------------------------------------

\subsubsection{Conjugate Transpose}\label{sec:conjugate_transpose}

The \emph{Conjugate Transpose} (or \emph{Hermitian Transpose}) of an $m \times
n$ Complex Matrix $A$ is the $n \times m$ Matrix $A^*$ resulting from the
Transpose (\S\ref{sec:transpose}) of $A$ followed by taking the Complex
Conjugate (\S\ref{sec:complex_conjugate}) of each entry.



\subsubsection{Normal Matrix}\label{sec:normal_matrix}

\subsubsection{Unitary Matrix}\label{sec:unitary_matrix}

A \emph{Unitary Matrix} $U$ has the Property that:
\[
  U^*U = UU^* = I
\]
where $U^* = \overline{U^T}$ is the Conjugate Transpose
(\S\ref{sec:conjugate_transpose}) of $U$

\fist Special Unitary Group (\S\ref{sec:special_unitary})
$\mathrm{SU}(n)$: the Lie Group of $n \times n$ Unitary Matrices with
Determinant $1$

only a Unitary Matrix can be used as a Quantum Gate in Quantum Computation

Polar Decomposition (\S\ref{sec:polar_decomposition})



\subsubsection{Hermitian Matrix}\label{sec:hermitian_matrix}

or \emph{Self-adjoint Matrix}

a Complex Square Matrix that is equal to its own Conjugate Transpose

Complex extension of Real Symmetric Matrices (\S\ref{sec:symmetric_matrix})

Positive-definite Matrix (\S\ref{sec:positive_definite})

Polar Decomposition (\S\ref{sec:polar_decomposition})



\subsubsection{Skew Hermitian Matrix}\label{sec:skew_hermitian}

or \emph{Antihermitian}



% --------------------------------------------------------------------
\subsection{Positive-definite Matrix}\label{sec:positive_definite}
% --------------------------------------------------------------------

a Property of Symmetric (\S\ref{sec:symmetric_matrix}) and Hermitian
(\S\ref{sec:hermitian_matrix}) Matrices

Positive Semi-definite



% --------------------------------------------------------------------
\subsection{Jacobian Matrix}\label{sec:jacobian_matrix}
% --------------------------------------------------------------------

%FIXME: move to multivariable calculus or merge with jacobian section ?

Matrix of all First-order Partial Derivatives (\S\ref{sec:partial_derivative})
of a Vector-valued Function (\S\ref{sec:vector_function})

a Square Jacobian Matrix and its Determinant (\S\ref{sec:determinant}) are both
called ``the Jacobian''

Jacobian (\S\ref{sec:jacobian}),
Locally Linear Functions (\S\ref{sec:locally_linear})

\fist Local Linearization (\S\ref{sec:local_linearization})

\fist the Jacobian Matrix for a System of DAEs (\S\ref{sec:system_of_daes}) is
a Singular (Non-invertible) Matrix (\S\ref{sec:singular_matrix})



\subsubsection{Jacobian Determinant}\label{sec:jacobian_determinant}



% --------------------------------------------------------------------
\subsection{Hessian Matrix}\label{sec:hessian_matrix}
% --------------------------------------------------------------------

%FIXME: move to multivariable calculus ?

Matrix of all Second-order Partial Derivatives (\S\ref{sec:partial_derivative})
of a Scalar-valued Function or Scalar Field

\fist Quadratic Approximation (\S\ref{sec:quadratic_approximation})



% ====================================================================
\section{Differential Algebra}\label{sec:differential_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Derivation}\label{sec:derivation}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Differential Ring}\label{sec:differential_ring}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Differential Field}\label{sec:differential_field}
% --------------------------------------------------------------------

% --------------------------------------------------------------------
\subsection{Differential Galois Theory}\label{sec:differential_galois}
% --------------------------------------------------------------------

Galois Theory (\S\ref{sec:galois_theory})

Extensions of Differential Fields

Galois Groups (\S\ref{sec:galois_group}) of Differential Equations



% ====================================================================
\section{Multilinear Algebra}\label{sec:multilinear_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Multilinear Map}\label{sec:multilinear_map}
% --------------------------------------------------------------------

\subsubsection{Bilinear Map}\label{sec:bilinear_map}

Module (\S\ref{sec:module}), Vector Space (\S\ref{sec:vector_space})

$R$-Algebra (\S\ref{sec:r_algebra})

$K$-Algebra (\S\ref{sec:k_algebra})

Bilinear Product (\S\ref{sec:bilinear_product})



% --------------------------------------------------------------------
\subsection{Multilinear Form}\label{sec:multilinear_form}
% --------------------------------------------------------------------

\subsubsection{Bilinear Form}\label{sec:bilinear_form}

generalization of Dot Product (\S\ref{sec:inner_product}) of Euclidean Space

Sesquilinear Form (\S\ref{sec:sesquilinear_form}) generalization of Bilinear
Forms to cases where $K$ is the Field of Complex Numbers and the mapping is
Antilinear (Conjugate-linear \S\ref{sec:antilinear}) in one argument.

An Inner Product (\S\ref{sec:inner_product}) is a Sesquilinear Form and an
Inner Product over a Real Vector Space is a Positive-definite
(\S\ref{sec:positive_definite}) Symmetric Bilinear Form
(\S\ref{sec:symmetric_bilinear}).



\paragraph{Orthogonality}\label{sec:orthogonality}\hfill

Two Vectors $\vec{u}, \vec{v}$ in a Vector Space with Bilinear Form $B$
are are \emph{Orthogonal} when $B(u,v) = 0$, denoted:
\[
  \vec{u} \bot \vec{v}
\]
The Zero Vector, $\vec{0}$, is Orthogonal to every Vector (including itself).

\fist Inner Product (\S\ref{sec:inner_product})

Two Linear Subspaces (\S\ref{sec:linear_subspace}) $A, B \subset V$ of an Inner
Product Space $V$ are called \emph{Orthogonal Subspaces}
(\S\ref{sec:orthogonal_subspace}) if each Vector in $A$ is Orthogonal to each
Vector in $B$.



\paragraph{Symmetric Bilinear Form}\label{sec:symmetric_bilinear}\hfill

Symmetric Bilinear Forms over a Vector Space correspond one-to-one
with Quadratic Forms (\S\ref{sec:quadratic_form}) over the Vector
Space

An Inner Product (\S\ref{sec:inner_product}) on a Real Vector Space is
a Positive-definite (\S\ref{sec:definite_quadratic}) Symmetric
Bilinear Form.



\paragraph{Degenerate Bilinear Form}
\label{sec:degenerate_bilinear_form}\hfill

Pseudo-Riemannian Manifold (\S\ref{sec:pseudo_riemannian})



\subsubsection{Quadratic Form}\label{sec:quadratic_form}

%FIXME: move to polynomials ???

Homogenous Polynomial (\S\ref{sec:homogenous_polynomial})

Quadratic Forms over a Vector Space correspond one-to-one with
Symmetric Bilinear Forms (\S\ref{sec:symmetric_bilinear}) over
the Vector Space

Matrix form $\vec{x}^T M \vec{x}$:
\[
  [x y]
  \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix}
  = ax^2 + 2bxy + cy^2
\]



\paragraph{Definite Quadratic Form}\label{sec:definite_quadratic}\hfill

a Quadratic Form over some Real Vector Space that has the same Sign
for every Non-zero Vector

\emph{Positive Definite}

\emph{Negative Definite}

\emph{Semidefinite}

an Inner Product (\S\ref{sec:inner_product}) on a Real Vector Space is
a Positive-definite Symmetric Bilinear
(\S\ref{sec:symmetric_bilinear}) Form



\paragraph{Quadratic Space}\label{sec:quadratic_space}\hfill



\subsubsection{Sesquilinear Form}\label{sec:sesquilinear_form}

generalization of Bilinear Forms (\S\ref{sec:bilinear_form}) to cases where $K$
is the Field of Complex Numbers and the mapping is Antilinear (Conjugate-linear
\S\ref{sec:antilinear}) in one argument

Inner Products (\S\ref{sec:inner_product}) are Sesquilinear Forms



\paragraph{Hermitian Form}\label{sec:hermitian_form}\hfill

an Inner Product (\S\ref{sec:inner_product}) is a Positive-definite
(\S\ref{sec:definite_quadratic}) Hermitian Form



% --------------------------------------------------------------------
\subsection{Dyadic Algebra}\label{sec:dyadic_algebra}
% --------------------------------------------------------------------

\subsubsection{Dyadic Product}\label{sec:dyadic_product}

Product of a Column Vector multiplied by a Row Vector produces a Second-order
Tensor called a \emph{Dyadic}



% ====================================================================
\section{Numerical Linear Algebra}\label{sec:numerical_linear_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Iterative Method}\label{sec:iterative_method}
% --------------------------------------------------------------------

%FIXME: move to optimization ???

cf. \emph{Direct Methods} attempt to solve a problem by a Finite Sequence of
Operations delivering the exact Solution

Attractive Fixed Points (Dynamical Systems \S\ref{sec:dynamical_systems})



\subsubsection{Stationary Iterative Method}
\label{sec:stationary_iterative}

or \emph{Relaxation Methods}

\fist not to be confused with Relaxation (Approximation \S\ref{sec:relaxation})

used to solve the Linear Equations resulting from a Discretization of the
Differential Equation, e.g. by Finite Differences



\paragraph{Jacobi Method}\label{sec:jacobi_method}\hfill

Diagonally Dominant (\S\ref{sec:diagonally_dominant}) System of Linear
Equations (\S\ref{sec:system_of_linear_equations})



\subparagraph{Projected Jacobi Method}
\label{sec:projected_jacobi_method}



\paragraph{Gauss-Seidel Method}\label{sec:gauss_seidel}\hfill

(or \emph{Liebmann Method} or \emph{Method of Successive
  Displacement})

Iterative Method for solving a Linear System of Equations
(\S\ref{sec:system_of_linear_equations})

Convergence only guaranteed if the Matrix is either Diagonally
Dominant (\S\ref{sec:diagonally_dominant}) or Symmetric
(\S\ref{sec:symmetric_matrix}) and Positive Definite
(\S\ref{sec:positive_definite})

Gauss-Seidel is the same as Successive Over-Relaxation
(\S\ref{sec:sucessive_over_relaxation}) with $\omega = 1$



\subparagraph{Projected Gauss-Seidel Method}\hfill
\label{sec:projected_gauss_seidel}

Gauss-Seidel applied to Linear Complementarity Problem (LCP
\S\ref{sec:linear_complementarity})

PGS



\subparagraph{Non-linear Gauss-Seidel Method}
\label{sec:nonlinear_gauss_seidel}

NGS



\paragraph{Successive Over-Relaxation Method}
\label{sec:successive_over_relaxation}\hfill

SOR

Gauss-Seidel (\S\ref{sec:gauss_seidel}) is the same as SOR with $\omega = 1$



\subsubsection{Krylov Subspace Method}\label{sec:krylov_subspace_method}



% ====================================================================
\section{Super Linear Algebra}\label{sec:super_linear_algebra}
% ====================================================================

% --------------------------------------------------------------------
\subsection{Superalgebra}\label{sec:superalgebra}
% --------------------------------------------------------------------

$Z_2$-graded Algebra



\subsubsection{Clifford Algebra}\label{sec:clifford_algebra}

\url{https://golem.ph.utexas.edu/category/2014/07/the_tenfold_way.html} -- kinds
of matter corresponding to Real Clifford Algebras:
\begin{itemize}
\item $Cl_0$ (equivalent to $\reals$)
\item $Cl_1$
\item $Cl_2$
\item $Cl_3$
\item $Cl_4$ (equivalent to $\quats$)
\item $Cl_5$
\item $Cl_6$
\item $Cl_7$
\end{itemize}
and Complex Clifford Algebras:
\begin{itemize}
\item $\comps{l}_0$ (equivalent to $\comps$)
\item $\comps{l}_1$ (Superalgebra created by adding an Odd Square Root of $-1$
  to the purely Even Algebra $\comps$)
\end{itemize}
