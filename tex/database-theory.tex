%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Database Theory}\label{part:database_theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ==============================================================================
\section{Data}\label{sec:data}
% ==============================================================================

\emph{Sample Data} (\emph{Statistical Sample} \S\ref{sec:sample})

when viewed ``in context'', Data conveys \emph{Information}
(\S\ref{sec:information})

\fist cf. Datatype (Type Theory \S\ref{sec:datatype}), Statistical Data Type
(\S\ref{sec:statistical_data_type})



% ------------------------------------------------------------------------------
\subsection{Data Item}\label{sec:data_item}
% ------------------------------------------------------------------------------

cf. ``\emph{Data Point}'' (Statistical Sample \S\ref{sec:sample})



% ------------------------------------------------------------------------------
\subsection{Data Collection}\label{sec:data_collection}
% ------------------------------------------------------------------------------

cf. \emph{Data Generating Process} (\S\ref{sec:data_generating_process}),
\emph{Sampling} (\S\ref{sec:sampling})



% ------------------------------------------------------------------------------
\subsection{Dataset}\label{sec:dataset}
% ------------------------------------------------------------------------------

cf. \emph{Sample Data} (\emph{Statistical Sample} \S\ref{sec:sample})

\fist Statistical Transformations (\S\ref{sec:data_transformation}) of Datasets

Datasets in Machine Learning Algorithms (\S\ref{sec:learning_algorithm}):
\begin{itemize}
  \item Training Dataset (\S\ref{sec:training_dataset})
  \item Validation Dataset (\S\ref{sec:validation_dataset})
  \item Test Dataset (\S\ref{sec:test_dataset})
\end{itemize}



% ==============================================================================
\section{Data Structure}\label{sec:data_structure}
% ==============================================================================

%FIXME possibly move this section?

potentially Infinite Data Structures: Abstracted as $F$-coalgebras
(\S\ref{sec:f_coalgebra})

(also Data Structure as Folds of Church Encodings)
%??? FIXME

(Milewski - Understanding F-Algebras)

As Recursive Functions are defined as Fixed Points of regular
Functions, (Nested) Data Structures can be defined as Fixed Points of
regular Type Constructors.

Functors as Type Constructors give rise to Nested Data Structures that
allow Recursive Evaluation (generalized Folding).



% -----------------------------------------------------------------------------
\subsection{Ordered Tree}\label{sec:ordered_tree}
% -----------------------------------------------------------------------------

\subsubsection{Trie}\label{sec:trie}

\subsubsection{Prefix Tree}\label{sec:prefix_tree}

\subsubsection{Suffix Tree}\label{sec:suffix_tree}



% -----------------------------------------------------------------------------
\subsection{Fingertree}\label{sec:fingertree}
% -----------------------------------------------------------------------------



% ==============================================================================
\section{Data Transformation}\label{sec:data_transformation}
% ==============================================================================

% -----------------------------------------------------------------------------
\subsection{Bidirectional Transformation}
\label{sec:bidirectional_transformation}
% -----------------------------------------------------------------------------

\subsubsection{Bidirectional Model Tranformation}
\label{sec:bidirectional_model_transformation}

\subsubsection{Lens}\label{sec:lens}

2019 - Fong, Johnson - \emph{Functorial Backpropagation and Symmetric Lenses} -
\url{https://www.youtube.com/watch?v=s0WTRHe-4ZI} -- ``Lenses are Learners'';
\fist Backpropagation (\S\ref{sec:backpropagation})

\fist cf. System T (\S\ref{sec:system_t}) -- Proof Interpretation of Heyting
Arithmetic (\S\ref{sec:heyting_arithmetic}) into a Finite-type Extension of
Primitive Recursive Arithmetic (\S\ref{sec:primitive_recursive}); the
Interpretation of Intuitionistic Implication is the first example of a ``Lense''
from Programming: a Proof of $\varphi \rightarrow \psi$ is a \emph{Lens} from
Proofs of $\varphi$ to Proofs of $\psi$
(\url{https://julesh.com/2018/08/16/lenses-for-philosophers/})

\fist cf. van Laarhoven Free Monad (\S\ref{sec:vanlaarhoven_free_monad}) -- van
Laarhoven Function: $(X \rightarrow F X) \rightarrow F Y$ equivalent to Store
Comonad

\emph{Profunctor Optics} (\S\ref{sec:profunctor_optics}):

%FIXME: move to profunctor subsection ?

\url{https://ncatlab.org/nlab/show/optic+%28in+computer+science%29}

\url{http://oleg.fi/gists/posts/2017-04-18-glassery.html}

\url{https://golem.ph.utexas.edu/category/2020/01/profunctor_optics_the_categori.html}

compositional representation of \emph{Bidirectional Data Accessors}

\fist \emph{Lens} (\S\ref{sec:lens}) -- Products; \emph{Note}: not the same as
``Lenses'' (Anamorphism or \emph{Co-iteration} \S\ref{sec:anamorphism}) in
Recursion Theory

\emph{view}, \emph{update}

``Generalized Functional References''

\emph{Prism} -- Tagged Unions; Lenses in the \emph{Opposite} Category

\emph{match}, \emph{build}

\emph{Traversal} -- Containers

\emph{extract}

\emph{Grate}



\paragraph{Asymmetric Lens}\label{sec:asymmetric_lens}\hfill

\paragraph{Symmetric Lens}\label{sec:symmetric_lens}\hfill

\paragraph{Bare Lens}\label{sec:bare_lens}\hfill

or \emph{Set-based Lens}



\paragraph{Constant Complement Lens}\label{sec:constant_complement_lens}\hfill



% ==============================================================================
\section{Relational Algebra}\label{sec:relational_algebra}
% ==============================================================================

\emph{Domain Relational Calculus}



% -----------------------------------------------------------------------------
\subsection{Projection}\label{sec:relational_projection}
% -----------------------------------------------------------------------------



% ==============================================================================
\section{Relational Model}\label{sec:relational_model}
% ==============================================================================

\emph{Relational Calculus}

%FIXME:

2016 - Gibbons - \emph{Comprehending Ringads} -- ``Ringad Comprehensions
represent a convenient notation for expressing \emph{Database Queries}.''



% -----------------------------------------------------------------------------
\subsection{Relation}\label{sec:database_relation}
% -----------------------------------------------------------------------------

(wiki):

A \emph{Relation} is a Heading (\S\ref{sec:heading}) paired with a Body
(\S\ref{sec:body}).

A Relation is a Set of Tuples $(d_1, d_2, \ldots, d_n)$ where each Element $d_j
\in D_j$ is a Member of a Data Domain $D_j$. Unlike a Set Theoretic Relation
(\S\ref{sec:relation}), there is no ordering to the Elements of the Tuples of a
Relation-- each element is called an \emph{Attribute Value} and an
\emph{Attribute} is a Name paired with a Data Domain (sometimes called a
\emph{Type} or \emph{Data Type} \S\ref{sec:datatype}), so that a Tuple is a
\emph{Set of Attribute Values} in which no two distinct Elements have the same
Attribute Name, i.e. a Tuple in this case is a Function mapping Names to
Values.

A Relation can be seen as an instantiation of a \emph{Relation Schema}
(\S\ref{sec:relation_schema}) if it has the Heading of the Schema and Satisfies
the applicable Constraints.

in SQL, Relations are represented by \emph{Tables} where each Row represents a
single Tuple and the Values of each Attribute form a Column



\subsubsection{Attribute}\label{sec:database_attribute}

Attribute Value

Data Domain (Data Type \S\ref{sec:datatype})

Tuples



\subsubsection{Heading}\label{sec:heading}

a Set of Attributes in which no two distinct Elements have the same Name is
called a \emph{Heading}; the number of Attributes constituting a Heading is
called the \emph{Degree}



\subsubsection{Body}\label{sec:body}

a Set of Tuples having the same Heading is called a \emph{Body}



\subsubsection{Relation Schema}\label{sec:relation_schema}

a Heading with a Set of \emph{Constraints} defined in terms of that Heading

a Relation can be seen as an instantiation of a \emph{Relation Schema}
(\S\ref{sec:relation_schema}) if it has the Heading of the Schema and Satisfies
the applicable Constraints

a Named Relation Schema is a \emph{Relation Variable}



% ==============================================================================
\section{Database Schema}\label{sec:database_schema}
% ==============================================================================

% ==============================================================================
\section{Time Series Analysis}\label{sec:time_series_analysis}
% ==============================================================================

Ergodic Processes (\S\ref{sec:ergodic_process})

\fist cf. Statistical Signal Processing (\S\ref{sec:statistical_signal})

\fist Markov Models (\S\ref{sec:markov_model})

Stationary Processes (\S\ref{sec:stationary_process}) -- Stationarity assumption
underlies many procedures in Time-series Analysis

if a Stochastic Process has a Unit Root (\S\ref{sec:unit_root}), i.e. if $1$ is
a Root of the Process's Characteristic Equation (FIXME: characteristic function
???), then the Process is Non-stationary, but does not necessarily have a
``trend''



% -----------------------------------------------------------------------------
\subsection{Time Series}\label{sec:time_series}
% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------
\subsection{Lag Operator}\label{sec:lag_operator}
% -----------------------------------------------------------------------------

or \emph{Backshift Operator}



% -----------------------------------------------------------------------------
\subsection{Lag Polynomial}\label{sec:lag_polynomial}
% -----------------------------------------------------------------------------

a Polynomial of Lag Operators



% -----------------------------------------------------------------------------
\subsection{Moving-average Model}\label{sec:moving_average}
% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------
\subsection{Integrated Model}\label{sec:integrated_model}
% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------
\subsection{Autoregressive Model}\label{sec:autoregressive_model}
% -----------------------------------------------------------------------------

$AR(p)$ of Order $p$

$X_t = c + \sum_{i=1}^p \varphi_i X_{t-i} + \varepsilon_t$

where $\varphi_1, \ldots, \varphi_p$ are \emph{Parameters} of the Model, $c$ is
Constant, and $\varepsilon_t$ is White Noise

can be viewed as the output of an All-pole Infinite Impulse Response Filter with
White Noise input

the Autocorrelation Function (\S\ref{sec:autocorrelation_function}) of an
$AR(p)$ Process is a Sum of decaying Exponentials:
\[
  \rho(\tau) = \sum_{k=1}^p a_k y_k^{-|\tau|}
\]
where $y_k$ are the roots of the Polynomial:
\[
  \phi(B) = 1 - \sum_{k=1}^p \varphi_k B^k
\]
where $B$ is the \emph{Backshift (Lag) Operator} (\S\ref{sec:lag_operator}),
$\phi$ is the Function defining the Autoregression, and $\varphi_k$ are the
Coefficients in the Autoregression;
each Real Root contributes a component to the Autocorrelation Function that
decays Exponentially, and each pair of Complex Conjugate Roots contributes an
Exponentially Damped Oscillation

\fist cf. Recurrent Neural Networks (RNNs \S\ref{sec:rnn})

2018 - Le -
\emph{A Purely Functional Typed Approach to Trainable Models (Part 2)} -
\url{https://blog.jle.im/entry/purely-functional-typed-models-2.html}
-- $AR(2)$ as a stateful model

2018 - Le -
\emph{A Purely Functional Typed Approach to Trainable Models (Part 3)} -
\url{https://blog.jle.im/entry/purely-functional-typed-models-3.html}
-- a general Autoregressive Model $AR(p)$ of any Order $p$ can be defined as a
\emph{Lagged} Fully Connected Layer



\subsubsection{AutoRegressive Moving Average (ARMA)}\label{sec:arma}

\subsubsection{AutoRegressive Integrated Moving Average (ARIMA)}
\label{sec:arima}

\subsection{AutoRegressive Fractionally Integrated Moving Average (ARFIMA)}
\label{sec:arfima}

\subsubsection{Vector AutoRegression (VAR)}\label{sec:var}

used to capture Linear Interdependencies among multiple Time Series



\subsubsection{Non-linear Autoregressive Exogenous Model (NARX)}\label{sec:narx}

\subsubsection{AutoRegressive Conditional Heteroskedasticity (ARCH)}
\label{sec:arch}

\subsubsection{Generalized AutoRegressive Conditional Heteroskedasticity (GARCH)}
\label{sec:garch}



% -----------------------------------------------------------------------------
\subsection{Doubly Stochastic Model}\label{sec:doubly_stochastic}
% -----------------------------------------------------------------------------

cf. Compound Probability Distribution (\S\ref{sec:compound_probability})



% -----------------------------------------------------------------------------
\subsection{Model-free Analysis}\label{sec:model_free_analysis}
% -----------------------------------------------------------------------------

Wavelet Transform (\S\ref{sec:wavelet_transform}) methods: Locally Stationary
Wavelets, Wavelet Decomposed Neural Networks



% -----------------------------------------------------------------------------
\subsection{Multiresolution Model}\label{sec:multiresolution_model}
% -----------------------------------------------------------------------------

\subsubsection{Markov Switching MultiFractal (MSMF)}\label{sec:msmf}

%FIXME: move this section ???

a Stochastic Volatility Model

\fist Multifractals (\S\ref{sec:multifractal})

\fist Markov Models (\S\ref{sec:markov_model})



% ==============================================================================
\section{Formal Concept Analysis}\label{sec:fca}
% ==============================================================================

%FIXME this is an information science topic

\url{http://www.upriss.org.uk/fca/fca.html}

\emph{Concept Hierarchy} (or \emph{Formal Ontology})

\url{https://golem.ph.utexas.edu/category/2014/02/galois_correspondences_and_enr.html}
(4th post in a series)



% -----------------------------------------------------------------------------
\subsection{Formal Context}\label{sec:formal_context}
% -----------------------------------------------------------------------------

\emph{Formal Context}:
\[
  K = (G,M,I)
\]

$G$ -- \emph{Objects}

$M$ -- \emph{Attributes}

$I \subseteq G \times M$ -- \emph{Incidence}



% -----------------------------------------------------------------------------
\subsection{Formal Concept}\label{sec:formal_concept}
% -----------------------------------------------------------------------------

a \emph{Formal Concept} is a pair $(A,B)$ where:
\begin{itemize}
  \item $A \subseteq G$ -- a Set of Objects
  \item $B \subseteq M$ -- a Set of Attributes
  \item $A' = B$ -- where $A' = \{ m \in M \ |\ \forall g \in A, gIm \}$
  \item $B' = A$ -- where $B' = \{ g \in G \ |\ \forall m \in B, gIm \}$
\end{itemize}
that is:
\begin{itemize}
  \item every Object in $A$ has every Attribute in $B$
  \item for every Object in $G$ that is \emph{not} in $A$, there is
    \emph{some} Attribute in $B$ that the Object \emph{does not} have
  \item for every Attribute in $M$ that is \emph{not} in $B$, there is
    \emph{some} Object in $A$ that \emph{does not} have that Attribute
\end{itemize}
